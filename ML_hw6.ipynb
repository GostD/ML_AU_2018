{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 417,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras import layers\n",
    "from keras.layers import Input, Dense, Activation, Flatten, Conv2D, MaxPooling2D, Reshape, Dropout\n",
    "from keras.models import Model, load_model, Sequential\n",
    "from keras.initializers import glorot_uniform\n",
    "from keras.optimizers import Adam, SGD\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "df_mnist = pd.read_csv('mnist.csv')\n",
    "train_mnist = np.array(df_mnist.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_train_val(tr_v_dict):\n",
    "    train_acc = tr_v_dict['acc']\n",
    "    val_acc = tr_v_dict['val_acc']\n",
    "    arr = np.arange(len(val_acc))\n",
    "    plt.plot(arr, train_acc, color='r', label='train')\n",
    "    plt.plot(arr, val_acc, color='b', label='val')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.ylabel('accuracy')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = np.unique(train_mnist[:,0])\n",
    "cls_data = {}\n",
    "for i in classes:\n",
    "    cls_data[i] = np.argwhere(train_mnist[:,0]==i)\n",
    "    cls_data[i] = cls_data[i][:,0]\n",
    "train = np.array([np.ones(train_mnist.shape[1])])\n",
    "val = np.array([np.ones(train_mnist.shape[1])])\n",
    "test = np.array([np.ones(train_mnist.shape[1])])\n",
    "for i in classes:\n",
    "    ln = cls_data[i].shape[0]\n",
    "    l1 = ln*8//10\n",
    "    l2 = ln*9//10\n",
    "    train = np.concatenate((train, train_mnist[cls_data[i][:l1]]), axis=0)\n",
    "    val = np.concatenate((val, train_mnist[cls_data[i][l1:l2]]), axis=0)\n",
    "    test = np.concatenate((test, train_mnist[cls_data[i][l2:]]), axis=0)\n",
    "train = train[1:]\n",
    "val = val[1:]\n",
    "test = test[1:]\n",
    "np.random.shuffle(train)\n",
    "np.random.shuffle(val)\n",
    "np.random.shuffle(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_xy(data):\n",
    "    X = data[:,1:]\n",
    "    Y_t = data[:,0]\n",
    "    Y = np.zeros((data.shape[0], 10))\n",
    "    Y_t = np.array(Y_t, dtype=int)\n",
    "    Y[np.arange(data.shape[0]),Y_t] += 1\n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X, train_Y = convert_to_xy(train)\n",
    "val_X, val_Y = convert_to_xy(val)\n",
    "test_X, test_Y = convert_to_xy(test)\n",
    "X, Y = convert_to_xy(train_mnist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "metadata": {},
   "outputs": [],
   "source": [
    "###1###\n",
    "def mod_v1(input_shape):\n",
    "    X_input = Input(input_shape)\n",
    "    X = X_input\n",
    "    X = Dense(10)(X)\n",
    "    X = Activation('softmax')(X)\n",
    "    model = Model(inputs = X_input, outputs = X, name='mod_v1')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_1 = mod_v1((train_mnist.shape[1]-1,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_1.compile(optimizer=SGD(lr=0.1), loss='mean_squared_logarithmic_error', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7997 samples, validate on 999 samples\n",
      "Epoch 1/300\n",
      "7997/7997 [==============================] - 6s 698us/step - loss: 0.0747 - acc: 0.2198 - val_loss: 0.0653 - val_acc: 0.3193\n",
      "Epoch 2/300\n",
      "7997/7997 [==============================] - 0s 47us/step - loss: 0.0624 - acc: 0.3483 - val_loss: 0.0581 - val_acc: 0.3924\n",
      "Epoch 3/300\n",
      "7997/7997 [==============================] - 0s 47us/step - loss: 0.0562 - acc: 0.4129 - val_loss: 0.0538 - val_acc: 0.4384\n",
      "Epoch 4/300\n",
      "7997/7997 [==============================] - 0s 46us/step - loss: 0.0536 - acc: 0.4403 - val_loss: 0.0542 - val_acc: 0.4344\n",
      "Epoch 5/300\n",
      "7997/7997 [==============================] - 0s 47us/step - loss: 0.0526 - acc: 0.4515 - val_loss: 0.0499 - val_acc: 0.4795\n",
      "Epoch 6/300\n",
      "7997/7997 [==============================] - 0s 47us/step - loss: 0.0519 - acc: 0.4579 - val_loss: 0.0497 - val_acc: 0.4825\n",
      "Epoch 7/300\n",
      "7997/7997 [==============================] - 0s 47us/step - loss: 0.0506 - acc: 0.4718 - val_loss: 0.0500 - val_acc: 0.4785\n",
      "Epoch 8/300\n",
      "7997/7997 [==============================] - 0s 47us/step - loss: 0.0503 - acc: 0.4754 - val_loss: 0.0500 - val_acc: 0.4795\n",
      "Epoch 9/300\n",
      "7997/7997 [==============================] - 0s 47us/step - loss: 0.0509 - acc: 0.4696 - val_loss: 0.0503 - val_acc: 0.4755\n",
      "Epoch 10/300\n",
      "7997/7997 [==============================] - 0s 48us/step - loss: 0.0502 - acc: 0.4769 - val_loss: 0.0500 - val_acc: 0.4795\n",
      "Epoch 11/300\n",
      "7997/7997 [==============================] - 0s 49us/step - loss: 0.0495 - acc: 0.4837 - val_loss: 0.0488 - val_acc: 0.4915\n",
      "Epoch 12/300\n",
      "7997/7997 [==============================] - 0s 47us/step - loss: 0.0496 - acc: 0.4826 - val_loss: 0.0488 - val_acc: 0.4905\n",
      "Epoch 13/300\n",
      "7997/7997 [==============================] - 0s 49us/step - loss: 0.0496 - acc: 0.4824 - val_loss: 0.0482 - val_acc: 0.4965\n",
      "Epoch 14/300\n",
      "7997/7997 [==============================] - 0s 52us/step - loss: 0.0500 - acc: 0.4791 - val_loss: 0.0486 - val_acc: 0.4935\n",
      "Epoch 15/300\n",
      "7997/7997 [==============================] - 0s 47us/step - loss: 0.0496 - acc: 0.4829 - val_loss: 0.0503 - val_acc: 0.4755\n",
      "Epoch 16/300\n",
      "7997/7997 [==============================] - 0s 48us/step - loss: 0.0501 - acc: 0.4772 - val_loss: 0.0486 - val_acc: 0.4935\n",
      "Epoch 17/300\n",
      "7997/7997 [==============================] - 0s 48us/step - loss: 0.0495 - acc: 0.4841 - val_loss: 0.0479 - val_acc: 0.5005\n",
      "Epoch 18/300\n",
      "7997/7997 [==============================] - 0s 51us/step - loss: 0.0492 - acc: 0.4867 - val_loss: 0.0490 - val_acc: 0.4895\n",
      "Epoch 19/300\n",
      "7997/7997 [==============================] - 0s 47us/step - loss: 0.0501 - acc: 0.4769 - val_loss: 0.0483 - val_acc: 0.4955\n",
      "Epoch 20/300\n",
      "7997/7997 [==============================] - 0s 49us/step - loss: 0.0492 - acc: 0.4874 - val_loss: 0.0506 - val_acc: 0.4725\n",
      "Epoch 21/300\n",
      "7997/7997 [==============================] - 0s 54us/step - loss: 0.0503 - acc: 0.4753 - val_loss: 0.0480 - val_acc: 0.4995\n",
      "Epoch 22/300\n",
      "7997/7997 [==============================] - 0s 49us/step - loss: 0.0491 - acc: 0.4878 - val_loss: 0.0485 - val_acc: 0.4945\n",
      "Epoch 23/300\n",
      "7997/7997 [==============================] - 0s 47us/step - loss: 0.0488 - acc: 0.4907 - val_loss: 0.0483 - val_acc: 0.4965\n",
      "Epoch 24/300\n",
      "7997/7997 [==============================] - 0s 54us/step - loss: 0.0489 - acc: 0.4898 - val_loss: 0.0487 - val_acc: 0.4915\n",
      "Epoch 25/300\n",
      "7997/7997 [==============================] - 0s 52us/step - loss: 0.0489 - acc: 0.4899 - val_loss: 0.0485 - val_acc: 0.4945\n",
      "Epoch 26/300\n",
      "7997/7997 [==============================] - 0s 47us/step - loss: 0.0491 - acc: 0.4886 - val_loss: 0.0485 - val_acc: 0.4945\n",
      "Epoch 27/300\n",
      "7997/7997 [==============================] - 0s 49us/step - loss: 0.0492 - acc: 0.4879 - val_loss: 0.0494 - val_acc: 0.4855\n",
      "Epoch 28/300\n",
      "7997/7997 [==============================] - 0s 51us/step - loss: 0.0498 - acc: 0.4814 - val_loss: 0.0491 - val_acc: 0.4885\n",
      "Epoch 29/300\n",
      "7997/7997 [==============================] - 0s 52us/step - loss: 0.0488 - acc: 0.4913 - val_loss: 0.0489 - val_acc: 0.4895\n",
      "Epoch 30/300\n",
      "7997/7997 [==============================] - 0s 47us/step - loss: 0.0489 - acc: 0.4899 - val_loss: 0.0483 - val_acc: 0.4965\n",
      "Epoch 31/300\n",
      "7997/7997 [==============================] - 0s 51us/step - loss: 0.0486 - acc: 0.4934 - val_loss: 0.0476 - val_acc: 0.5045\n",
      "Epoch 32/300\n",
      "7997/7997 [==============================] - 0s 52us/step - loss: 0.0488 - acc: 0.4917 - val_loss: 0.0478 - val_acc: 0.5015\n",
      "Epoch 33/300\n",
      "7997/7997 [==============================] - 0s 48us/step - loss: 0.0489 - acc: 0.4907 - val_loss: 0.0483 - val_acc: 0.4965\n",
      "Epoch 34/300\n",
      "7997/7997 [==============================] - 0s 48us/step - loss: 0.0489 - acc: 0.4909 - val_loss: 0.0484 - val_acc: 0.4955\n",
      "Epoch 35/300\n",
      "7997/7997 [==============================] - 0s 47us/step - loss: 0.0487 - acc: 0.4919 - val_loss: 0.0479 - val_acc: 0.5005\n",
      "Epoch 36/300\n",
      "7997/7997 [==============================] - 0s 47us/step - loss: 0.0485 - acc: 0.4944 - val_loss: 0.0478 - val_acc: 0.5015\n",
      "Epoch 37/300\n",
      "7997/7997 [==============================] - 0s 46us/step - loss: 0.0484 - acc: 0.4952 - val_loss: 0.0486 - val_acc: 0.4925\n",
      "Epoch 38/300\n",
      "7997/7997 [==============================] - 0s 52us/step - loss: 0.0484 - acc: 0.4956 - val_loss: 0.0480 - val_acc: 0.5005\n",
      "Epoch 39/300\n",
      "7997/7997 [==============================] - 0s 52us/step - loss: 0.0488 - acc: 0.4917 - val_loss: 0.0488 - val_acc: 0.4915\n",
      "Epoch 40/300\n",
      "7997/7997 [==============================] - 0s 50us/step - loss: 0.0493 - acc: 0.4862 - val_loss: 0.0489 - val_acc: 0.4905\n",
      "Epoch 41/300\n",
      "7997/7997 [==============================] - 0s 50us/step - loss: 0.0493 - acc: 0.4854 - val_loss: 0.0481 - val_acc: 0.4985\n",
      "Epoch 42/300\n",
      "7997/7997 [==============================] - 0s 52us/step - loss: 0.0487 - acc: 0.4924 - val_loss: 0.0485 - val_acc: 0.4945\n",
      "Epoch 43/300\n",
      "7997/7997 [==============================] - 0s 52us/step - loss: 0.0486 - acc: 0.4931 - val_loss: 0.0482 - val_acc: 0.4975\n",
      "Epoch 44/300\n",
      "7997/7997 [==============================] - 0s 50us/step - loss: 0.0487 - acc: 0.4924 - val_loss: 0.0476 - val_acc: 0.5035\n",
      "Epoch 45/300\n",
      "7997/7997 [==============================] - 0s 52us/step - loss: 0.0486 - acc: 0.4931 - val_loss: 0.0478 - val_acc: 0.5025\n",
      "Epoch 46/300\n",
      "7997/7997 [==============================] - 0s 52us/step - loss: 0.0488 - acc: 0.4919 - val_loss: 0.0487 - val_acc: 0.4925\n",
      "Epoch 47/300\n",
      "7997/7997 [==============================] - 0s 53us/step - loss: 0.0489 - acc: 0.4904 - val_loss: 0.0478 - val_acc: 0.5015\n",
      "Epoch 48/300\n",
      "7997/7997 [==============================] - 0s 52us/step - loss: 0.0483 - acc: 0.4961 - val_loss: 0.0481 - val_acc: 0.4985\n",
      "Epoch 49/300\n",
      "7997/7997 [==============================] - 0s 53us/step - loss: 0.0488 - acc: 0.4916 - val_loss: 0.0483 - val_acc: 0.4975\n",
      "Epoch 50/300\n",
      "7997/7997 [==============================] - 0s 54us/step - loss: 0.0487 - acc: 0.4929 - val_loss: 0.0484 - val_acc: 0.4955\n",
      "Epoch 51/300\n",
      "7997/7997 [==============================] - 0s 48us/step - loss: 0.0488 - acc: 0.4914 - val_loss: 0.0480 - val_acc: 0.4995\n",
      "Epoch 52/300\n",
      "7997/7997 [==============================] - 0s 51us/step - loss: 0.0474 - acc: 0.5061 - val_loss: 0.0439 - val_acc: 0.5425\n",
      "Epoch 53/300\n",
      "7997/7997 [==============================] - 0s 50us/step - loss: 0.0444 - acc: 0.5372 - val_loss: 0.0404 - val_acc: 0.5786\n",
      "Epoch 54/300\n",
      "7997/7997 [==============================] - 0s 50us/step - loss: 0.0430 - acc: 0.5522 - val_loss: 0.0398 - val_acc: 0.5846\n",
      "Epoch 55/300\n",
      "7997/7997 [==============================] - 0s 52us/step - loss: 0.0424 - acc: 0.5576 - val_loss: 0.0405 - val_acc: 0.5786\n",
      "Epoch 56/300\n",
      "7997/7997 [==============================] - 0s 49us/step - loss: 0.0422 - acc: 0.5600 - val_loss: 0.0399 - val_acc: 0.5836\n",
      "Epoch 57/300\n",
      "7997/7997 [==============================] - 0s 48us/step - loss: 0.0417 - acc: 0.5653 - val_loss: 0.0399 - val_acc: 0.5836\n",
      "Epoch 58/300\n",
      "7997/7997 [==============================] - 0s 52us/step - loss: 0.0411 - acc: 0.5717 - val_loss: 0.0390 - val_acc: 0.5936\n",
      "Epoch 59/300\n",
      "7997/7997 [==============================] - 0s 49us/step - loss: 0.0412 - acc: 0.5707 - val_loss: 0.0401 - val_acc: 0.5826\n",
      "Epoch 60/300\n",
      "7997/7997 [==============================] - 0s 50us/step - loss: 0.0418 - acc: 0.5646 - val_loss: 0.0397 - val_acc: 0.5866\n",
      "Epoch 61/300\n",
      "7997/7997 [==============================] - 0s 50us/step - loss: 0.0411 - acc: 0.5712 - val_loss: 0.0395 - val_acc: 0.5886\n",
      "Epoch 62/300\n",
      "7997/7997 [==============================] - 0s 49us/step - loss: 0.0408 - acc: 0.5743 - val_loss: 0.0399 - val_acc: 0.5846\n",
      "Epoch 63/300\n",
      "7997/7997 [==============================] - 0s 53us/step - loss: 0.0413 - acc: 0.5690 - val_loss: 0.0405 - val_acc: 0.5786\n",
      "Epoch 64/300\n",
      "7997/7997 [==============================] - 0s 53us/step - loss: 0.0405 - acc: 0.5776 - val_loss: 0.0396 - val_acc: 0.5876\n",
      "Epoch 65/300\n",
      "7997/7997 [==============================] - 0s 53us/step - loss: 0.0406 - acc: 0.5767 - val_loss: 0.0394 - val_acc: 0.5896\n",
      "Epoch 66/300\n",
      "7997/7997 [==============================] - 0s 52us/step - loss: 0.0409 - acc: 0.5732 - val_loss: 0.0386 - val_acc: 0.5966\n",
      "Epoch 67/300\n",
      "7997/7997 [==============================] - 0s 50us/step - loss: 0.0404 - acc: 0.5792 - val_loss: 0.0395 - val_acc: 0.5886\n",
      "Epoch 68/300\n",
      "7997/7997 [==============================] - 0s 52us/step - loss: 0.0404 - acc: 0.5787 - val_loss: 0.0392 - val_acc: 0.5916\n",
      "Epoch 69/300\n",
      "7997/7997 [==============================] - 0s 50us/step - loss: 0.0402 - acc: 0.5806 - val_loss: 0.0390 - val_acc: 0.5936\n",
      "Epoch 70/300\n",
      "7997/7997 [==============================] - 0s 53us/step - loss: 0.0405 - acc: 0.5775 - val_loss: 0.0408 - val_acc: 0.5746\n",
      "Epoch 71/300\n",
      "7997/7997 [==============================] - 0s 47us/step - loss: 0.0414 - acc: 0.5688 - val_loss: 0.0399 - val_acc: 0.5846\n",
      "Epoch 72/300\n",
      "7997/7997 [==============================] - 0s 53us/step - loss: 0.0412 - acc: 0.5703 - val_loss: 0.0401 - val_acc: 0.5826\n",
      "Epoch 73/300\n",
      "7997/7997 [==============================] - 0s 53us/step - loss: 0.0407 - acc: 0.5755 - val_loss: 0.0391 - val_acc: 0.5926\n",
      "Epoch 74/300\n",
      "7997/7997 [==============================] - 0s 50us/step - loss: 0.0402 - acc: 0.5806 - val_loss: 0.0398 - val_acc: 0.5856\n",
      "Epoch 75/300\n",
      "7997/7997 [==============================] - 0s 51us/step - loss: 0.0408 - acc: 0.5752 - val_loss: 0.0394 - val_acc: 0.5896\n",
      "Epoch 76/300\n",
      "7997/7997 [==============================] - 0s 49us/step - loss: 0.0404 - acc: 0.5790 - val_loss: 0.0393 - val_acc: 0.5906\n",
      "Epoch 77/300\n",
      "7997/7997 [==============================] - 0s 46us/step - loss: 0.0400 - acc: 0.5835 - val_loss: 0.0395 - val_acc: 0.5876\n",
      "Epoch 78/300\n",
      "7997/7997 [==============================] - 0s 47us/step - loss: 0.0400 - acc: 0.5827 - val_loss: 0.0391 - val_acc: 0.5926\n",
      "Epoch 79/300\n",
      "7997/7997 [==============================] - 0s 46us/step - loss: 0.0401 - acc: 0.5818 - val_loss: 0.0395 - val_acc: 0.5876\n",
      "Epoch 80/300\n",
      "7997/7997 [==============================] - 0s 47us/step - loss: 0.0401 - acc: 0.5825 - val_loss: 0.0393 - val_acc: 0.5906\n",
      "Epoch 81/300\n",
      "7997/7997 [==============================] - 0s 46us/step - loss: 0.0398 - acc: 0.5852 - val_loss: 0.0386 - val_acc: 0.5966\n",
      "Epoch 82/300\n",
      "7997/7997 [==============================] - 0s 46us/step - loss: 0.0402 - acc: 0.5805 - val_loss: 0.0396 - val_acc: 0.5876\n",
      "Epoch 83/300\n",
      "7997/7997 [==============================] - 0s 47us/step - loss: 0.0406 - acc: 0.5773 - val_loss: 0.0392 - val_acc: 0.5916\n",
      "Epoch 84/300\n",
      "7997/7997 [==============================] - 0s 47us/step - loss: 0.0400 - acc: 0.5832 - val_loss: 0.0396 - val_acc: 0.5876\n",
      "Epoch 85/300\n",
      "7997/7997 [==============================] - 0s 48us/step - loss: 0.0402 - acc: 0.5810 - val_loss: 0.0391 - val_acc: 0.5926\n",
      "Epoch 86/300\n",
      "7997/7997 [==============================] - 0s 50us/step - loss: 0.0396 - acc: 0.5870 - val_loss: 0.0390 - val_acc: 0.5936\n",
      "Epoch 87/300\n",
      "7997/7997 [==============================] - 0s 58us/step - loss: 0.0401 - acc: 0.5818 - val_loss: 0.0383 - val_acc: 0.6006\n",
      "Epoch 88/300\n",
      "7997/7997 [==============================] - 0s 54us/step - loss: 0.0400 - acc: 0.5831 - val_loss: 0.0386 - val_acc: 0.5976\n",
      "Epoch 89/300\n",
      "7997/7997 [==============================] - 0s 51us/step - loss: 0.0397 - acc: 0.5862 - val_loss: 0.0387 - val_acc: 0.5966\n",
      "Epoch 90/300\n",
      "7997/7997 [==============================] - 0s 46us/step - loss: 0.0397 - acc: 0.5861 - val_loss: 0.0387 - val_acc: 0.5966\n",
      "Epoch 91/300\n",
      "7997/7997 [==============================] - 0s 56us/step - loss: 0.0400 - acc: 0.5826 - val_loss: 0.0394 - val_acc: 0.5896\n",
      "Epoch 92/300\n",
      "7997/7997 [==============================] - 0s 52us/step - loss: 0.0398 - acc: 0.5855 - val_loss: 0.0392 - val_acc: 0.5916\n",
      "Epoch 93/300\n",
      "7997/7997 [==============================] - 0s 51us/step - loss: 0.0401 - acc: 0.5825 - val_loss: 0.0392 - val_acc: 0.5916\n",
      "Epoch 94/300\n",
      "7997/7997 [==============================] - 0s 50us/step - loss: 0.0398 - acc: 0.5848 - val_loss: 0.0393 - val_acc: 0.5896\n",
      "Epoch 95/300\n",
      "7997/7997 [==============================] - 0s 50us/step - loss: 0.0401 - acc: 0.5825 - val_loss: 0.0397 - val_acc: 0.5866\n",
      "Epoch 96/300\n",
      "7997/7997 [==============================] - 0s 47us/step - loss: 0.0400 - acc: 0.5832 - val_loss: 0.0388 - val_acc: 0.5956\n",
      "Epoch 97/300\n",
      "7997/7997 [==============================] - 0s 49us/step - loss: 0.0397 - acc: 0.5858 - val_loss: 0.0390 - val_acc: 0.5936\n",
      "Epoch 98/300\n",
      "7997/7997 [==============================] - 0s 49us/step - loss: 0.0397 - acc: 0.5860 - val_loss: 0.0393 - val_acc: 0.5906\n",
      "Epoch 99/300\n",
      "7997/7997 [==============================] - 0s 47us/step - loss: 0.0399 - acc: 0.5843 - val_loss: 0.0388 - val_acc: 0.5956\n",
      "Epoch 100/300\n",
      "7997/7997 [==============================] - 0s 49us/step - loss: 0.0395 - acc: 0.5885 - val_loss: 0.0390 - val_acc: 0.5936\n",
      "Epoch 101/300\n",
      "7997/7997 [==============================] - 0s 49us/step - loss: 0.0398 - acc: 0.5852 - val_loss: 0.0397 - val_acc: 0.5856\n",
      "Epoch 102/300\n",
      "7997/7997 [==============================] - 0s 51us/step - loss: 0.0399 - acc: 0.5842 - val_loss: 0.0394 - val_acc: 0.5886\n",
      "Epoch 103/300\n",
      "7997/7997 [==============================] - 0s 47us/step - loss: 0.0396 - acc: 0.5868 - val_loss: 0.0390 - val_acc: 0.5936\n",
      "Epoch 104/300\n",
      "7997/7997 [==============================] - 0s 49us/step - loss: 0.0395 - acc: 0.5886 - val_loss: 0.0390 - val_acc: 0.5926\n",
      "Epoch 105/300\n",
      "7997/7997 [==============================] - 0s 48us/step - loss: 0.0395 - acc: 0.5886 - val_loss: 0.0390 - val_acc: 0.5936\n",
      "Epoch 106/300\n",
      "7997/7997 [==============================] - 0s 47us/step - loss: 0.0395 - acc: 0.5882 - val_loss: 0.0387 - val_acc: 0.5966\n",
      "Epoch 107/300\n",
      "7997/7997 [==============================] - 0s 48us/step - loss: 0.0396 - acc: 0.5875 - val_loss: 0.0388 - val_acc: 0.5966\n",
      "Epoch 108/300\n",
      "7997/7997 [==============================] - 0s 49us/step - loss: 0.0395 - acc: 0.5885 - val_loss: 0.0387 - val_acc: 0.5966\n",
      "Epoch 109/300\n",
      "7997/7997 [==============================] - 0s 46us/step - loss: 0.0395 - acc: 0.5888 - val_loss: 0.0390 - val_acc: 0.5936\n",
      "Epoch 110/300\n",
      "7997/7997 [==============================] - 0s 49us/step - loss: 0.0395 - acc: 0.5885 - val_loss: 0.0388 - val_acc: 0.5956\n",
      "Epoch 111/300\n",
      "7997/7997 [==============================] - 0s 50us/step - loss: 0.0394 - acc: 0.5892 - val_loss: 0.0390 - val_acc: 0.5936\n",
      "Epoch 112/300\n",
      "7997/7997 [==============================] - 0s 49us/step - loss: 0.0396 - acc: 0.5871 - val_loss: 0.0393 - val_acc: 0.5906\n",
      "Epoch 113/300\n",
      "7997/7997 [==============================] - 0s 49us/step - loss: 0.0397 - acc: 0.5861 - val_loss: 0.0394 - val_acc: 0.5896\n",
      "Epoch 114/300\n",
      "7997/7997 [==============================] - 0s 44us/step - loss: 0.0398 - acc: 0.5853 - val_loss: 0.0385 - val_acc: 0.5986\n",
      "Epoch 115/300\n",
      "7997/7997 [==============================] - 0s 48us/step - loss: 0.0394 - acc: 0.5898 - val_loss: 0.0386 - val_acc: 0.5976\n",
      "Epoch 116/300\n",
      "7997/7997 [==============================] - 0s 49us/step - loss: 0.0395 - acc: 0.5885 - val_loss: 0.0390 - val_acc: 0.5936\n",
      "Epoch 117/300\n",
      "7997/7997 [==============================] - 0s 46us/step - loss: 0.0394 - acc: 0.5897 - val_loss: 0.0388 - val_acc: 0.5956\n",
      "Epoch 118/300\n",
      "7997/7997 [==============================] - 0s 48us/step - loss: 0.0396 - acc: 0.5878 - val_loss: 0.0389 - val_acc: 0.5946\n",
      "Epoch 119/300\n",
      "7997/7997 [==============================] - 0s 49us/step - loss: 0.0394 - acc: 0.5895 - val_loss: 0.0388 - val_acc: 0.5956\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 120/300\n",
      "7997/7997 [==============================] - 0s 45us/step - loss: 0.0395 - acc: 0.5883 - val_loss: 0.0392 - val_acc: 0.5916\n",
      "Epoch 121/300\n",
      "7997/7997 [==============================] - 0s 47us/step - loss: 0.0395 - acc: 0.5880 - val_loss: 0.0387 - val_acc: 0.5966\n",
      "Epoch 122/300\n",
      "7997/7997 [==============================] - 0s 48us/step - loss: 0.0395 - acc: 0.5886 - val_loss: 0.0387 - val_acc: 0.5966\n",
      "Epoch 123/300\n",
      "7997/7997 [==============================] - 0s 48us/step - loss: 0.0397 - acc: 0.5861 - val_loss: 0.0386 - val_acc: 0.5976\n",
      "Epoch 124/300\n",
      "7997/7997 [==============================] - 0s 48us/step - loss: 0.0395 - acc: 0.5886 - val_loss: 0.0388 - val_acc: 0.5956\n",
      "Epoch 125/300\n",
      "7997/7997 [==============================] - 0s 52us/step - loss: 0.0394 - acc: 0.5891 - val_loss: 0.0391 - val_acc: 0.5926\n",
      "Epoch 126/300\n",
      "7997/7997 [==============================] - 0s 49us/step - loss: 0.0396 - acc: 0.5876 - val_loss: 0.0388 - val_acc: 0.5956\n",
      "Epoch 127/300\n",
      "7997/7997 [==============================] - 0s 47us/step - loss: 0.0394 - acc: 0.5898 - val_loss: 0.0387 - val_acc: 0.5966\n",
      "Epoch 128/300\n",
      "7997/7997 [==============================] - 0s 48us/step - loss: 0.0395 - acc: 0.5880 - val_loss: 0.0392 - val_acc: 0.5916\n",
      "Epoch 129/300\n",
      "7997/7997 [==============================] - 0s 48us/step - loss: 0.0396 - acc: 0.5871 - val_loss: 0.0392 - val_acc: 0.5916\n",
      "Epoch 130/300\n",
      "7997/7997 [==============================] - 0s 46us/step - loss: 0.0397 - acc: 0.5861 - val_loss: 0.0392 - val_acc: 0.5916\n",
      "Epoch 131/300\n",
      "7997/7997 [==============================] - 0s 48us/step - loss: 0.0396 - acc: 0.5871 - val_loss: 0.0393 - val_acc: 0.5906\n",
      "Epoch 132/300\n",
      "7997/7997 [==============================] - 0s 48us/step - loss: 0.0396 - acc: 0.5871 - val_loss: 0.0391 - val_acc: 0.5926\n",
      "Epoch 133/300\n",
      "7997/7997 [==============================] - 0s 48us/step - loss: 0.0398 - acc: 0.5852 - val_loss: 0.0393 - val_acc: 0.5906\n",
      "Epoch 134/300\n",
      "7997/7997 [==============================] - 0s 48us/step - loss: 0.0397 - acc: 0.5865 - val_loss: 0.0388 - val_acc: 0.5956\n",
      "Epoch 135/300\n",
      "7997/7997 [==============================] - 0s 48us/step - loss: 0.0393 - acc: 0.5901 - val_loss: 0.0391 - val_acc: 0.5926\n",
      "Epoch 136/300\n",
      "7997/7997 [==============================] - 0s 48us/step - loss: 0.0393 - acc: 0.5906 - val_loss: 0.0390 - val_acc: 0.5936\n",
      "Epoch 137/300\n",
      "7997/7997 [==============================] - 0s 47us/step - loss: 0.0392 - acc: 0.5916 - val_loss: 0.0388 - val_acc: 0.5966\n",
      "Epoch 138/300\n",
      "7997/7997 [==============================] - 0s 48us/step - loss: 0.0393 - acc: 0.5902 - val_loss: 0.0390 - val_acc: 0.5936\n",
      "Epoch 139/300\n",
      "7997/7997 [==============================] - 0s 49us/step - loss: 0.0395 - acc: 0.5877 - val_loss: 0.0389 - val_acc: 0.5946\n",
      "Epoch 140/300\n",
      "7997/7997 [==============================] - 0s 48us/step - loss: 0.0394 - acc: 0.5895 - val_loss: 0.0392 - val_acc: 0.5916\n",
      "Epoch 141/300\n",
      "7997/7997 [==============================] - 0s 48us/step - loss: 0.0395 - acc: 0.5883 - val_loss: 0.0391 - val_acc: 0.5926\n",
      "Epoch 142/300\n",
      "7997/7997 [==============================] - 0s 49us/step - loss: 0.0394 - acc: 0.5891 - val_loss: 0.0391 - val_acc: 0.5926\n",
      "Epoch 143/300\n",
      "7997/7997 [==============================] - 0s 48us/step - loss: 0.0394 - acc: 0.5893 - val_loss: 0.0390 - val_acc: 0.5936\n",
      "Epoch 144/300\n",
      "7997/7997 [==============================] - 0s 46us/step - loss: 0.0393 - acc: 0.5910 - val_loss: 0.0389 - val_acc: 0.5946\n",
      "Epoch 145/300\n",
      "7997/7997 [==============================] - 0s 48us/step - loss: 0.0393 - acc: 0.5910 - val_loss: 0.0389 - val_acc: 0.5946\n",
      "Epoch 146/300\n",
      "7997/7997 [==============================] - 0s 48us/step - loss: 0.0393 - acc: 0.5900 - val_loss: 0.0387 - val_acc: 0.5976\n",
      "Epoch 147/300\n",
      "7997/7997 [==============================] - 0s 47us/step - loss: 0.0394 - acc: 0.5897 - val_loss: 0.0387 - val_acc: 0.5966\n",
      "Epoch 148/300\n",
      "7997/7997 [==============================] - 0s 49us/step - loss: 0.0392 - acc: 0.5910 - val_loss: 0.0387 - val_acc: 0.5966\n",
      "Epoch 149/300\n",
      "7997/7997 [==============================] - 0s 49us/step - loss: 0.0392 - acc: 0.5915 - val_loss: 0.0389 - val_acc: 0.5946\n",
      "Epoch 150/300\n",
      "7997/7997 [==============================] - 0s 46us/step - loss: 0.0393 - acc: 0.5906 - val_loss: 0.0388 - val_acc: 0.5956\n",
      "Epoch 151/300\n",
      "7997/7997 [==============================] - 0s 49us/step - loss: 0.0392 - acc: 0.5917 - val_loss: 0.0387 - val_acc: 0.5966\n",
      "Epoch 152/300\n",
      "7997/7997 [==============================] - 0s 49us/step - loss: 0.0393 - acc: 0.5906 - val_loss: 0.0386 - val_acc: 0.5976\n",
      "Epoch 153/300\n",
      "7997/7997 [==============================] - 0s 48us/step - loss: 0.0394 - acc: 0.5892 - val_loss: 0.0385 - val_acc: 0.5986\n",
      "Epoch 154/300\n",
      "7997/7997 [==============================] - 0s 47us/step - loss: 0.0392 - acc: 0.5910 - val_loss: 0.0386 - val_acc: 0.5976\n",
      "Epoch 155/300\n",
      "7997/7997 [==============================] - 0s 48us/step - loss: 0.0394 - acc: 0.5897 - val_loss: 0.0387 - val_acc: 0.5966\n",
      "Epoch 156/300\n",
      "7997/7997 [==============================] - 0s 49us/step - loss: 0.0392 - acc: 0.5915 - val_loss: 0.0386 - val_acc: 0.5976\n",
      "Epoch 157/300\n",
      "7997/7997 [==============================] - 0s 47us/step - loss: 0.0392 - acc: 0.5911 - val_loss: 0.0387 - val_acc: 0.5976\n",
      "Epoch 158/300\n",
      "7997/7997 [==============================] - 0s 46us/step - loss: 0.0393 - acc: 0.5908 - val_loss: 0.0383 - val_acc: 0.6006\n",
      "Epoch 159/300\n",
      "7997/7997 [==============================] - 0s 49us/step - loss: 0.0395 - acc: 0.5887 - val_loss: 0.0388 - val_acc: 0.5956\n",
      "Epoch 160/300\n",
      "7997/7997 [==============================] - 0s 48us/step - loss: 0.0393 - acc: 0.5903 - val_loss: 0.0386 - val_acc: 0.5976\n",
      "Epoch 161/300\n",
      "7997/7997 [==============================] - 0s 46us/step - loss: 0.0393 - acc: 0.5910 - val_loss: 0.0385 - val_acc: 0.5986\n",
      "Epoch 162/300\n",
      "7997/7997 [==============================] - 0s 50us/step - loss: 0.0392 - acc: 0.5917 - val_loss: 0.0383 - val_acc: 0.6006\n",
      "Epoch 163/300\n",
      "7997/7997 [==============================] - 0s 48us/step - loss: 0.0393 - acc: 0.5906 - val_loss: 0.0387 - val_acc: 0.5966\n",
      "Epoch 164/300\n",
      "7997/7997 [==============================] - 0s 47us/step - loss: 0.0394 - acc: 0.5893 - val_loss: 0.0390 - val_acc: 0.5946\n",
      "Epoch 165/300\n",
      "7997/7997 [==============================] - 0s 49us/step - loss: 0.0392 - acc: 0.5913 - val_loss: 0.0387 - val_acc: 0.5966\n",
      "Epoch 166/300\n",
      "7997/7997 [==============================] - 0s 49us/step - loss: 0.0392 - acc: 0.5918 - val_loss: 0.0389 - val_acc: 0.5946\n",
      "Epoch 167/300\n",
      "7997/7997 [==============================] - 0s 48us/step - loss: 0.0392 - acc: 0.5917 - val_loss: 0.0390 - val_acc: 0.5926\n",
      "Epoch 168/300\n",
      "7997/7997 [==============================] - 0s 47us/step - loss: 0.0394 - acc: 0.5896 - val_loss: 0.0391 - val_acc: 0.5926\n",
      "Epoch 169/300\n",
      "7997/7997 [==============================] - 0s 48us/step - loss: 0.0393 - acc: 0.5902 - val_loss: 0.0388 - val_acc: 0.5956\n",
      "Epoch 170/300\n",
      "7997/7997 [==============================] - 0s 53us/step - loss: 0.0392 - acc: 0.5918 - val_loss: 0.0383 - val_acc: 0.6006\n",
      "Epoch 171/300\n",
      "7997/7997 [==============================] - 0s 47us/step - loss: 0.0392 - acc: 0.5911 - val_loss: 0.0382 - val_acc: 0.6016\n",
      "Epoch 172/300\n",
      "7997/7997 [==============================] - 0s 50us/step - loss: 0.0393 - acc: 0.5910 - val_loss: 0.0385 - val_acc: 0.5986\n",
      "Epoch 173/300\n",
      "7997/7997 [==============================] - 0s 48us/step - loss: 0.0394 - acc: 0.5900 - val_loss: 0.0388 - val_acc: 0.5956\n",
      "Epoch 174/300\n",
      "7997/7997 [==============================] - 0s 46us/step - loss: 0.0396 - acc: 0.5873 - val_loss: 0.0395 - val_acc: 0.5886\n",
      "Epoch 175/300\n",
      "7997/7997 [==============================] - 0s 49us/step - loss: 0.0394 - acc: 0.5900 - val_loss: 0.0391 - val_acc: 0.5926\n",
      "Epoch 176/300\n",
      "7997/7997 [==============================] - 0s 49us/step - loss: 0.0392 - acc: 0.5910 - val_loss: 0.0387 - val_acc: 0.5966\n",
      "Epoch 177/300\n",
      "7997/7997 [==============================] - 0s 47us/step - loss: 0.0393 - acc: 0.5901 - val_loss: 0.0390 - val_acc: 0.5936\n",
      "Epoch 178/300\n",
      "7997/7997 [==============================] - 0s 49us/step - loss: 0.0393 - acc: 0.5907 - val_loss: 0.0385 - val_acc: 0.5986\n",
      "Epoch 179/300\n",
      "7997/7997 [==============================] - 0s 49us/step - loss: 0.0394 - acc: 0.5897 - val_loss: 0.0395 - val_acc: 0.5886\n",
      "Epoch 180/300\n",
      "7997/7997 [==============================] - 0s 46us/step - loss: 0.0395 - acc: 0.5888 - val_loss: 0.0391 - val_acc: 0.5926\n",
      "Epoch 181/300\n",
      "7997/7997 [==============================] - 0s 47us/step - loss: 0.0393 - acc: 0.5907 - val_loss: 0.0386 - val_acc: 0.5976\n",
      "Epoch 182/300\n",
      "7997/7997 [==============================] - 0s 47us/step - loss: 0.0394 - acc: 0.5890 - val_loss: 0.0384 - val_acc: 0.5996\n",
      "Epoch 183/300\n",
      "7997/7997 [==============================] - 0s 46us/step - loss: 0.0394 - acc: 0.5893 - val_loss: 0.0386 - val_acc: 0.5976\n",
      "Epoch 184/300\n",
      "7997/7997 [==============================] - 0s 47us/step - loss: 0.0397 - acc: 0.5865 - val_loss: 0.0387 - val_acc: 0.5966\n",
      "Epoch 185/300\n",
      "7997/7997 [==============================] - 0s 46us/step - loss: 0.0396 - acc: 0.5868 - val_loss: 0.0385 - val_acc: 0.5986\n",
      "Epoch 186/300\n",
      "7997/7997 [==============================] - 0s 46us/step - loss: 0.0394 - acc: 0.5896 - val_loss: 0.0384 - val_acc: 0.5996\n",
      "Epoch 187/300\n",
      "7997/7997 [==============================] - 0s 45us/step - loss: 0.0391 - acc: 0.5925 - val_loss: 0.0388 - val_acc: 0.5956\n",
      "Epoch 188/300\n",
      "7997/7997 [==============================] - 0s 46us/step - loss: 0.0391 - acc: 0.5922 - val_loss: 0.0387 - val_acc: 0.5966\n",
      "Epoch 189/300\n",
      "7997/7997 [==============================] - 0s 47us/step - loss: 0.0393 - acc: 0.5905 - val_loss: 0.0388 - val_acc: 0.5956\n",
      "Epoch 190/300\n",
      "7997/7997 [==============================] - 0s 46us/step - loss: 0.0392 - acc: 0.5913 - val_loss: 0.0387 - val_acc: 0.5976\n",
      "Epoch 191/300\n",
      "7997/7997 [==============================] - 0s 46us/step - loss: 0.0392 - acc: 0.5911 - val_loss: 0.0386 - val_acc: 0.5976\n",
      "Epoch 192/300\n",
      "7997/7997 [==============================] - 0s 45us/step - loss: 0.0393 - acc: 0.5906 - val_loss: 0.0387 - val_acc: 0.5966\n",
      "Epoch 193/300\n",
      "7997/7997 [==============================] - 0s 50us/step - loss: 0.0391 - acc: 0.5923 - val_loss: 0.0385 - val_acc: 0.5986\n",
      "Epoch 194/300\n",
      "7997/7997 [==============================] - 0s 47us/step - loss: 0.0391 - acc: 0.5926 - val_loss: 0.0386 - val_acc: 0.5976\n",
      "Epoch 195/300\n",
      "7997/7997 [==============================] - 0s 49us/step - loss: 0.0394 - acc: 0.5895 - val_loss: 0.0389 - val_acc: 0.5946\n",
      "Epoch 196/300\n",
      "7997/7997 [==============================] - 0s 49us/step - loss: 0.0391 - acc: 0.5932 - val_loss: 0.0386 - val_acc: 0.5976\n",
      "Epoch 197/300\n",
      "7997/7997 [==============================] - 0s 49us/step - loss: 0.0391 - acc: 0.5928 - val_loss: 0.0387 - val_acc: 0.5966\n",
      "Epoch 198/300\n",
      "7997/7997 [==============================] - 0s 44us/step - loss: 0.0391 - acc: 0.5930 - val_loss: 0.0387 - val_acc: 0.5966\n",
      "Epoch 199/300\n",
      "7997/7997 [==============================] - 0s 45us/step - loss: 0.0390 - acc: 0.5935 - val_loss: 0.0387 - val_acc: 0.5966\n",
      "Epoch 200/300\n",
      "7997/7997 [==============================] - 0s 44us/step - loss: 0.0392 - acc: 0.5916 - val_loss: 0.0386 - val_acc: 0.5976\n",
      "Epoch 201/300\n",
      "7997/7997 [==============================] - 0s 43us/step - loss: 0.0394 - acc: 0.5897 - val_loss: 0.0384 - val_acc: 0.5996\n",
      "Epoch 202/300\n",
      "7997/7997 [==============================] - 0s 48us/step - loss: 0.0393 - acc: 0.5907 - val_loss: 0.0385 - val_acc: 0.5996\n",
      "Epoch 203/300\n",
      "7997/7997 [==============================] - 0s 44us/step - loss: 0.0392 - acc: 0.5921 - val_loss: 0.0390 - val_acc: 0.5936\n",
      "Epoch 204/300\n",
      "7997/7997 [==============================] - 0s 46us/step - loss: 0.0392 - acc: 0.5917 - val_loss: 0.0390 - val_acc: 0.5936\n",
      "Epoch 205/300\n",
      "7997/7997 [==============================] - 0s 46us/step - loss: 0.0391 - acc: 0.5931 - val_loss: 0.0392 - val_acc: 0.5916\n",
      "Epoch 206/300\n",
      "7997/7997 [==============================] - 0s 44us/step - loss: 0.0390 - acc: 0.5935 - val_loss: 0.0392 - val_acc: 0.5916\n",
      "Epoch 207/300\n",
      "7997/7997 [==============================] - 0s 46us/step - loss: 0.0391 - acc: 0.5928 - val_loss: 0.0389 - val_acc: 0.5946\n",
      "Epoch 208/300\n",
      "7997/7997 [==============================] - 0s 45us/step - loss: 0.0391 - acc: 0.5927 - val_loss: 0.0389 - val_acc: 0.5946\n",
      "Epoch 209/300\n",
      "7997/7997 [==============================] - 0s 44us/step - loss: 0.0394 - acc: 0.5893 - val_loss: 0.0397 - val_acc: 0.5866\n",
      "Epoch 210/300\n",
      "7997/7997 [==============================] - 0s 45us/step - loss: 0.0391 - acc: 0.5922 - val_loss: 0.0388 - val_acc: 0.5956\n",
      "Epoch 211/300\n",
      "7997/7997 [==============================] - 0s 45us/step - loss: 0.0392 - acc: 0.5918 - val_loss: 0.0386 - val_acc: 0.5976\n",
      "Epoch 212/300\n",
      "7997/7997 [==============================] - 0s 45us/step - loss: 0.0398 - acc: 0.5848 - val_loss: 0.0397 - val_acc: 0.5866\n",
      "Epoch 213/300\n",
      "7997/7997 [==============================] - 0s 45us/step - loss: 0.0394 - acc: 0.5890 - val_loss: 0.0387 - val_acc: 0.5966\n",
      "Epoch 214/300\n",
      "7997/7997 [==============================] - 0s 44us/step - loss: 0.0392 - acc: 0.5916 - val_loss: 0.0388 - val_acc: 0.5956\n",
      "Epoch 215/300\n",
      "7997/7997 [==============================] - 0s 47us/step - loss: 0.0392 - acc: 0.5917 - val_loss: 0.0392 - val_acc: 0.5916\n",
      "Epoch 216/300\n",
      "7997/7997 [==============================] - 0s 45us/step - loss: 0.0391 - acc: 0.5922 - val_loss: 0.0394 - val_acc: 0.5896\n",
      "Epoch 217/300\n",
      "7997/7997 [==============================] - 0s 44us/step - loss: 0.0398 - acc: 0.5857 - val_loss: 0.0400 - val_acc: 0.5826\n",
      "Epoch 218/300\n",
      "7997/7997 [==============================] - 0s 44us/step - loss: 0.0394 - acc: 0.5900 - val_loss: 0.0393 - val_acc: 0.5906\n",
      "Epoch 219/300\n",
      "7997/7997 [==============================] - 0s 45us/step - loss: 0.0392 - acc: 0.5920 - val_loss: 0.0389 - val_acc: 0.5946\n",
      "Epoch 220/300\n",
      "7997/7997 [==============================] - 0s 43us/step - loss: 0.0392 - acc: 0.5918 - val_loss: 0.0391 - val_acc: 0.5926\n",
      "Epoch 221/300\n",
      "7997/7997 [==============================] - 0s 45us/step - loss: 0.0393 - acc: 0.5910 - val_loss: 0.0385 - val_acc: 0.5986\n",
      "Epoch 222/300\n",
      "7997/7997 [==============================] - 0s 45us/step - loss: 0.0393 - acc: 0.5907 - val_loss: 0.0388 - val_acc: 0.5956\n",
      "Epoch 223/300\n",
      "7997/7997 [==============================] - 0s 44us/step - loss: 0.0392 - acc: 0.5913 - val_loss: 0.0393 - val_acc: 0.5896\n",
      "Epoch 224/300\n",
      "7997/7997 [==============================] - 0s 44us/step - loss: 0.0393 - acc: 0.5908 - val_loss: 0.0392 - val_acc: 0.5916\n",
      "Epoch 225/300\n",
      "7997/7997 [==============================] - 0s 44us/step - loss: 0.0394 - acc: 0.5900 - val_loss: 0.0394 - val_acc: 0.5896\n",
      "Epoch 226/300\n",
      "7997/7997 [==============================] - 0s 44us/step - loss: 0.0393 - acc: 0.5906 - val_loss: 0.0387 - val_acc: 0.5966\n",
      "Epoch 227/300\n",
      "7997/7997 [==============================] - 0s 44us/step - loss: 0.0390 - acc: 0.5936 - val_loss: 0.0392 - val_acc: 0.5916\n",
      "Epoch 228/300\n",
      "7997/7997 [==============================] - 0s 44us/step - loss: 0.0392 - acc: 0.5922 - val_loss: 0.0388 - val_acc: 0.5956\n",
      "Epoch 229/300\n",
      "7997/7997 [==============================] - 0s 44us/step - loss: 0.0393 - acc: 0.5908 - val_loss: 0.0388 - val_acc: 0.5966\n",
      "Epoch 230/300\n",
      "7997/7997 [==============================] - 0s 44us/step - loss: 0.0391 - acc: 0.5927 - val_loss: 0.0388 - val_acc: 0.5956\n",
      "Epoch 231/300\n",
      "7997/7997 [==============================] - 0s 43us/step - loss: 0.0391 - acc: 0.5922 - val_loss: 0.0386 - val_acc: 0.5976\n",
      "Epoch 232/300\n",
      "7997/7997 [==============================] - 0s 43us/step - loss: 0.0396 - acc: 0.5877 - val_loss: 0.0389 - val_acc: 0.5946\n",
      "Epoch 233/300\n",
      "7997/7997 [==============================] - 0s 44us/step - loss: 0.0393 - acc: 0.5911 - val_loss: 0.0384 - val_acc: 0.5996\n",
      "Epoch 234/300\n",
      "7997/7997 [==============================] - 0s 43us/step - loss: 0.0390 - acc: 0.5932 - val_loss: 0.0389 - val_acc: 0.5946\n",
      "Epoch 235/300\n",
      "7997/7997 [==============================] - 0s 43us/step - loss: 0.0391 - acc: 0.5923 - val_loss: 0.0391 - val_acc: 0.5926\n",
      "Epoch 236/300\n",
      "7997/7997 [==============================] - 0s 44us/step - loss: 0.0392 - acc: 0.5911 - val_loss: 0.0392 - val_acc: 0.5916\n",
      "Epoch 237/300\n",
      "7997/7997 [==============================] - 0s 43us/step - loss: 0.0392 - acc: 0.5912 - val_loss: 0.0385 - val_acc: 0.5976\n",
      "Epoch 238/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7997/7997 [==============================] - 0s 44us/step - loss: 0.0390 - acc: 0.5931 - val_loss: 0.0388 - val_acc: 0.5956\n",
      "Epoch 239/300\n",
      "7997/7997 [==============================] - 0s 45us/step - loss: 0.0391 - acc: 0.5928 - val_loss: 0.0388 - val_acc: 0.5956\n",
      "Epoch 240/300\n",
      "7997/7997 [==============================] - 0s 49us/step - loss: 0.0392 - acc: 0.5917 - val_loss: 0.0392 - val_acc: 0.5926\n",
      "Epoch 241/300\n",
      "7997/7997 [==============================] - 0s 51us/step - loss: 0.0392 - acc: 0.5912 - val_loss: 0.0383 - val_acc: 0.6016\n",
      "Epoch 242/300\n",
      "7997/7997 [==============================] - 0s 47us/step - loss: 0.0392 - acc: 0.5918 - val_loss: 0.0385 - val_acc: 0.5986\n",
      "Epoch 243/300\n",
      "7997/7997 [==============================] - 0s 44us/step - loss: 0.0391 - acc: 0.5923 - val_loss: 0.0385 - val_acc: 0.5986\n",
      "Epoch 244/300\n",
      "7997/7997 [==============================] - 0s 45us/step - loss: 0.0391 - acc: 0.5922 - val_loss: 0.0384 - val_acc: 0.5996\n",
      "Epoch 245/300\n",
      "7997/7997 [==============================] - 0s 45us/step - loss: 0.0391 - acc: 0.5928 - val_loss: 0.0383 - val_acc: 0.6006\n",
      "Epoch 246/300\n",
      "7997/7997 [==============================] - 0s 48us/step - loss: 0.0391 - acc: 0.5931 - val_loss: 0.0386 - val_acc: 0.5986\n",
      "Epoch 247/300\n",
      "7997/7997 [==============================] - 0s 45us/step - loss: 0.0391 - acc: 0.5926 - val_loss: 0.0389 - val_acc: 0.5946\n",
      "Epoch 248/300\n",
      "7997/7997 [==============================] - 0s 45us/step - loss: 0.0392 - acc: 0.5921 - val_loss: 0.0395 - val_acc: 0.5886\n",
      "Epoch 249/300\n",
      "7997/7997 [==============================] - 0s 45us/step - loss: 0.0393 - acc: 0.5901 - val_loss: 0.0396 - val_acc: 0.5866\n",
      "Epoch 250/300\n",
      "7997/7997 [==============================] - 0s 45us/step - loss: 0.0393 - acc: 0.5908 - val_loss: 0.0392 - val_acc: 0.5916\n",
      "Epoch 251/300\n",
      "7997/7997 [==============================] - 0s 52us/step - loss: 0.0390 - acc: 0.5932 - val_loss: 0.0390 - val_acc: 0.5936\n",
      "Epoch 252/300\n",
      "7997/7997 [==============================] - 0s 50us/step - loss: 0.0393 - acc: 0.5911 - val_loss: 0.0394 - val_acc: 0.5896\n",
      "Epoch 253/300\n",
      "7997/7997 [==============================] - 0s 47us/step - loss: 0.0391 - acc: 0.5925 - val_loss: 0.0393 - val_acc: 0.5906\n",
      "Epoch 254/300\n",
      "7997/7997 [==============================] - 0s 45us/step - loss: 0.0391 - acc: 0.5930 - val_loss: 0.0387 - val_acc: 0.5956\n",
      "Epoch 255/300\n",
      "7997/7997 [==============================] - 0s 45us/step - loss: 0.0392 - acc: 0.5917 - val_loss: 0.0387 - val_acc: 0.5966\n",
      "Epoch 256/300\n",
      "7997/7997 [==============================] - 0s 44us/step - loss: 0.0392 - acc: 0.5917 - val_loss: 0.0392 - val_acc: 0.5916\n",
      "Epoch 257/300\n",
      "7997/7997 [==============================] - 0s 47us/step - loss: 0.0392 - acc: 0.5918 - val_loss: 0.0392 - val_acc: 0.5916\n",
      "Epoch 258/300\n",
      "7997/7997 [==============================] - 0s 49us/step - loss: 0.0391 - acc: 0.5922 - val_loss: 0.0390 - val_acc: 0.5936\n",
      "Epoch 259/300\n",
      "7997/7997 [==============================] - 0s 46us/step - loss: 0.0391 - acc: 0.5922 - val_loss: 0.0389 - val_acc: 0.5946\n",
      "Epoch 260/300\n",
      "7997/7997 [==============================] - 0s 50us/step - loss: 0.0391 - acc: 0.5926 - val_loss: 0.0389 - val_acc: 0.5946\n",
      "Epoch 261/300\n",
      "7997/7997 [==============================] - 0s 45us/step - loss: 0.0392 - acc: 0.5918 - val_loss: 0.0388 - val_acc: 0.5956\n",
      "Epoch 262/300\n",
      "7997/7997 [==============================] - 0s 44us/step - loss: 0.0392 - acc: 0.5921 - val_loss: 0.0393 - val_acc: 0.5906\n",
      "Epoch 263/300\n",
      "7997/7997 [==============================] - 0s 45us/step - loss: 0.0395 - acc: 0.5888 - val_loss: 0.0394 - val_acc: 0.5896\n",
      "Epoch 264/300\n",
      "7997/7997 [==============================] - 0s 44us/step - loss: 0.0392 - acc: 0.5918 - val_loss: 0.0388 - val_acc: 0.5956\n",
      "Epoch 265/300\n",
      "7997/7997 [==============================] - 0s 44us/step - loss: 0.0391 - acc: 0.5930 - val_loss: 0.0386 - val_acc: 0.5976\n",
      "Epoch 266/300\n",
      "7997/7997 [==============================] - 0s 45us/step - loss: 0.0389 - acc: 0.5941 - val_loss: 0.0386 - val_acc: 0.5986\n",
      "Epoch 267/300\n",
      "7997/7997 [==============================] - 0s 43us/step - loss: 0.0391 - acc: 0.5923 - val_loss: 0.0390 - val_acc: 0.5936\n",
      "Epoch 268/300\n",
      "7997/7997 [==============================] - 0s 43us/step - loss: 0.0390 - acc: 0.5936 - val_loss: 0.0391 - val_acc: 0.5936\n",
      "Epoch 269/300\n",
      "7997/7997 [==============================] - 0s 44us/step - loss: 0.0390 - acc: 0.5936 - val_loss: 0.0384 - val_acc: 0.5996\n",
      "Epoch 270/300\n",
      "7997/7997 [==============================] - 0s 43us/step - loss: 0.0391 - acc: 0.5931 - val_loss: 0.0387 - val_acc: 0.5976\n",
      "Epoch 271/300\n",
      "7997/7997 [==============================] - 0s 44us/step - loss: 0.0393 - acc: 0.5905 - val_loss: 0.0393 - val_acc: 0.5906\n",
      "Epoch 272/300\n",
      "7997/7997 [==============================] - 0s 43us/step - loss: 0.0393 - acc: 0.5906 - val_loss: 0.0389 - val_acc: 0.5946\n",
      "Epoch 273/300\n",
      "7997/7997 [==============================] - 0s 44us/step - loss: 0.0391 - acc: 0.5930 - val_loss: 0.0386 - val_acc: 0.5976\n",
      "Epoch 274/300\n",
      "7997/7997 [==============================] - 0s 45us/step - loss: 0.0392 - acc: 0.5918 - val_loss: 0.0387 - val_acc: 0.5966\n",
      "Epoch 275/300\n",
      "7997/7997 [==============================] - 0s 43us/step - loss: 0.0392 - acc: 0.5913 - val_loss: 0.0393 - val_acc: 0.5906\n",
      "Epoch 276/300\n",
      "7997/7997 [==============================] - 0s 43us/step - loss: 0.0390 - acc: 0.5933 - val_loss: 0.0385 - val_acc: 0.5996\n",
      "Epoch 277/300\n",
      "7997/7997 [==============================] - 0s 44us/step - loss: 0.0391 - acc: 0.5923 - val_loss: 0.0385 - val_acc: 0.5986\n",
      "Epoch 278/300\n",
      "7997/7997 [==============================] - 0s 43us/step - loss: 0.0390 - acc: 0.5936 - val_loss: 0.0384 - val_acc: 0.5996\n",
      "Epoch 279/300\n",
      "7997/7997 [==============================] - 0s 44us/step - loss: 0.0390 - acc: 0.5932 - val_loss: 0.0396 - val_acc: 0.5876\n",
      "Epoch 280/300\n",
      "7997/7997 [==============================] - 0s 44us/step - loss: 0.0393 - acc: 0.5905 - val_loss: 0.0388 - val_acc: 0.5956\n",
      "Epoch 281/300\n",
      "7997/7997 [==============================] - 0s 45us/step - loss: 0.0392 - acc: 0.5911 - val_loss: 0.0389 - val_acc: 0.5946\n",
      "Epoch 282/300\n",
      "7997/7997 [==============================] - 0s 44us/step - loss: 0.0390 - acc: 0.5936 - val_loss: 0.0384 - val_acc: 0.6006\n",
      "Epoch 283/300\n",
      "7997/7997 [==============================] - 0s 45us/step - loss: 0.0389 - acc: 0.5942 - val_loss: 0.0382 - val_acc: 0.6016\n",
      "Epoch 284/300\n",
      "7997/7997 [==============================] - 0s 43us/step - loss: 0.0389 - acc: 0.5943 - val_loss: 0.0384 - val_acc: 0.5996\n",
      "Epoch 285/300\n",
      "7997/7997 [==============================] - 0s 45us/step - loss: 0.0390 - acc: 0.5933 - val_loss: 0.0392 - val_acc: 0.5916\n",
      "Epoch 286/300\n",
      "7997/7997 [==============================] - 0s 44us/step - loss: 0.0392 - acc: 0.5917 - val_loss: 0.0384 - val_acc: 0.5996\n",
      "Epoch 287/300\n",
      "7997/7997 [==============================] - 0s 44us/step - loss: 0.0389 - acc: 0.5948 - val_loss: 0.0383 - val_acc: 0.6006\n",
      "Epoch 288/300\n",
      "7997/7997 [==============================] - 0s 45us/step - loss: 0.0388 - acc: 0.5953 - val_loss: 0.0385 - val_acc: 0.5986\n",
      "Epoch 289/300\n",
      "7997/7997 [==============================] - 0s 45us/step - loss: 0.0391 - acc: 0.5925 - val_loss: 0.0383 - val_acc: 0.6006\n",
      "Epoch 290/300\n",
      "7997/7997 [==============================] - 0s 45us/step - loss: 0.0393 - acc: 0.5908 - val_loss: 0.0386 - val_acc: 0.5976\n",
      "Epoch 291/300\n",
      "7997/7997 [==============================] - 0s 45us/step - loss: 0.0392 - acc: 0.5918 - val_loss: 0.0386 - val_acc: 0.5976\n",
      "Epoch 292/300\n",
      "7997/7997 [==============================] - 0s 45us/step - loss: 0.0388 - acc: 0.5951 - val_loss: 0.0385 - val_acc: 0.5996\n",
      "Epoch 293/300\n",
      "7997/7997 [==============================] - 0s 44us/step - loss: 0.0389 - acc: 0.5946 - val_loss: 0.0385 - val_acc: 0.5996\n",
      "Epoch 294/300\n",
      "7997/7997 [==============================] - 0s 44us/step - loss: 0.0389 - acc: 0.5950 - val_loss: 0.0386 - val_acc: 0.5986\n",
      "Epoch 295/300\n",
      "7997/7997 [==============================] - 0s 44us/step - loss: 0.0389 - acc: 0.5946 - val_loss: 0.0386 - val_acc: 0.5976\n",
      "Epoch 296/300\n",
      "7997/7997 [==============================] - 0s 45us/step - loss: 0.0389 - acc: 0.5947 - val_loss: 0.0386 - val_acc: 0.5976\n",
      "Epoch 297/300\n",
      "7997/7997 [==============================] - 0s 45us/step - loss: 0.0389 - acc: 0.5945 - val_loss: 0.0384 - val_acc: 0.5996\n",
      "Epoch 298/300\n",
      "7997/7997 [==============================] - 0s 43us/step - loss: 0.0389 - acc: 0.5946 - val_loss: 0.0384 - val_acc: 0.6006\n",
      "Epoch 299/300\n",
      "7997/7997 [==============================] - 0s 43us/step - loss: 0.0389 - acc: 0.5950 - val_loss: 0.0382 - val_acc: 0.6026\n",
      "Epoch 300/300\n",
      "7997/7997 [==============================] - 0s 43us/step - loss: 0.0390 - acc: 0.5937 - val_loss: 0.0386 - val_acc: 0.5976\n"
     ]
    }
   ],
   "source": [
    "ht_1 = model_1.fit(train_X, train_Y, validation_data=(val_X, val_Y), epochs = 300, batch_size = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train accuracy: 0.593722647289\n",
      "val accuracy: 0.597597599925\n",
      "1004/1004 [==============================] - 0s 86us/step\n",
      "test accuracy: 0.574701194982\n"
     ]
    }
   ],
   "source": [
    "print(\"train accuracy: \" + str(ht_1.history['acc'][-1]))\n",
    "print(\"val accuracy: \" + str(ht_1.history['val_acc'][-1]))\n",
    "print(\"test accuracy: \" + str(model_1.evaluate(test_X, test_Y)[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYsAAAEKCAYAAADjDHn2AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvhp/UCwAAIABJREFUeJzt3Xl8VNX5x/HPkxAIssiqLAEBRRFcEKN1qYpbBbWgVRSrFa2K+4L1V7UquFRL3bq5W62gtqi4QBVFVMTauqHgBqIIIhFQViGBJCT5/v44dzKTMMkEyJAQnvfrNa+599xz75w7d+Y+95xzF5OEc845V52Mui6Ac865+s+DhXPOuZQ8WDjnnEvJg4VzzrmUPFg455xLyYOFc865lDxYOOecS8mDhXPOuZQ8WDjnnEupUV0XoLa0a9dO3bp1q+tiOOfcVuXDDz9cJql9qnwNJlh069aN6dOn13UxnHNuq2JmC2qSz5uhnHPOpeTBwjnnXEoeLJxzzqXUYPosklm/fj15eXkUFhbWdVHSLjs7m5ycHLKysuq6KM65BqhBB4u8vDxatGhBt27dMLO6Lk7aSGL58uXk5eXRvXv3ui6Oc64BatDNUIWFhbRt27ZBBwoAM6Nt27bbRA3KOVc30hoszGyAmc0xs7lmdk0VeU4xs1lm9rmZ/TMhfZiZfRW9hm1GGTZ11q3KtrKezrm6kbZmKDPLBO4FjgbygA/MbKKkWQl5egLXAgdLWmlmO0TpbYBRQC4g4MNo3pXpKq9zzm1t/vtfyMiAAw9M/2els2axPzBX0jxJxcA4YHClPOcB98aCgKQfovRjgCmSVkTTpgAD0ljWtFm1ahX33XffRs937LHHsmrVqjSUaMtasAAmTqzrUtRf69bBY4/B+vVhvKgI/vnPkF6Vv/0NcnLggQe2SBG3OmefDaecAlJ4vfoqTJ1a16Xa0H33QefOMHJkKGdRETzxBHz1Fbz0EsybV/W8EowdC/37w4knxn8/aSUpLS/gZODvCeO/Au6plOcF4Hbgv8C7wIAo/Srg+oR8NwBXVfd5++67ryqbNWvWBmlb2vz589WnT58N0ktKSmr9s+rD+lb2q1+Fv+xrr22Zz3v5Zemrr7bMZ22shQulceOk1avjaVdcEb6fRx+VVqyQfv7zMD5qVPJlfPhhmN6qldS0afJ1festacGCMLxypXTEEdJZZ0nr10tvvikdeKB0/vnJlz9rlvTvf0ulpcmn33KL1KKF1KePVFhY41WvUlmZ9MwzFb+T6pSWhvX505+STy8piYUI6bLLpJ/8JAxnZITPqS3ffReWV1Ymff55+D4mT94w39KlUu/e0i9+Ie2yi/T22yF99uxQrq5dw/s//iHts08YbtEivO+6a/icQw+VtttOOvzwsG3z86Wjjqo4//PPb/q6ANNVk316TTJtygsYkiRY/K1SnheB54EsoDuhuaoV8H9JgsVvknzGcGA6ML1r164bfAn1Yed56qmnKjs7W3vvvbdyc3PVv39/nXbaadp9990lSYMHD1a/fv3Uu3dvPfjgg+Xz7bTTTlq6dKnmz5+vXr166dxzz1Xv3r119NFHa+3atUk/a3PX9733pEWLNm3eb76RPvqoYlppqbTDDuFX1qGDdMcdYWe+qUpKwh/0/vulsWOldesqTl+wIL6jePrpDeefOjXsZHJzpSuvDH/0sjJp2jTp669Tf/7ChRuuY2Vjxkh77imNHh12umVl4Q9/6aVS48ahbG3bSmeeKXXsGC/vzjtL228vmcWHly8Py1izJr78Cy4IQeLzz0Oeww6L79jff1/64gupSRPpjDNC2tFHS5mZ4TMOOEDKygrzg/TEE2Hndc45oUwHHBCCEEj9+0u33Ra+q7vvDt/50KFhWm5ueH/xxaq/h+eek/bYQ1q8WJowQfrxx+T53norLCtW3pjXXgvzrF8ffjP//a90yCEhH4Tvbv36kPfss0NgKC2VPvkk/p3Gdqb33x9ft1Wrqi7zV19J//lPtZtXUgj42dlh+RMmSL/5TRjOzg7LmDUrBI+rr5b+9a+K5bn++rCMW24J4wsXhu29665hfMQIqUuXsF0h/GYaN5YuvDD8h0Bq1iwEv3vuCQG7Uyfp2GNTl7sq9SFYHAhMThi/Fri2Up4HgLMSxl8H9gNOAx5MSH8QOK26z0tZs7j88rAFavN1+eUpN0RizWLq1KnabrvtNG/evPLpy5cvlyStXbtWffr00bJlyyRVDBaZmZmaMWOGJGnIkCF6/PHHk37W5gSLdevCj7Dyn7amjjwy/GjLyuJpsaPg3/wmftRkFnYgiWbMCF/nrFnSxx9LP/1pmLeyO++s+Mc78cSwE4gdMT71VHxat25h5zJsmHTQQSFIxHbWO+4Y3q+7TjrhhPgOMJmbbgo7o4KC8KfOzg47r8rlW7BAeuQRqXnz8IqV4+ijwzyZmdK550qTJkkDBoTv4cQTQw3ipptC3j59wvrPmBF2BrFyHnRQOPJetiwcdZ55ZvjMRx5R+VHpvHlSo0bxnVivXvEd5+23Sw8/LLVvL+2/f8gbO3qNvYYOlbp3D6/f/z6enrgu220Xdnbr1oVAddZZ8fVfty58/y++GAJqbJ5YzTIzU/ryy3j+srIQKIYPj+d9/fUwbf78MD58ePx3k/gyC+8vvFDxAGHHHcNvEKQ5c0KtKlaBj5XplluSb+cxY0IZMzOlvLyQtmhROCi5/PJwdL9mTQhamZnht9Vhx1Idf7y0007SvvuG5d92W6jNxAJyhw4heBcVSbvtJg0cGGoIse2q+fN1zs+/L1+Hr+9+QSWDTpTGj9fYsdLFF0vTRjwvjRyptb+7RX89frIuPvQTvXTFqyES/f73um/Yu/rT3aUV/nsboz4Ei0bAvKjG0Bj4GOhTKc8AYEw03A5YCLQF2gDzgdbRaz7QprrP21qCRf/+/StMHzVqlPbaay/ttddeatmypd555x1JFYPFLrvsUp5/9OjRuqWKX3x1waK4ODRxxKrBlb36avg1dO4cdoxDh4YdTHw94n+imLy8cET+ww9h5wahhiGFtEGDQtqSJWHnMHVqGB8zpuJyRowI6W3ahB1pbPj778PnHnJIaK7Jzg7LXLw4HEnG/mCZmaH8I0aEPOPHx3dujRvHj6whrH9paQgisbSDDw7vUTyWFNZjxoywDJD23lvlR3UQdgCvvRZ2kBddFI7YY+X+5ptQ9vPOC9/LWWdtWHNJbIVcsybsoJcsiac98UTY+f/iF/Gd0+DBIe2TT0KesjKpZ8+wAxo+PL4NYjvUs88O6x8df6i4OH40/sKjy/WXEfN175+K9I9/xMtUtCJfmjJFpw1cqZ12XKvlv7pcS35zuxafdLHyf3le2AMrBKzGjct0WL/VWnjCJfp1x0nln33yyWXlw7HglZkpnXFGWWiXkfTss/GyDhwotW4dD4IPPFAxONzzlxKNPOsbTbj1U+24Y5lGXFGmLtuvUk6r1brgnCKBNGrwDPXvOrd8nrIyhb39zJmhmnn11Tp2lznq0KZQJfnrVFYmjf71HJ2970yVLluhnjuu0q47riwPKI88UKzsJqXRdxnW5+F7i3T22VLL5iVafczJupbbyj/vH51+p73afKtmzULeJ06fpF27h7Id2O5LafRondjmTWVQUj7PQ91vk0BjOUMg5bRYqTIIkaZ58xDV3357wyiZ7HXUUVW3HaZQ58EilIFjgS+Br4HrorSbgUHRsAF3A7OAT4GhCfP+Gpgbvc5O9VlbQ5/F1KlTddxxx5VPmzp1qg4++GAVFBRIkg477DBNnTpVUsVgkdjncccdd2hUFQ3a1a1v7Cj/iiuST7/yyvjvbrfdwvsee8Sn77NPOLqSwm944UJpv/3CUdX998fn/fnPQ3v4kCFhBzpyZHwZpaXh6C8rKyzvmGPibbLNmsV3LAcdFN7Hjq1YrkMPjddKCgpCzWD6dGn33UPVvVevsOMvKwtHeO+9F45433gjzH/AAfGyrFoVyj5wYGjuyc4OTRt77hkCZaw5BqSePcvUrGmJ7jzgGb1y1B363WVr1Lp1fHpWlnTBqSv06UX3afXdD4e9/+zZKrv4Eq06/vTwpR90kPTPf4Yocu+90pQpIfKde24YLywMBb7ssvDl3Hmn8uculoYM0VMtzlEm68OO7LwFIXIdd5x03XW67NQlatJEysoq04VHfKHbz52ja079urxspw1YHo/glVcewvvVV0u//GXYS0btIWVNt9O6Tj3iEShxw82cqc/6X6xfMUYtWaU2tlwgnc7jYZ9GqfrwqXpmhJ13315rdVXHx5VBid5jP5W9OU179/hRbW2ZMijR8/3/rLP2+0zbZ+XruJ5fyKxMTTKLQyDnPyrbpWf5l50/4nqVjLhKM9lLbVkqkHZv/q0EWmvbqTGFGtRqWjy6x16NG2ucDRVIb3U8RX878/3ySb9tdFc4iOFXOiLzTTVjTdj/8qpmsLeW0Ua9+Uz72Ay1brpWZ9gTUsuWWtzvWJ3NIzq//bNaM3CIfmu3hwOL7eepDPS7jD8IpBHcJYGua3WPQMqgRIsO/IXKshpLd92lBfe/VP79aeedQ1tiy5ahetyli5STEwJfQUHYdosXx9u7fvwxdHjdfXeV//1U6kWw2JKv+hosli1bplh/SuVg8cILL+j444+XJM2ePVtNmjRJW7B46KGwtY84Iox//XU40pTiTSzdu1f8f/XtG5+ekRGODq+6Kgwn7ixbtw6dd7Gqd+wgaMSIDctx0klhep8+4eBpl13C+OWXh2amjIwQ2Fq3KtUZZ4S29JNOKtPyV95X2Wuvh+hSVhYO+8eMkSZM0Dtn3ld+9HflpcXhMPySS0J9/667pAsu0M3HvqO3Jq8N7TyrVklffaX80X9TyStTpBNO0H0HP6ETB5dq0KAQuLp2Dd9Jbq607vRztJrm4TA9O1vKydErFzyvK/d6VdN//7J+OPu3FdtrsrNDFaBp03jHRGznnJivXbvwgnijdZMm8R1dVlZY1llnafGeR2uuRV9W165hR5KZqUkMCFkp0gK6SKBFdCj/iBkZ/cIGGj483pFz5plhY/7lLyEImMV7Slu3Du1asSrU5Mnx6shLL8UjepMm0mWX6f1z7tcpJxbpssukdcvy1a1xnkA6p+1zOolnBNKvtx+vH9r2UrfmS9Wa5Rq3160C6ZGckVr9q4ukZs30IsdW+O39ijG6Ovc1fXTrpFCmYcNCNSv2+RdeqO8mfKBrOj6ml1ufFn6YBQXKv/XPWndA/9DpMnp0qGb+5z9SSYlW5/2o7MYlurjxg8rhWx2W/Y66tgiBrl2rYhU8+7LeGfwHnbTzDI34yX+17uHHQ+fLxIm6Z8DE8rK90P2KEPTXrw8dRVH7z38vfypMZ5B0/fX6ZNidyqBE//7Dp9Inn+jJx0IA3JcPwraNVekk3TxqvT645LF49fb990M1f489pP/9r8r/dW3wYKH6ESwk6bTTTlOfPn2Um5tbIVgUFhZqwIAB2nPPPXXyySentWZxwQXx/dO//hV2yhddFKYdf3wYnzAhvPfoEQ54d9ghTH/nnYpB5PDDQ74mTcLOHEJb9SGHhOHttw/vs6YuCW1PMcXF+vTTEBjWrIkf8UMoU1lZ1NT15JMaxITyaZOPvqNiAWI73oTX+zser3NOXK5Zo8bF05s02SBfOPTbu+JOO/baf3/pxRe1cswErbrmDyp8+Q2tnvJu+c5JixeHqkxshxV7tWgRqkl5eeGPfeWVoaMmtkNZsiRUq+66K0TgRx8NtY1Yu9Pdd4flXHZZCGRlZWFjHHZYaF+TQvqvfx2O/qN+Lq1erYITT1dLVumCjAdCb3jUcQDSbhlzwkbZa694h8aoUeGzYr+h77+P1zyWLYv3po8fH2oclRvC//OfsG4J/W6Jhh0Xjvb/fudK3dTqboF0LxdKU6boiy/CR3ciBJSln0bVxPXrVbRgsa4cUaZXxy3XpYfO0Oz73og3q8Ta7KZPDwvo3DmcErQJQrNeOLAY/2ShXn45/A9SnUFXWiqNOXuqLmz8sArfm1llvu+mfB6O+COLF8e/wli/yZVnLa++pz1m/foNv/808GCh+hMstpTK6xs7CJdCk1Hi/q1p03CgHPsB33BDyBer7d54Y0hfsiScxRTV5NW4cdgn3nNP2Mc99ph0882SZs7UcyNnaNSocBrftddKOuWUsJNaskT64x/DEWzv3qGXcORIac0anXJUOLL79ul3QnvSbrtJXbvqD1wdmo52/Dq0415/fYguo0eHXuM//zmcEjRtWjgftHXrsPzmzaV+/UJb9YoVoT3qrbdC7+nNN0u/+11YmfbtQy1jxIhQ7XriiQ2DUEZGiK477ljxlKQXXww72/nzw+fHjrw3RywAbKw1a7TwyGEq+vvYCslLlkgFKwrDXq6sLOy1YpH9oIPi1cpa9uSTYTN88YX0ynF/FUgfHXBh+fQ9Ooft3afNd5v2AbfcEn4Hm+i770I/T6dOodN5o23G91ZcHA7C6ttuyYOFtu1gMWdO2E8/8kjoQ8jKip+xAWF/3ahRaKOHDU8ZjDVbxfYvEJqJ/vznKMOECdKtt4Z/33ffhYzZ2WGnN29e6MGMNWWcfXYowMEHhz1JrMe5UyeVkKHZF/8tVGO6dy/vMPn2rBt0YsbzWkjnqi8KSLR8eYhwe+5Z/TmdUmhaiPUSJyouDudFjh8fTj8aNCh0dMys+khyqxI7N7Q2LpCoQmlpCBSSVDb5VX1hvSr8uK66tFAgXTRs02oGtWHNmg3PyNuWebDQth0sYudxxzpq99wznLYJ8Y7q2JlHjRqF2kSil54vqnCQffDBCr3FhYXS2rXxKJKZGTrjYk0+d90VmmQSOzQgBIMffginBkE8cvXoES/oZ5+FDrvYBQrffis9+OCGF1S4rUfsVKxI7Iy4Z5+tm+K4DXmw0LYdLBJPBsnJqdjsG7um7+GH4/vtcm+9JY0dq5ltDi+f/447or6EQw8NZ+HEZnzssdCuPXy49O67IaLk5IQA0rx5aNZ5443Q2Ry72q+wMASFNWvCnmP16nCkX9NLeN1W73//2yJN8a6GPFho2w0WCxeGLfvLX4b3xNNXEy1dGmoVlx75WWhs/vHH8gsLfmiSUx4s8vIUv8IrduZM374b/uNjh40QolJiO79zrl6qabBo0A8/2lZ9/314P/VUOPNMOPTQ5PnatYO3XlpDz2MOg9eXQ69esHYt3HMP7U4eQlaHYlpnFdCpU2u46x/QqBGUlMC338Jf/wqVb4vevz9cdBG89x7067fhdOfcVsuDRQO0enV4b9ky7L+TKiiA447jwG7dgOWw++4weza0bw/nn481akSXlkvpVfAhtu5QePxxOOEEWLUq3B5z0KDky73nnvDugcK5BsWDRT3TvHlz8vPzN2sZicGinAQPPgh9+sAhh8CNN8K0aeGVkQGTJ8N++4XqSKPws3jiyhm0vfEyuHUILFsGv/41HHFEWF5VwcCDhHMNkgeLBihpsJgxAy68MAz/+c/h1aEDLFkCe+8NXbrAnDnQtGn5LAee3Qtu/Apuuy3ceP9nP4PMzC23Is65eqNBP4O7Prj66qsrPPzoxhtv5KabbuLII4+kX79+7LnnnkyYMKFWPzNpsIh9Rrt2cM01oe/h4YchKyveqbH99tC4cXyeLl3i43fc4YHCuW3YNlOzuOIKmDmzdpfZt284QK/O0KFDueKKK7jooosAePrpp3nllVcYMWIELVu2ZNmyZRxwwAEMGjSo1p6jXWWw+OlP4fDD4ZZbYIcd4Nhj4Z13oHv35AsyC53VWVmh+co5t83aZoJFXdlnn3344YcfWLRoEUuXLqV169Z07NiRESNG8NZbb5GRkcF3333H999/T4cOHWrlM1evDvv3Jk2ihK+/ho8/DrWDo48OweK440Jfxb77Vr+wvn1rpUzOua3bNhMsUtUA0unkk09m/PjxLFmyhKFDh/Lkk0+ydOlSPvzwQ7KysujWrRuFhYW19nmrV4daRXlF5fHHw8jQoaHv4Y9/rPpsJuecS2KbCRZ1aejQoZx33nksW7aMadOm8fTTT7PDDjuQlZXF1KlTWbBgQa1+XixYAFBWFoLFEUdATk5I++1va/XznHMNnweLLaBPnz6sWbOGzp0707FjR04//XR+/vOfk5ubS9++fenVq1etfl6FYHHDDTBvHvzhD7X6Gc65bYsHiy3k008/LR9u164d77zzTtJ8m3uNBSQEi1mzwmmv55wDQ4Zs9nKdc9uutJ46a2YDzGyOmc01s2uSTD/LzJaa2czodW7CtNKE9InpLGdDUx4sxo4Np7vedptfLOec2yxpq1mYWSZwL3A0kAd8YGYTJc2qlPUpSZckWcQ6SX4qziZYvRp227UMnngCBg4Mp8k659xmSGfNYn9grqR5koqBccDgNH5eUuGmig1f4nquXg0tS1fBd9/BySfXYamccw1FOoNFZ2BhwnhelFbZSWb2iZmNN7MuCenZZjbdzN41sxOSfYCZDY/yTF+6dOkG07Ozs1m+fHmDDxiSWL58OdnZ2UAULEpWhIm7716HJXPONRTp7OBO1kheea/9b+BfkorM7AJgDBDdqY6ukhaZWQ/gDTP7VNLXFRYmPQQ8BJCbm7tBRMjJySEvL49kgaShyc7OJicnh/XrobAQWhb9ECZUdXW2c85thHQGizwgsaaQAyxKzCBpecLow8AfE6Ytit7nmdmbwD5AhWCRSlZWFt23sZ3liqhC0bJgCTRvHu4F5ZxzmymdzVAfAD3NrLuZNQaGAhXOajKzjgmjg4DZUXprM2sSDbcDDgYqd4y7JMrvC/Xjt9Cjh58F5ZyrFWmrWUgqMbNLgMlAJvCopM/N7GbCY/wmApeZ2SCgBFgBnBXNvjvwoJmVEQLa6CRnUbkkYsGixfJvoN+2VatyzqVPWi/KkzQJmFQpbWTC8LXAtUnm+x+wZzrL1lDFrulr/sN86LFz3RbGOddg+PMsGpiCgvDevGiZd24752qNB4sGprxmQT506lS3hXHONRgeLBqYCsGideu6LYxzrsHwYNHAxIJFMwo8WDjnao0HiwamvM+CfGjVqm4L45xrMDxYNDCxmsV2rPVg4ZyrNR4sGpj8fGiWVUwGSngCknPObR5/+FEDU1AAzRoVQdOW4VkWzjlXC7xm0cDk50PzzHXeue2cq1UeLBqY/HxonlHg/RXOuVrlwaKByc/3M6Gcc7XPg0UDU1AAzco8WDjnapcHiwYmPx+al/7ofRbOuVrlwaKByc+H5iUrvWbhnKtVHiwamPx80Xy9BwvnXO3yYNHAFBRE94XyYOGcq0UeLBqQ0lJYu9b8bCjnXK1La7AwswFmNsfM5prZNUmmn2VmS81sZvQ6N2HaMDP7KnoNS2c568qzz8KQIVBcXPN5ysrg7bdh/foNp61dG96bkw877FA7hXTOOdIYLMwsE7gXGAj0Bk4zs95Jsj4lqW/0+ns0bxtgFPATYH9glJk1uNN7rroKxo+H0aPjaatXwxdfVD3PE0/AIYdAz56wYgV89BFIYVqFZ1ns7I9Udc7VnnTWLPYH5kqaJ6kYGAcMruG8xwBTJK2QtBKYAgxIUzk32513wqOPxsdXrIClS6ufp6QEli0Lw7feGobfegu23x523x3eeAPOPHPDGsRTT4X3BQvgt7+FffeF3/8+pMVuT96MtbDTTpu/Ys45F0lnsOgMLEwYz4vSKjvJzD4xs/Fm1mVj5jWz4WY23cymL021d06T4mK48Ub461/jabm5oRUodsSfzIwZoSZw/fVhGY8/DjfdFJ9+/vkh7aOP4KuvYNgwyMuDKVPg0kshKwuefDLkHTkSnnkGZs8O423aZ0KTJrW+rs65bVc6g4UlSau8+/w30E3SXsBrwJiNmBdJD0nKlZTbvn37zSrsxrj99tB8BPDf/4Yj+tmzYeXKMDx/fpg2YgTcf3/yZUybFt4vvhh+8hO4665Qm7j2WsjIgLlzw/R334WxY8Nr4MBQ0zj9dNhjDygshDZtRJ9OKznllJDetfESjtr9u/R+Ac65bU46g0Ue0CVhPAdYlJhB0nJJRdHow8C+NZ23rixbBr/7XTjSnz8fXn45pBcXQ5s20K8ftG0b0v7yF7jiChgzBk46Cf785/hypkyBXr1Eh+3XcdNNsGZNqC1c0v9T9sqeU57v3cmrygPLZ5/BL34Rgku/fiFtyN5f8vGidlx+6AzWrIFrm9xN457eBOWcq13pDBYfAD3NrLuZNQaGAhMTM5hZx4TRQUDUkMJk4Gdm1jrq2P5ZlFaniovhuefCKarr14e+hqlToUOHeJ4vv4SiovAoidNPD/OcdRa88EIIMiUl4ayladNgYNNp0KEDx2S+xvz5MPOVJXT69UAO4D0AuthCpr4h3nsPjj2qmF90epd738uFo46i38RRABzw/UQyKeNPXx7HzB6/4Pw1d0CPHnXw7TjnGrK0BQtJJcAlhJ38bOBpSZ+b2c1mNijKdpmZfW5mHwOXAWdF864AbiEEnA+Am6O0OpWbG/oSunSBAw4IfQnffAPHHguNEh4jlZ8f+jGe+NlYDuw4nxZN1zN6xPesWweffALTLnuWoiIYMHN0iCxHH02bY/aj9/E9YMUKTr/nQI47Di494nO+L2pNcTFc2PEFnl10IB326wJLlnBc18/oz1QGzLoL9t4bW7KYvec9H9rvdtutbr4g51yDZaquF3Yrkpubq+nTp6dt+T/+GL/O7b77wrUOb74JixaFs5HGj4eZM+P577t2IRfe0YMFJZ1YTUtaWAHdNY/Tf76a919dxXdFbVnWZjeaTv8PPPZYqGr07g1XXgm77AJA4dw8bug5jvd2GsJLHc+jxfoVkLiOTzwBd98dere//x723jucUvWzn/lT8pxzNWJmH0rKTZXPH6taQ7Ezjf79bzj++NBfsSjqRcnJgSeHvMD9nfbinkmhCajNq+OgTRt2eutVWLECPfx3sv+xjif/3ZKdbCXPDnyEpv/8LESgxNOgEmTvksMdh0+Cz26Hd5fCdddVzHDGGeEF8esqBg6s7VV3zjm/3UdNzZoV3nvvHmpiOTnxaTnff0jv607kp5OuLU9rm/cxHHlkaBI68EDs0UfYOSf05T+rXzBgSIt4a/+7AAAYhUlEQVSa3ZLj/PPjF20cc0ytrItzzm0sDxZVeOml0LITM+tz0dTWsdOxfeC228jJik/MeWgkADvwQ3lam+9nQa9eFZb53AuZTM4YyL58FK6mq4lTTw3nz/797/DTn276Cjnn3GbwYJHEpEmhqemcc+Jpsz7Ip5dmk/nlbLjuOnJuLL+NFZ2/ngb9+9Oe+IWBbVm+QbDYdd8W/Ozw9ZCdHfonauonPwmFsWSXnzjnXPp5sKhEggsvDNc8vPRS6DN+8EH4+GPYndnw7bfw4ovk/PAhAK1YSXMK4IYbKtQs2rI83LejsjvvDBdeNPLuIufc1sODRSVr14Z48H//Bx07wpw5cMklsGh1CwZ3/jCcN3vccex49dlkZpSRY99B9+5w+OG03aERRhlZFIf7M/XsueEH9O0Lp5yy5VfMOec2gweLSmI34+vUCRbMK2XqoaMoKYHeNouTh8SbgTJH30qnzhl07rkdjBoFZmQOP4d2262jbaMfsR7dQ3OTc841AN4WUkn5nVubQda/xnLglJu5o/F6Dip+k4yBN1bIO3o0dOjQA46Irpi+5RZ2eB5YtR6u2eDxHc45t9XyYFFJLFg0byb4v5sgM5Oriv8QagmHHFIh7y9/ueH8u+0GpaWt4LzztkBpnXNuy/BmqEpiDxBqVro6PDTiootCwmGHQdOmKecfOzZ+63DnnGsovGZRSXkz1IrocRoDBkD79nDooTWav1mzNBXMOefqkAeLSsqDxdJvwsAuu4Q7BTrn3DbMm6EqKQ8WS74ON+Pr1q1Oy+Occ/WBB4tKyvssvvsyPMe6ceO6LZBzztUDHiwqKT8bauHs8luFO+fcts6DRSXlzVDzPk1+BbZzzm2D0hoszGyAmc0xs7lmVuVVamZ2spnJzHKj8W5mts7MZkavB9JZzkQFBdAoUzRevSw8Gs8551z6zoYys0zgXuBoIA/4wMwmSppVKV8LwiNV36u0iK8l9U1X+apSUADNGhWCMsKtZ51zzqW1ZrE/MFfSPEnFwDhgcJJ8twC3A4VpLEuN5edDs9I1cPDB0K5dXRfHOefqhXQGi87AwoTxvCitnJntA3SR9GKS+bub2Qwzm2ZmhySZnhYFK4toXrLSr61wzrkENQoWZvasmR1nZhsTXJI9qUcJy8wA/gT8Jkm+xUBXSfsAVwL/NLOWSco13Mymm9n0pUuXbrCQTVGwJJ9mFIRbiTvnnANqXrO4H/gl8JWZjTazXqlmINQkuiSM5wCLEsZbAHsAb5rZN8ABwEQzy5VUJGk5gKQPga+BXSt/gKSHJOVKym3fvn0NV6V6BcvXhWCx5561sjznnGsIahQsJL0m6XSgH/ANMMXM/mdmZ5tZVhWzfQD0NLPuZtYYGApMTFjmj5LaSeomqRvwLjBI0nQzax91kGNmPYCewLxNXMeNkr+yhGaNisIDLZxzzgEb0WdhZm2Bs4BzgRnAXwjBY0qy/JJKgEuAycBs4GlJn5vZzWY2KMXHHQp8YmYfA+OBCyStqGlZN0dBvmi2fZY/79o55xLU6NRZM3sO6AU8Dvxc0uJo0lNmNr2q+SRNAiZVShtZRd7+CcPPAs/WpGy1SqKgMIPmXZts8Y92zrn6rKbXWdwj6Y1kEyQ1iCvXJLj8giIWaCeatV1Z18Vxzrl6pabNULubWavYiJm1NrOL0lSmOrFiBfztofDM7GYt/c7tzjmXqKbB4jxJq2IjklYCDeq5oWvXxofnr25bdwVxzrl6qKbBIsMs3uMbnanUoO7dHbuBIMClv1xedwVxzrl6qKbBYjLwtJkdaWZHAP8CXklfsba8WLCYwCAOO9KboZxzLlFN94pXA+cDFxKuzH4V+Hu6ClUXym9NTgG09WYo55xLVKNgIamMcBX3/ektTt2pECxat67bwjjnXD1T0+ssegJ/AHoD2bF0ST3SVK4trjxYNM+ARt4M5ZxziWraZ/EPQq2iBDgcGEu4QK/BiAWL7Vo1qH5755yrFTUNFk0lvQ6YpAWSbgSOSF+xtrzymkXb7OozOufcNqim7S2F0S3FvzKzS4DvgB3SV6wtz4OFc85VraY1iyuA7QiPP90XOAMYlq5C1YXyZqj2zeq2IM45Vw+lrFlEF+CdIun/gHzg7LSXqg4UFEA268hs52dCOedcZSlrFpJKgX0Tr+BuiAryy/waC+ecq0JN+yxmABPM7Bmg/MYYkp5LS6nqQMGqkhAsWm7w9FbnnNvm1TRYtAGWU/EMKAENJ1isiWoWTZvWdVGcc67eqekV3A2ynyLR2gKFYJHtZ0M551xlNb2C+x+EmkQFkn6dYr4BhMevZgJ/lzS6inwnA88A+0maHqVdC5wDlAKXSZpck7JuqoJ8DxbOOVeVmjZDvZgwnA2cCCyqboboLKp7gaOBPOADM5soaValfC0Ip+S+l5DWGxgK9AE6Aa+Z2a5RZ3taFKyFHT1YOOdcUjVthqrwPGwz+xfwWorZ9gfmSpoXzTMOGAzMqpTvFuB24KqEtMHAOElFwHwzmxst752alHdTFKy1ULNo4tdZOOdcZTW9KK+ynkDXFHk6AwsTxvOitHJmtg/QRVJizaVG89a28mDhNQvnnNtATfss1lCxz2IJ4RkX1c6WJK18GdHtQ/4EnLWx8yYsYzgwHKBr11Sxq3oF6zI8WDjnXBVq2gzVYhOWnQd0SRjPoWI/RwtgD+DN6Hq/DsBEMxtUg3lj5XoIeAggNzd3g2CyMQoKMz1YOOdcFWrUDGVmJ5rZ9gnjrczshBSzfQD0NLPuZtaY0GE9MTZR0o+S2knqJqkb8C4wKDobaiIw1MyamFl3QrPX+xu1ZhuhtBSK1nuwcM65qtS0z2KUpB9jI5JWAaOqm0FSCXAJ4fnds4GnJX1uZjdHtYfq5v0ceJrQGf4KcHFaz4SK3USQtdCkSbo+xjnntlo1PXU2WVBJOa+kScCkSmkjq8jbv9L4rcCtNSzfZiksDO9NWec1C+ecS6KmNYvpZna3me1sZj3M7E/Ah+ks2JZUVBTem1DkwcI555KoabC4FCgGniI0D60DLk5XobY0DxbOOVe9mp4NVQBck+ay1JlYsGhMsfdZOOdcEjU9G2qKmbVKGG9tZmm9V9OWVF6zsPXQqKbdOM45t+2oaTNUu+gMKAAkraQBPYO7PFg0FjTsZzw559wmqWmwKDOz8kukzawbSa6o3loVF4f3Jo0bzCo551ytqmmby3XA22Y2LRo/lOg2Gw1BhZqFc865DdS0g/sVM8slBIiZwATCGVENQnmw8L5t55xLqqY3EjwXuJxwj6aZwAGE24UfUd18WwsPFs45V72a9llcDuwHLJB0OLAPsDRtpdrCyoNFtnduO+dcMjUNFoWSCgHMrImkL4Dd0lesLas8WDTd1Md7OOdcw1bTDu686DqLF4ApZraSFI9V3Zp4zcI556pX0w7uE6PBG81sKrA94W6wDYLXLJxzrnobfbmypGmpc21dym/30TSzbgvinHP1lB9Kk1Cz2M6DhXPOJePBghAsMiil0XaN67oozjlXL3mwINzuw29P7pxzVUtrsDCzAWY2x8zmmtkGtzg3swvM7FMzm2lmb5tZ7yi9m5mti9JnmtkD6SxnURE08duTO+dcldJ2P24zywTuBY4G8oAPzGyipFkJ2f4p6YEo/yDgbmBANO1rSX3TVb5EIVgUes3COeeqkM6axf7AXEnzJBUD44DBiRkkrU4YbUYd3cm2qEjeDOWcc9VIZ7DoDCxMGM+L0iows4vN7GvgduCyhEndzWyGmU0zs0PSWE6K1pZ5sHDOuWqkM1gkuxx6g5qDpHsl7QxcDVwfJS8GukraB7gS+KeZtdzgA8yGm9l0M5u+dOmm36qqaF1pCBbeZ+Gcc0mlM1jkAV0SxnOo/hYh44ATACQVSVoeDX8IfA3sWnkGSQ9JypWU2759+00uaFFhdDZUVtYmL8M55xqydAaLD4CeZtbdzBoDQ4GJiRnMrGfC6HHAV1F6+6iDHDPrAfQE5qWroOV9Fv78beecSypte0dJJWZ2CTAZyAQelfS5md0MTJc0EbjEzI4C1gMrgWHR7IcCN5tZCVAKXCBpRbrKWl6z8GDhnHNJpXXvKGkSMKlS2siE4curmO9Z4Nl0li1RURE0o9iDhXPOVcGv4AaKir1m4Zxz1fFgQcLtPjxYOOdcUh4sgKIi82DhnHPV8GCBN0M551wqHiyAouKoZpHpz7NwzrlkPFgAReszvGbhnHPV8GBBQs3Cg4VzziW1zQeLsjJYX+I1C+ecq842HyyKi8O7BwvnnKvaNh8siorCuwcL55yrmgeLKFg09tt9OOdclbb5YLH99jDphnc4nhc9WDjnXBW2+b1jkyYwcO9FwAIPFs45V4VtvmYBQElJePdg4ZxzSXmwAA8WzjmXggcL8GDhnHMpeLAADxbOOZdCWoOFmQ0wszlmNtfMrkky/QIz+9TMZprZ22bWO2HatdF8c8zsmHSW04OFc85VL23BwswygXuBgUBv4LTEYBD5p6Q9JfUFbgfujubtDQwF+gADgPui5aWHBwvnnKtWOmsW+wNzJc2TVAyMAwYnZpC0OmG0GaBoeDAwTlKRpPnA3Gh56RELFn6LcuecSyqdh9KdgYUJ43nATypnMrOLgSuBxsARCfO+W2nezukpJl6zcM65FNJZs7AkadogQbpX0s7A1cD1GzOvmQ03s+lmNn3p0qWbXlIPFs45V610Bos8oEvCeA6wqJr844ATNmZeSQ9JypWU2759+00vaWlpePdg4ZxzSaUzWHwA9DSz7mbWmNBhPTExg5n1TBg9DvgqGp4IDDWzJmbWHegJvJ+2knrNwjnnqpW2vaOkEjO7BJgMZAKPSvrczG4GpkuaCFxiZkcB64GVwLBo3s/N7GlgFlACXCypNF1lpaQEzCDDLztxzrlk0nooLWkSMKlS2siE4curmfdW4Nb0lS5BSYnXKpxzrhp+KA0eLJxzLgUPFuDBwjnnUvBgAR4snHMuBQ8W4MHCOedS8GABHiyccy4FDxbgwcI551LwYAEeLJxzLgUPFuDBwjnnUvBgAR4snHMuBQ8WEIKFP8vCOeeq5MECvGbhnHMpeLAADxbOOZeCBwsIz7PwYOGcc1XyYAFes3DOuRQ8WIAHC+ecS8GDBXiwcM65FDxYgAcL55xLIa3BwswGmNkcM5trZtckmX6lmc0ys0/M7HUz2ylhWqmZzYxeEyvPW6s8WDjnXLXStoc0s0zgXuBoIA/4wMwmSpqVkG0GkCtprZldCNwOnBpNWyepb7rKV4EHC+ecq1Y6axb7A3MlzZNUDIwDBidmkDRV0tpo9F0gJ43lqZoHC+ecq1Y6g0VnYGHCeF6UVpVzgJcTxrPNbLqZvWtmJySbwcyGR3mmL126dNNL6sHCOeeqlc49pCVJU9KMZmcAucBhCcldJS0ysx7AG2b2qaSvKyxMegh4CCA3NzfpsmvEg4VzzlUrnTWLPKBLwngOsKhyJjM7CrgOGCSpKJYuaVH0Pg94E9gnbSX1YOGcc9VKZ7D4AOhpZt3NrDEwFKhwVpOZ7QM8SAgUPySktzazJtFwO+BgILFjvHZ5sHDOuWqlbQ8pqcTMLgEmA5nAo5I+N7ObgemSJgJ3AM2BZ8wM4FtJg4DdgQfNrIwQ0EZXOouqdnmwcM65aqV1DylpEjCpUtrIhOGjqpjvf8Ce6SxbBf48C+ecq5ZfwQ1es3DOuRQ8WIDfotw551LwYAFes3DOuRQ8WIAHC+ecS8GDheTNUM45l4IHi9LS8O7BwjnnquTBoqQkvHuwcM65Knmw8GDhnHMpebDwYOGccyl5sPBg4ZxzKXmwaNQIhgyBnj3ruiTOOVdv+eF0q1bw9NN1XQrnnKvXvGbhnHMuJQ8WzjnnUvJg4ZxzLiUPFs4551JKa7AwswFmNsfM5prZNUmmX2lms8zsEzN73cx2Spg2zMy+il7D0llO55xz1UtbsDCzTOBeYCDQGzjNzHpXyjYDyJW0FzAeuD2atw0wCvgJsD8wysxap6uszjnnqpfOmsX+wFxJ8yQVA+OAwYkZJE2VtDYafRfIiYaPAaZIWiFpJTAFGJDGsjrnnKtGOoNFZ2BhwnhelFaVc4CXN3Fe55xzaZTOi/IsSZqSZjQ7A8gFDtuYec1sODA8Gs03szmbUM6YdsCyzZi/Pmko69JQ1gN8XeorXxfYKXWW9AaLPKBLwngOsKhyJjM7CrgOOExSUcK8/SvN+2bleSU9BDxUG4U1s+mScmtjWXWtoaxLQ1kP8HWpr3xdai6dzVAfAD3NrLuZNQaGAhMTM5jZPsCDwCBJPyRMmgz8zMxaRx3bP4vSnHPO1YG01SwklZjZJYSdfCbwqKTPzexmYLqkicAdQHPgGTMD+FbSIEkrzOwWQsABuFnSinSV1TnnXPXSeiNBSZOASZXSRiYMH1XNvI8Cj6avdBuoleaseqKhrEtDWQ/wdamvfF1qyKSkfc7OOedcOb/dh3POuZS2+WCR6pYk9Z2ZfWNmn5rZTDObHqW1MbMp0a1SptTXq9/N7FEz+8HMPktIS1p2C/4abadPzKxf3ZV8Q1Wsy41m9l20bWaa2bEJ066N1mWOmR1TN6VOzsy6mNlUM5ttZp+b2eVR+la1bapZj61uu5hZtpm9b2YfR+tyU5Te3czei7bJU9HJRJhZk2h8bjS922YXQtI2+yJ0vH8N9AAaAx8Dveu6XBu5Dt8A7Sql3Q5cEw1fA/yxrstZRdkPBfoBn6UqO3As4aJNAw4A3qvr8tdgXW4ErkqSt3f0W2sCdI9+g5l1vQ4J5esI9IuGWwBfRmXeqrZNNeux1W2X6LttHg1nAe9F3/XTwNAo/QHgwmj4IuCBaHgo8NTmlmFbr1mkvCXJVmowMCYaHgOcUIdlqZKkt4DKZ7lVVfbBwFgF7wKtzKzjlilpalWsS1UGA+MkFUmaD8wl/BbrBUmLJX0UDa8BZhPuoLBVbZtq1qMq9Xa7RN9tfjSaFb0EHEG4rx5suE1i22o8cKRFp5xuqm09WDSE24oIeNXMPoyuaAfYUdJiCH8YYIc6K93Gq6rsW+u2uiRqmnk0oTlwq1mXqPliH8KR7Fa7bSqtB2yF28XMMs1sJvAD4X55XwOrJJVEWRLLW74u0fQfgbab8/nberCo8S1J6rGDJfUj3N33YjM7tK4LlCZb47a6H9gZ6AssBu6K0reKdTGz5sCzwBWSVleXNUlavVmfJOuxVW4XSaWS+hLuaLE/sHuybNF7ra/Lth4sanRLkvpM0qLo/QfgecKP6PtYM0D0/kPVS6h3qir7VretJH0f/cHLgIeJN2nU+3UxsyzCDvZJSc9FyVvdtkm2HlvzdgGQtIpw+6MDCE1+sevlEstbvi7R9O2peTNpUtt6sEh5S5L6zMyamVmL2DDhtiifEdYh9sCoYcCEuinhJqmq7BOBM6Mzbw4Afow1idRXldrtTyRsGwjrMjQ6Y6U70BN4f0uXrypR2/YjwGxJdydM2qq2TVXrsTVuFzNrb2atouGmwFGEPpipwMlRtsrbJLatTgbeUNTbvcnqupe/rl+EMzm+JLT/XVfX5dnIsvcgnL3xMfB5rPyEtsnXga+i9zZ1XdYqyv8vQjPAesKR0DlVlZ1Qrb432k6fEh6aVefrkGJdHo/K+kn05+2YkP+6aF3mAAPruvyV1uWnhCaLT4CZ0evYrW3bVLMeW912AfYiPCzuE0JwGxml9yAEtLnAM0CTKD07Gp8bTe+xuWXwK7idc86ltK03QznnnKsBDxbOOedS8mDhnHMuJQ8WzjnnUvJg4ZxzLiUPFs7VA2bW38xerOtyOFcVDxbOOedS8mDh3EYwszOi5wrMNLMHo5u75ZvZXWb2kZm9bmbto7x9zezd6IZ1zyc8/2EXM3stejbBR2a2c7T45mY23sy+MLMnN/cuoc7VJg8WztWQme0OnEq4eWNfoBQ4HWgGfKRwQ8dpwKholrHA1ZL2IlwxHEt/ErhX0t7AQYQrvyHcFfUKwnMVegAHp32lnKuhRqmzOOciRwL7Ah9EB/1NCTfTKwOeivI8ATxnZtsDrSRNi9LHAM9E9/LqLOl5AEmFANHy3peUF43PBLoBb6d/tZxLzYOFczVnwBhJ11ZINLuhUr7q7qFTXdNSUcJwKf7/dPWIN0M5V3OvAyeb2Q5Q/kzqnQj/o9idP38JvC3pR2ClmR0Spf8KmKbwPIU8MzshWkYTM9tui66Fc5vAj1ycqyFJs8zsesKTCTMId5i9GCgA+pjZh4Qnkp0azTIMeCAKBvOAs6P0XwEPmtnN0TKGbMHVcG6T+F1nndtMZpYvqXldl8O5dPJmKOeccyl5zcI551xKXrNwzjmXkgcL55xzKXmwcM45l5IHC+eccyl5sHDOOZeSBwvnnHMp/T/yDfBx9jfrvwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "draw_train_val(ht_1.history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 423,
   "metadata": {},
   "outputs": [],
   "source": [
    "###2###\n",
    "def mod_v2(input_shape):\n",
    "    X_input = Input(input_shape)\n",
    "    X = X_input\n",
    "    X = Dense(1024)(X)\n",
    "    X = Activation('tanh')(X)\n",
    "    X = Dense(10)(X)\n",
    "    X = Activation('softmax')(X)\n",
    "    model = Model(inputs = X_input, outputs = X, name='mod_v2')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 424,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_2 = mod_v2((train_mnist.shape[1]-1,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 425,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_2.compile(optimizer=SGD(lr=0.06), loss='mean_squared_logarithmic_error', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 426,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7997 samples, validate on 999 samples\n",
      "Epoch 1/30\n",
      "7997/7997 [==============================] - 7s 863us/step - loss: 0.0411 - acc: 0.3156 - val_loss: 0.0274 - val_acc: 0.5816\n",
      "Epoch 2/30\n",
      "7997/7997 [==============================] - 1s 142us/step - loss: 0.0246 - acc: 0.6267 - val_loss: 0.0169 - val_acc: 0.7748\n",
      "Epoch 3/30\n",
      "7997/7997 [==============================] - 1s 137us/step - loss: 0.0172 - acc: 0.7599 - val_loss: 0.0122 - val_acc: 0.8448\n",
      "Epoch 4/30\n",
      "7997/7997 [==============================] - 1s 137us/step - loss: 0.0131 - acc: 0.8274 - val_loss: 0.0101 - val_acc: 0.8799\n",
      "Epoch 5/30\n",
      "7997/7997 [==============================] - 1s 135us/step - loss: 0.0107 - acc: 0.8672 - val_loss: 0.0089 - val_acc: 0.8929\n",
      "Epoch 6/30\n",
      "7997/7997 [==============================] - 1s 135us/step - loss: 0.0091 - acc: 0.8920 - val_loss: 0.0083 - val_acc: 0.8979\n",
      "Epoch 7/30\n",
      "7997/7997 [==============================] - 1s 139us/step - loss: 0.0078 - acc: 0.9132 - val_loss: 0.0076 - val_acc: 0.9099\n",
      "Epoch 8/30\n",
      "7997/7997 [==============================] - 1s 141us/step - loss: 0.0068 - acc: 0.9292 - val_loss: 0.0072 - val_acc: 0.9139\n",
      "Epoch 9/30\n",
      "7997/7997 [==============================] - 1s 135us/step - loss: 0.0061 - acc: 0.9386 - val_loss: 0.0070 - val_acc: 0.9169\n",
      "Epoch 10/30\n",
      "7997/7997 [==============================] - 1s 137us/step - loss: 0.0054 - acc: 0.9475 - val_loss: 0.0067 - val_acc: 0.9159\n",
      "Epoch 11/30\n",
      "7997/7997 [==============================] - 1s 136us/step - loss: 0.0049 - acc: 0.9542 - val_loss: 0.0066 - val_acc: 0.9179\n",
      "Epoch 12/30\n",
      "7997/7997 [==============================] - 1s 135us/step - loss: 0.0045 - acc: 0.9596 - val_loss: 0.0063 - val_acc: 0.9229\n",
      "Epoch 13/30\n",
      "7997/7997 [==============================] - 1s 140us/step - loss: 0.0042 - acc: 0.9622 - val_loss: 0.0062 - val_acc: 0.9269\n",
      "Epoch 14/30\n",
      "7997/7997 [==============================] - 1s 136us/step - loss: 0.0039 - acc: 0.9641 - val_loss: 0.0062 - val_acc: 0.9269\n",
      "Epoch 15/30\n",
      "7997/7997 [==============================] - 1s 142us/step - loss: 0.0037 - acc: 0.9665 - val_loss: 0.0060 - val_acc: 0.9279\n",
      "Epoch 16/30\n",
      "7997/7997 [==============================] - 1s 140us/step - loss: 0.0035 - acc: 0.9679 - val_loss: 0.0060 - val_acc: 0.9289\n",
      "Epoch 17/30\n",
      "7997/7997 [==============================] - 1s 144us/step - loss: 0.0034 - acc: 0.9697 - val_loss: 0.0058 - val_acc: 0.9309\n",
      "Epoch 18/30\n",
      "7997/7997 [==============================] - 1s 144us/step - loss: 0.0032 - acc: 0.9704 - val_loss: 0.0057 - val_acc: 0.9279\n",
      "Epoch 19/30\n",
      "7997/7997 [==============================] - 1s 140us/step - loss: 0.0031 - acc: 0.9722 - val_loss: 0.0057 - val_acc: 0.9259\n",
      "Epoch 20/30\n",
      "7997/7997 [==============================] - 1s 140us/step - loss: 0.0030 - acc: 0.9731 - val_loss: 0.0056 - val_acc: 0.9329\n",
      "Epoch 21/30\n",
      "7997/7997 [==============================] - 1s 136us/step - loss: 0.0029 - acc: 0.9739 - val_loss: 0.0056 - val_acc: 0.9299\n",
      "Epoch 22/30\n",
      "7997/7997 [==============================] - 1s 139us/step - loss: 0.0028 - acc: 0.9747 - val_loss: 0.0055 - val_acc: 0.9309\n",
      "Epoch 23/30\n",
      "7997/7997 [==============================] - 1s 137us/step - loss: 0.0027 - acc: 0.9755 - val_loss: 0.0054 - val_acc: 0.9309\n",
      "Epoch 24/30\n",
      "7997/7997 [==============================] - 1s 136us/step - loss: 0.0027 - acc: 0.9762 - val_loss: 0.0054 - val_acc: 0.9309\n",
      "Epoch 25/30\n",
      "7997/7997 [==============================] - 1s 136us/step - loss: 0.0026 - acc: 0.9771 - val_loss: 0.0054 - val_acc: 0.9319\n",
      "Epoch 26/30\n",
      "7997/7997 [==============================] - 1s 139us/step - loss: 0.0025 - acc: 0.9780 - val_loss: 0.0054 - val_acc: 0.9319\n",
      "Epoch 27/30\n",
      "7997/7997 [==============================] - 1s 141us/step - loss: 0.0025 - acc: 0.9787 - val_loss: 0.0053 - val_acc: 0.9339\n",
      "Epoch 28/30\n",
      "7997/7997 [==============================] - 1s 142us/step - loss: 0.0024 - acc: 0.9787 - val_loss: 0.0053 - val_acc: 0.9319\n",
      "Epoch 29/30\n",
      "7997/7997 [==============================] - 1s 144us/step - loss: 0.0024 - acc: 0.9787 - val_loss: 0.0052 - val_acc: 0.9319\n",
      "Epoch 30/30\n",
      "7997/7997 [==============================] - 1s 137us/step - loss: 0.0023 - acc: 0.9795 - val_loss: 0.0052 - val_acc: 0.9349\n"
     ]
    }
   ],
   "source": [
    "ht_2 = model_2.fit(train_X, train_Y, validation_data=(val_X, val_Y), epochs = 30, batch_size = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 427,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "draw_train_val(ht_2.history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 428,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train accuracy: 0.979492317867\n",
      "val accuracy: 0.93493493935\n",
      "1004/1004 [==============================] - 0s 137us/step\n",
      "test accuracy: 0.876494023904\n"
     ]
    }
   ],
   "source": [
    "print(\"train accuracy: \" + str(ht_2.history['acc'][-1]))\n",
    "print(\"val accuracy: \" + str(ht_2.history['val_acc'][-1]))\n",
    "print(\"test accuracy: \" + str(model_2.evaluate(test_X, test_Y)[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 470,
   "metadata": {},
   "outputs": [],
   "source": [
    "###3###\n",
    "def mod_v3(input_shape):\n",
    "    X_input = Input(input_shape)\n",
    "    X = Reshape((28,28,1))(X_input)\n",
    "    X = Conv2D(32, (5, 5), strides = (2, 2), kernel_initializer = glorot_uniform(seed=0))(X)\n",
    "    X = Activation('relu')(X)\n",
    "    X = Flatten()(X)\n",
    "    X = Dense(1024)(X)\n",
    "    X = Activation('tanh')(X)\n",
    "    X = Dense(10)(X)\n",
    "    X = Activation('softmax')(X)\n",
    "    model = Model(inputs = X_input, outputs = X, name='mod_v3')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 471,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_3 = mod_v3((28,28))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 472,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_3.compile(optimizer=SGD(lr=0.2), loss='mean_squared_logarithmic_error', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 473,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7997 samples, validate on 999 samples\n",
      "Epoch 1/30\n",
      "7997/7997 [==============================] - 12s 2ms/step - loss: 0.0179 - acc: 0.7369 - val_loss: 0.0073 - val_acc: 0.9069\n",
      "Epoch 2/30\n",
      "7997/7997 [==============================] - 6s 790us/step - loss: 0.0083 - acc: 0.8986 - val_loss: 0.0051 - val_acc: 0.9439\n",
      "Epoch 3/30\n",
      "7997/7997 [==============================] - 6s 748us/step - loss: 0.0066 - acc: 0.9197 - val_loss: 0.0042 - val_acc: 0.9580\n",
      "Epoch 4/30\n",
      "7997/7997 [==============================] - 6s 778us/step - loss: 0.0055 - acc: 0.9374 - val_loss: 0.0041 - val_acc: 0.9499\n",
      "Epoch 5/30\n",
      "7997/7997 [==============================] - 6s 737us/step - loss: 0.0048 - acc: 0.9442 - val_loss: 0.0034 - val_acc: 0.9630\n",
      "Epoch 6/30\n",
      "7997/7997 [==============================] - 7s 813us/step - loss: 0.0043 - acc: 0.9546 - val_loss: 0.0034 - val_acc: 0.9590\n",
      "Epoch 7/30\n",
      "7997/7997 [==============================] - 7s 841us/step - loss: 0.0038 - acc: 0.9590 - val_loss: 0.0030 - val_acc: 0.9610\n",
      "Epoch 8/30\n",
      "7997/7997 [==============================] - 6s 796us/step - loss: 0.0034 - acc: 0.9651 - val_loss: 0.0028 - val_acc: 0.9690\n",
      "Epoch 9/30\n",
      "7997/7997 [==============================] - 6s 785us/step - loss: 0.0029 - acc: 0.9700 - val_loss: 0.0028 - val_acc: 0.9640\n",
      "Epoch 10/30\n",
      "7997/7997 [==============================] - 6s 771us/step - loss: 0.0026 - acc: 0.9737 - val_loss: 0.0028 - val_acc: 0.9660\n",
      "Epoch 11/30\n",
      "7997/7997 [==============================] - 6s 722us/step - loss: 0.0023 - acc: 0.9771 - val_loss: 0.0026 - val_acc: 0.9640\n",
      "Epoch 12/30\n",
      "7997/7997 [==============================] - 6s 725us/step - loss: 0.0020 - acc: 0.9830 - val_loss: 0.0027 - val_acc: 0.9630\n",
      "Epoch 13/30\n",
      "7997/7997 [==============================] - 6s 729us/step - loss: 0.0018 - acc: 0.9850 - val_loss: 0.0023 - val_acc: 0.9690\n",
      "Epoch 14/30\n",
      "7997/7997 [==============================] - 6s 732us/step - loss: 0.0015 - acc: 0.9882 - val_loss: 0.0025 - val_acc: 0.9650\n",
      "Epoch 15/30\n",
      "7997/7997 [==============================] - 6s 727us/step - loss: 0.0013 - acc: 0.9904 - val_loss: 0.0024 - val_acc: 0.9680\n",
      "Epoch 16/30\n",
      "7997/7997 [==============================] - 6s 727us/step - loss: 0.0011 - acc: 0.9919 - val_loss: 0.0022 - val_acc: 0.9710\n",
      "Epoch 17/30\n",
      "7997/7997 [==============================] - 6s 728us/step - loss: 8.8816e-04 - acc: 0.9941 - val_loss: 0.0021 - val_acc: 0.9700\n",
      "Epoch 18/30\n",
      "7997/7997 [==============================] - 6s 744us/step - loss: 7.4363e-04 - acc: 0.9954 - val_loss: 0.0022 - val_acc: 0.9700\n",
      "Epoch 19/30\n",
      "7997/7997 [==============================] - 6s 797us/step - loss: 6.3594e-04 - acc: 0.9964 - val_loss: 0.0021 - val_acc: 0.9710\n",
      "Epoch 20/30\n",
      "7997/7997 [==============================] - 6s 774us/step - loss: 5.4458e-04 - acc: 0.9967 - val_loss: 0.0021 - val_acc: 0.9700\n",
      "Epoch 21/30\n",
      "7997/7997 [==============================] - 6s 752us/step - loss: 4.8017e-04 - acc: 0.9969 - val_loss: 0.0021 - val_acc: 0.9690\n",
      "Epoch 22/30\n",
      "7997/7997 [==============================] - 6s 791us/step - loss: 4.3606e-04 - acc: 0.9971 - val_loss: 0.0020 - val_acc: 0.9700\n",
      "Epoch 23/30\n",
      "7997/7997 [==============================] - 6s 740us/step - loss: 3.9834e-04 - acc: 0.9971 - val_loss: 0.0021 - val_acc: 0.9700\n",
      "Epoch 24/30\n",
      "7997/7997 [==============================] - 6s 767us/step - loss: 3.6750e-04 - acc: 0.9972 - val_loss: 0.0021 - val_acc: 0.9700\n",
      "Epoch 25/30\n",
      "7997/7997 [==============================] - 6s 760us/step - loss: 3.4599e-04 - acc: 0.9975 - val_loss: 0.0021 - val_acc: 0.9700\n",
      "Epoch 26/30\n",
      "7997/7997 [==============================] - 6s 768us/step - loss: 3.2678e-04 - acc: 0.9975 - val_loss: 0.0020 - val_acc: 0.9720\n",
      "Epoch 27/30\n",
      "7997/7997 [==============================] - 6s 711us/step - loss: 3.1203e-04 - acc: 0.9976 - val_loss: 0.0020 - val_acc: 0.9710\n",
      "Epoch 28/30\n",
      "7997/7997 [==============================] - 6s 708us/step - loss: 3.0003e-04 - acc: 0.9976 - val_loss: 0.0020 - val_acc: 0.9720\n",
      "Epoch 29/30\n",
      "7997/7997 [==============================] - 6s 706us/step - loss: 2.8812e-04 - acc: 0.9979 - val_loss: 0.0020 - val_acc: 0.9720\n",
      "Epoch 30/30\n",
      "7997/7997 [==============================] - 6s 754us/step - loss: 2.7863e-04 - acc: 0.9979 - val_loss: 0.0020 - val_acc: 0.9720\n"
     ]
    }
   ],
   "source": [
    "ht_3 = model_3.fit(train_X.reshape(train_X.shape[0], 28, 28), train_Y, validation_data=(val_X.reshape(val_X.shape[0], 28, 28), val_Y), epochs = 30, batch_size = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 474,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "draw_train_val(ht_3.history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 475,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train accuracy: 0.997874204727\n",
      "val accuracy: 0.971971990766\n",
      "1004/1004 [==============================] - 0s 306us/step\n",
      "test accuracy: 0.946215139442\n"
     ]
    }
   ],
   "source": [
    "print(\"train accuracy: \" + str(ht_3.history['acc'][-1]))\n",
    "print(\"val accuracy: \" + str(ht_3.history['val_acc'][-1]))\n",
    "print(\"test accuracy: \" + str(model_3.evaluate(test_X.reshape(test_X.shape[0], 28, 28), test_Y)[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 443,
   "metadata": {},
   "outputs": [],
   "source": [
    "###4###\n",
    "def mod_v4(input_shape):\n",
    "    X_input = Input(input_shape)\n",
    "    X = Reshape((28,28,1))(X_input)\n",
    "    X = Conv2D(32, (5, 5), strides = (2, 2), kernel_initializer = glorot_uniform(seed=0))(X)\n",
    "    X = Activation('relu')(X)\n",
    "    X = MaxPooling2D((2,2))(X)\n",
    "    X = Flatten()(X)\n",
    "    X = Dense(1024)(X)\n",
    "    X = Activation('tanh')(X)\n",
    "    X = Dense(10)(X)\n",
    "    X = Activation('softmax')(X)\n",
    "    model = Model(inputs = X_input, outputs = X, name='mod_v4')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 444,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_4 = mod_v4((28,28))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 445,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_4.compile(optimizer='adam', loss='mean_squared_logarithmic_error', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 446,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7997 samples, validate on 999 samples\n",
      "Epoch 1/30\n",
      "7997/7997 [==============================] - 8s 1ms/step - loss: 0.0224 - acc: 0.6951 - val_loss: 0.0127 - val_acc: 0.8509\n",
      "Epoch 2/30\n",
      "7997/7997 [==============================] - 3s 380us/step - loss: 0.0133 - acc: 0.8284 - val_loss: 0.0101 - val_acc: 0.8659\n",
      "Epoch 3/30\n",
      "7997/7997 [==============================] - 3s 379us/step - loss: 0.0108 - acc: 0.8558 - val_loss: 0.0088 - val_acc: 0.8739\n",
      "Epoch 4/30\n",
      "7997/7997 [==============================] - 3s 375us/step - loss: 0.0099 - acc: 0.8631 - val_loss: 0.0081 - val_acc: 0.8839\n",
      "Epoch 5/30\n",
      "7997/7997 [==============================] - 3s 369us/step - loss: 0.0092 - acc: 0.8705 - val_loss: 0.0084 - val_acc: 0.8749\n",
      "Epoch 6/30\n",
      "7997/7997 [==============================] - 3s 364us/step - loss: 0.0086 - acc: 0.8755 - val_loss: 0.0076 - val_acc: 0.8889\n",
      "Epoch 7/30\n",
      "7997/7997 [==============================] - 3s 370us/step - loss: 0.0083 - acc: 0.8791 - val_loss: 0.0077 - val_acc: 0.8819\n",
      "Epoch 8/30\n",
      "7997/7997 [==============================] - 3s 358us/step - loss: 0.0081 - acc: 0.8815 - val_loss: 0.0077 - val_acc: 0.8839\n",
      "Epoch 9/30\n",
      "7997/7997 [==============================] - 3s 361us/step - loss: 0.0079 - acc: 0.8822 - val_loss: 0.0076 - val_acc: 0.8879\n",
      "Epoch 10/30\n",
      "7997/7997 [==============================] - 3s 388us/step - loss: 0.0076 - acc: 0.8857 - val_loss: 0.0077 - val_acc: 0.8819\n",
      "Epoch 11/30\n",
      "7997/7997 [==============================] - 3s 358us/step - loss: 0.0074 - acc: 0.8885 - val_loss: 0.0076 - val_acc: 0.8889\n",
      "Epoch 12/30\n",
      "7997/7997 [==============================] - 3s 350us/step - loss: 0.0072 - acc: 0.8895 - val_loss: 0.0075 - val_acc: 0.8849\n",
      "Epoch 13/30\n",
      "7997/7997 [==============================] - 3s 354us/step - loss: 0.0072 - acc: 0.8916 - val_loss: 0.0073 - val_acc: 0.8879\n",
      "Epoch 14/30\n",
      "7997/7997 [==============================] - 3s 353us/step - loss: 0.0070 - acc: 0.8926 - val_loss: 0.0073 - val_acc: 0.8839\n",
      "Epoch 15/30\n",
      "7997/7997 [==============================] - 3s 352us/step - loss: 0.0069 - acc: 0.8938 - val_loss: 0.0072 - val_acc: 0.8859\n",
      "Epoch 16/30\n",
      "7997/7997 [==============================] - 3s 352us/step - loss: 0.0067 - acc: 0.8958 - val_loss: 0.0071 - val_acc: 0.8889\n",
      "Epoch 17/30\n",
      "7997/7997 [==============================] - 3s 356us/step - loss: 0.0066 - acc: 0.8962 - val_loss: 0.0075 - val_acc: 0.8829\n",
      "Epoch 18/30\n",
      "7997/7997 [==============================] - 3s 355us/step - loss: 0.0066 - acc: 0.8960 - val_loss: 0.0072 - val_acc: 0.8859\n",
      "Epoch 19/30\n",
      "7997/7997 [==============================] - 3s 353us/step - loss: 0.0029 - acc: 0.9581 - val_loss: 0.0012 - val_acc: 0.9850\n",
      "Epoch 20/30\n",
      "7997/7997 [==============================] - 3s 353us/step - loss: 9.2655e-04 - acc: 0.9904 - val_loss: 0.0012 - val_acc: 0.9890\n",
      "Epoch 21/30\n",
      "7997/7997 [==============================] - 3s 354us/step - loss: 7.9921e-04 - acc: 0.9917 - val_loss: 0.0015 - val_acc: 0.9810\n",
      "Epoch 22/30\n",
      "7997/7997 [==============================] - 3s 356us/step - loss: 6.7823e-04 - acc: 0.9926 - val_loss: 0.0011 - val_acc: 0.9850\n",
      "Epoch 23/30\n",
      "7997/7997 [==============================] - 3s 352us/step - loss: 6.6112e-04 - acc: 0.9936 - val_loss: 0.0014 - val_acc: 0.9830\n",
      "Epoch 24/30\n",
      "7997/7997 [==============================] - 3s 359us/step - loss: 6.4353e-04 - acc: 0.9931 - val_loss: 0.0010 - val_acc: 0.9850\n",
      "Epoch 25/30\n",
      "7997/7997 [==============================] - 3s 355us/step - loss: 4.1779e-04 - acc: 0.9965 - val_loss: 0.0012 - val_acc: 0.9840\n",
      "Epoch 26/30\n",
      "7997/7997 [==============================] - 3s 359us/step - loss: 4.6682e-04 - acc: 0.9954 - val_loss: 0.0012 - val_acc: 0.9830\n",
      "Epoch 27/30\n",
      "7997/7997 [==============================] - 3s 348us/step - loss: 3.2864e-04 - acc: 0.9980 - val_loss: 0.0013 - val_acc: 0.9840\n",
      "Epoch 28/30\n",
      "7997/7997 [==============================] - 3s 364us/step - loss: 2.9994e-04 - acc: 0.9976 - val_loss: 0.0013 - val_acc: 0.9850\n",
      "Epoch 29/30\n",
      "7997/7997 [==============================] - 3s 373us/step - loss: 3.0435e-04 - acc: 0.9981 - val_loss: 0.0013 - val_acc: 0.9840\n",
      "Epoch 30/30\n",
      "7997/7997 [==============================] - 3s 343us/step - loss: 2.2562e-04 - acc: 0.9992 - val_loss: 0.0014 - val_acc: 0.9830\n"
     ]
    }
   ],
   "source": [
    "ht_4 = model_4.fit(train_X.reshape(train_X.shape[0], 28, 28), train_Y, validation_data=(val_X.reshape(val_X.shape[0], 28, 28), val_Y), epochs = 30, batch_size = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 447,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "draw_train_val(ht_4.history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 448,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train accuracy: 0.99924971936\n",
      "val accuracy: 0.982982991276\n",
      "1004/1004 [==============================] - 0s 227us/step\n",
      "test accuracy: 0.980079681275\n"
     ]
    }
   ],
   "source": [
    "print(\"train accuracy: \" + str(ht_4.history['acc'][-1]))\n",
    "print(\"val accuracy: \" + str(ht_4.history['val_acc'][-1]))\n",
    "print(\"test accuracy: \" + str(model_4.evaluate(test_X.reshape(test_X.shape[0], 28, 28), test_Y)[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 449,
   "metadata": {},
   "outputs": [],
   "source": [
    "###5###\n",
    "def mod_v5(input_shape, drop):\n",
    "    X_input = Input(input_shape)\n",
    "    X = Reshape((28,28,1))(X_input)\n",
    "    X = Conv2D(32, (5, 5), strides = (2, 2), kernel_initializer = glorot_uniform(seed=0))(X)\n",
    "    X = Activation('relu')(X)\n",
    "    X = MaxPooling2D((2,2))(X)\n",
    "    X = Flatten()(X)\n",
    "    X = Dense(1024)(X)\n",
    "    X = Dropout(drop)(X)\n",
    "    X = Activation('tanh')(X)\n",
    "    X = Dense(10)(X)\n",
    "    X = Activation('softmax')(X)\n",
    "    model = Model(inputs = X_input, outputs = X, name='mod_v5')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropout: 0.0\n",
      "Train on 7997 samples, validate on 999 samples\n",
      "Epoch 1/10\n",
      "7997/7997 [==============================] - 4s 500us/step - loss: 0.0448 - acc: 0.2401 - val_loss: 0.0346 - val_acc: 0.4615\n",
      "Epoch 2/10\n",
      "7997/7997 [==============================] - 2s 257us/step - loss: 0.0285 - acc: 0.5712 - val_loss: 0.0224 - val_acc: 0.6967\n",
      "Epoch 3/10\n",
      "7997/7997 [==============================] - 2s 257us/step - loss: 0.0202 - acc: 0.7331 - val_loss: 0.0165 - val_acc: 0.7998\n",
      "Epoch 4/10\n",
      "7997/7997 [==============================] - 2s 259us/step - loss: 0.0160 - acc: 0.8063 - val_loss: 0.0135 - val_acc: 0.8348\n",
      "Epoch 5/10\n",
      "7997/7997 [==============================] - 2s 264us/step - loss: 0.0136 - acc: 0.8439 - val_loss: 0.0117 - val_acc: 0.8549\n",
      "Epoch 6/10\n",
      "7997/7997 [==============================] - 2s 263us/step - loss: 0.0120 - acc: 0.8670 - val_loss: 0.0106 - val_acc: 0.8689\n",
      "Epoch 7/10\n",
      "7997/7997 [==============================] - 2s 263us/step - loss: 0.0107 - acc: 0.8826 - val_loss: 0.0098 - val_acc: 0.8809\n",
      "Epoch 8/10\n",
      "7997/7997 [==============================] - 2s 259us/step - loss: 0.0098 - acc: 0.8947 - val_loss: 0.0091 - val_acc: 0.8909\n",
      "Epoch 9/10\n",
      "7997/7997 [==============================] - 2s 259us/step - loss: 0.0091 - acc: 0.9046 - val_loss: 0.0085 - val_acc: 0.8989\n",
      "Epoch 10/10\n",
      "7997/7997 [==============================] - 2s 256us/step - loss: 0.0085 - acc: 0.9123 - val_loss: 0.0081 - val_acc: 0.9029\n",
      "Dropout: 0.01\n",
      "Train on 7997 samples, validate on 999 samples\n",
      "Epoch 1/10\n",
      "7997/7997 [==============================] - 4s 518us/step - loss: 0.0426 - acc: 0.2651 - val_loss: 0.0323 - val_acc: 0.5125\n",
      "Epoch 2/10\n",
      "7997/7997 [==============================] - 2s 279us/step - loss: 0.0275 - acc: 0.5957 - val_loss: 0.0215 - val_acc: 0.7247\n",
      "Epoch 3/10\n",
      "7997/7997 [==============================] - 2s 296us/step - loss: 0.0202 - acc: 0.7314 - val_loss: 0.0159 - val_acc: 0.8208\n",
      "Epoch 4/10\n",
      "7997/7997 [==============================] - 2s 270us/step - loss: 0.0163 - acc: 0.7960 - val_loss: 0.0131 - val_acc: 0.8589\n",
      "Epoch 5/10\n",
      "7997/7997 [==============================] - 2s 285us/step - loss: 0.0139 - acc: 0.8337 - val_loss: 0.0111 - val_acc: 0.8839\n",
      "Epoch 6/10\n",
      "7997/7997 [==============================] - 2s 286us/step - loss: 0.0122 - acc: 0.8584 - val_loss: 0.0100 - val_acc: 0.8989\n",
      "Epoch 7/10\n",
      "7997/7997 [==============================] - 2s 280us/step - loss: 0.0109 - acc: 0.8786 - val_loss: 0.0092 - val_acc: 0.9079\n",
      "Epoch 8/10\n",
      "7997/7997 [==============================] - 2s 278us/step - loss: 0.0100 - acc: 0.8926 - val_loss: 0.0086 - val_acc: 0.9139\n",
      "Epoch 9/10\n",
      "7997/7997 [==============================] - 2s 278us/step - loss: 0.0092 - acc: 0.9023 - val_loss: 0.0080 - val_acc: 0.9219\n",
      "Epoch 10/10\n",
      "7997/7997 [==============================] - 2s 279us/step - loss: 0.0085 - acc: 0.9101 - val_loss: 0.0075 - val_acc: 0.9309\n",
      "Dropout: 0.02\n",
      "Train on 7997 samples, validate on 999 samples\n",
      "Epoch 1/10\n",
      "7997/7997 [==============================] - 4s 521us/step - loss: 0.0428 - acc: 0.2628 - val_loss: 0.0326 - val_acc: 0.4995\n",
      "Epoch 2/10\n",
      "7997/7997 [==============================] - 2s 282us/step - loss: 0.0279 - acc: 0.5861 - val_loss: 0.0217 - val_acc: 0.7217\n",
      "Epoch 3/10\n",
      "7997/7997 [==============================] - 2s 280us/step - loss: 0.0206 - acc: 0.7229 - val_loss: 0.0157 - val_acc: 0.8318\n",
      "Epoch 4/10\n",
      "7997/7997 [==============================] - 2s 278us/step - loss: 0.0166 - acc: 0.7932 - val_loss: 0.0130 - val_acc: 0.8619\n",
      "Epoch 5/10\n",
      "7997/7997 [==============================] - 2s 279us/step - loss: 0.0141 - acc: 0.8312 - val_loss: 0.0112 - val_acc: 0.8729\n",
      "Epoch 6/10\n",
      "7997/7997 [==============================] - 2s 284us/step - loss: 0.0123 - acc: 0.8564 - val_loss: 0.0100 - val_acc: 0.8959\n",
      "Epoch 7/10\n",
      "7997/7997 [==============================] - 2s 305us/step - loss: 0.0111 - acc: 0.8728 - val_loss: 0.0091 - val_acc: 0.8989\n",
      "Epoch 8/10\n",
      "7997/7997 [==============================] - 2s 283us/step - loss: 0.0102 - acc: 0.8880 - val_loss: 0.0085 - val_acc: 0.9079\n",
      "Epoch 9/10\n",
      "7997/7997 [==============================] - 2s 278us/step - loss: 0.0094 - acc: 0.8978 - val_loss: 0.0079 - val_acc: 0.9249\n",
      "Epoch 10/10\n",
      "7997/7997 [==============================] - 2s 279us/step - loss: 0.0087 - acc: 0.9080 - val_loss: 0.0073 - val_acc: 0.9299\n",
      "Dropout: 0.03\n",
      "Train on 7997 samples, validate on 999 samples\n",
      "Epoch 1/10\n",
      "7997/7997 [==============================] - 4s 546us/step - loss: 0.0429 - acc: 0.2625 - val_loss: 0.0328 - val_acc: 0.5015\n",
      "Epoch 2/10\n",
      "7997/7997 [==============================] - 2s 283us/step - loss: 0.0284 - acc: 0.5721 - val_loss: 0.0220 - val_acc: 0.7057\n",
      "Epoch 3/10\n",
      "7997/7997 [==============================] - 2s 284us/step - loss: 0.0210 - acc: 0.7158 - val_loss: 0.0159 - val_acc: 0.8258\n",
      "Epoch 4/10\n",
      "7997/7997 [==============================] - 2s 285us/step - loss: 0.0169 - acc: 0.7865 - val_loss: 0.0129 - val_acc: 0.8619\n",
      "Epoch 5/10\n",
      "7997/7997 [==============================] - 2s 280us/step - loss: 0.0143 - acc: 0.8264 - val_loss: 0.0110 - val_acc: 0.8849\n",
      "Epoch 6/10\n",
      "7997/7997 [==============================] - 2s 279us/step - loss: 0.0125 - acc: 0.8527 - val_loss: 0.0099 - val_acc: 0.9029\n",
      "Epoch 7/10\n",
      "7997/7997 [==============================] - 2s 277us/step - loss: 0.0113 - acc: 0.8735 - val_loss: 0.0091 - val_acc: 0.9139\n",
      "Epoch 8/10\n",
      "7997/7997 [==============================] - 2s 290us/step - loss: 0.0103 - acc: 0.8850 - val_loss: 0.0084 - val_acc: 0.9179\n",
      "Epoch 9/10\n",
      "7997/7997 [==============================] - 2s 292us/step - loss: 0.0095 - acc: 0.8966 - val_loss: 0.0079 - val_acc: 0.9209\n",
      "Epoch 10/10\n",
      "7997/7997 [==============================] - 2s 286us/step - loss: 0.0089 - acc: 0.9075 - val_loss: 0.0073 - val_acc: 0.9299\n",
      "Dropout: 0.04\n",
      "Train on 7997 samples, validate on 999 samples\n",
      "Epoch 1/10\n",
      "7997/7997 [==============================] - 4s 544us/step - loss: 0.0431 - acc: 0.2580 - val_loss: 0.0330 - val_acc: 0.4945\n",
      "Epoch 2/10\n",
      "7997/7997 [==============================] - 2s 295us/step - loss: 0.0286 - acc: 0.5701 - val_loss: 0.0221 - val_acc: 0.7137\n",
      "Epoch 3/10\n",
      "7997/7997 [==============================] - 2s 291us/step - loss: 0.0214 - acc: 0.7038 - val_loss: 0.0160 - val_acc: 0.8278\n",
      "Epoch 4/10\n",
      "7997/7997 [==============================] - 2s 295us/step - loss: 0.0173 - acc: 0.7818 - val_loss: 0.0131 - val_acc: 0.8579\n",
      "Epoch 5/10\n",
      "7997/7997 [==============================] - 2s 305us/step - loss: 0.0147 - acc: 0.8186 - val_loss: 0.0110 - val_acc: 0.8869\n",
      "Epoch 6/10\n",
      "7997/7997 [==============================] - 2s 291us/step - loss: 0.0128 - acc: 0.8488 - val_loss: 0.0099 - val_acc: 0.8999\n",
      "Epoch 7/10\n",
      "7997/7997 [==============================] - 2s 298us/step - loss: 0.0116 - acc: 0.8658 - val_loss: 0.0090 - val_acc: 0.9009\n",
      "Epoch 8/10\n",
      "7997/7997 [==============================] - 2s 294us/step - loss: 0.0106 - acc: 0.8798 - val_loss: 0.0084 - val_acc: 0.9109\n",
      "Epoch 9/10\n",
      "7997/7997 [==============================] - 3s 327us/step - loss: 0.0098 - acc: 0.8925 - val_loss: 0.0079 - val_acc: 0.9189\n",
      "Epoch 10/10\n",
      "7997/7997 [==============================] - 2s 288us/step - loss: 0.0091 - acc: 0.8998 - val_loss: 0.0073 - val_acc: 0.9269\n",
      "Dropout: 0.05\n",
      "Train on 7997 samples, validate on 999 samples\n",
      "Epoch 1/10\n",
      "7997/7997 [==============================] - 4s 538us/step - loss: 0.0433 - acc: 0.2540 - val_loss: 0.0330 - val_acc: 0.4935\n",
      "Epoch 2/10\n",
      "7997/7997 [==============================] - 2s 292us/step - loss: 0.0291 - acc: 0.5535 - val_loss: 0.0223 - val_acc: 0.7047\n",
      "Epoch 3/10\n",
      "7997/7997 [==============================] - 2s 297us/step - loss: 0.0218 - acc: 0.6956 - val_loss: 0.0161 - val_acc: 0.8228\n",
      "Epoch 4/10\n",
      "7997/7997 [==============================] - 2s 304us/step - loss: 0.0176 - acc: 0.7732 - val_loss: 0.0128 - val_acc: 0.8669\n",
      "Epoch 5/10\n",
      "7997/7997 [==============================] - 2s 294us/step - loss: 0.0149 - acc: 0.8143 - val_loss: 0.0110 - val_acc: 0.8889\n",
      "Epoch 6/10\n",
      "7997/7997 [==============================] - 2s 289us/step - loss: 0.0130 - acc: 0.8467 - val_loss: 0.0097 - val_acc: 0.9019\n",
      "Epoch 7/10\n",
      "7997/7997 [==============================] - 2s 297us/step - loss: 0.0118 - acc: 0.8627 - val_loss: 0.0089 - val_acc: 0.9059\n",
      "Epoch 8/10\n",
      "7997/7997 [==============================] - 2s 288us/step - loss: 0.0108 - acc: 0.8748 - val_loss: 0.0082 - val_acc: 0.9199\n",
      "Epoch 9/10\n",
      "7997/7997 [==============================] - 2s 285us/step - loss: 0.0099 - acc: 0.8865 - val_loss: 0.0077 - val_acc: 0.9189\n",
      "Epoch 10/10\n",
      "7997/7997 [==============================] - 2s 283us/step - loss: 0.0093 - acc: 0.8983 - val_loss: 0.0072 - val_acc: 0.9269\n",
      "Dropout: 0.06\n",
      "Train on 7997 samples, validate on 999 samples\n",
      "Epoch 1/10\n",
      "7997/7997 [==============================] - 4s 559us/step - loss: 0.0434 - acc: 0.2507 - val_loss: 0.0332 - val_acc: 0.4785\n",
      "Epoch 2/10\n",
      "7997/7997 [==============================] - 2s 289us/step - loss: 0.0294 - acc: 0.5481 - val_loss: 0.0224 - val_acc: 0.7017\n",
      "Epoch 3/10\n",
      "7997/7997 [==============================] - 2s 286us/step - loss: 0.0221 - acc: 0.6888 - val_loss: 0.0163 - val_acc: 0.8088\n",
      "Epoch 4/10\n",
      "7997/7997 [==============================] - 2s 285us/step - loss: 0.0178 - acc: 0.7724 - val_loss: 0.0130 - val_acc: 0.8549\n",
      "Epoch 5/10\n",
      "7997/7997 [==============================] - 2s 283us/step - loss: 0.0152 - acc: 0.8106 - val_loss: 0.0112 - val_acc: 0.8829\n",
      "Epoch 6/10\n",
      "7997/7997 [==============================] - 2s 284us/step - loss: 0.0132 - acc: 0.8409 - val_loss: 0.0098 - val_acc: 0.9009\n",
      "Epoch 7/10\n",
      "7997/7997 [==============================] - 2s 289us/step - loss: 0.0121 - acc: 0.8538 - val_loss: 0.0090 - val_acc: 0.9059\n",
      "Epoch 8/10\n",
      "7997/7997 [==============================] - 2s 286us/step - loss: 0.0110 - acc: 0.8713 - val_loss: 0.0083 - val_acc: 0.9169\n",
      "Epoch 9/10\n",
      "7997/7997 [==============================] - 2s 284us/step - loss: 0.0102 - acc: 0.8830 - val_loss: 0.0077 - val_acc: 0.9229\n",
      "Epoch 10/10\n",
      "7997/7997 [==============================] - 2s 283us/step - loss: 0.0095 - acc: 0.8928 - val_loss: 0.0072 - val_acc: 0.9289\n",
      "Dropout: 0.07\n",
      "Train on 7997 samples, validate on 999 samples\n",
      "Epoch 1/10\n",
      "7997/7997 [==============================] - 4s 545us/step - loss: 0.0436 - acc: 0.2466 - val_loss: 0.0333 - val_acc: 0.4825\n",
      "Epoch 2/10\n",
      "7997/7997 [==============================] - 2s 281us/step - loss: 0.0296 - acc: 0.5446 - val_loss: 0.0226 - val_acc: 0.6967\n",
      "Epoch 3/10\n",
      "7997/7997 [==============================] - 2s 276us/step - loss: 0.0224 - acc: 0.6839 - val_loss: 0.0164 - val_acc: 0.8098\n",
      "Epoch 4/10\n",
      "7997/7997 [==============================] - 2s 290us/step - loss: 0.0181 - acc: 0.7624 - val_loss: 0.0133 - val_acc: 0.8529\n",
      "Epoch 5/10\n",
      "7997/7997 [==============================] - 2s 285us/step - loss: 0.0154 - acc: 0.8057 - val_loss: 0.0112 - val_acc: 0.8759\n",
      "Epoch 6/10\n",
      "7997/7997 [==============================] - 2s 282us/step - loss: 0.0134 - acc: 0.8381 - val_loss: 0.0101 - val_acc: 0.8909\n",
      "Epoch 7/10\n",
      "7997/7997 [==============================] - 2s 279us/step - loss: 0.0123 - acc: 0.8513 - val_loss: 0.0093 - val_acc: 0.8969\n",
      "Epoch 8/10\n",
      "7997/7997 [==============================] - 2s 297us/step - loss: 0.0111 - acc: 0.8696 - val_loss: 0.0085 - val_acc: 0.9059\n",
      "Epoch 9/10\n",
      "7997/7997 [==============================] - 2s 289us/step - loss: 0.0104 - acc: 0.8820 - val_loss: 0.0080 - val_acc: 0.9159\n",
      "Epoch 10/10\n",
      "7997/7997 [==============================] - 2s 306us/step - loss: 0.0096 - acc: 0.8948 - val_loss: 0.0073 - val_acc: 0.9229\n",
      "Dropout: 0.08\n",
      "Train on 7997 samples, validate on 999 samples\n",
      "Epoch 1/10\n",
      "7997/7997 [==============================] - 5s 574us/step - loss: 0.0438 - acc: 0.2392 - val_loss: 0.0334 - val_acc: 0.4795\n",
      "Epoch 2/10\n",
      "7997/7997 [==============================] - 2s 278us/step - loss: 0.0298 - acc: 0.5426 - val_loss: 0.0228 - val_acc: 0.6887\n",
      "Epoch 3/10\n",
      "7997/7997 [==============================] - 2s 283us/step - loss: 0.0226 - acc: 0.6770 - val_loss: 0.0166 - val_acc: 0.8028\n",
      "Epoch 4/10\n",
      "7997/7997 [==============================] - 2s 291us/step - loss: 0.0184 - acc: 0.7573 - val_loss: 0.0133 - val_acc: 0.8539\n",
      "Epoch 5/10\n",
      "7997/7997 [==============================] - 2s 286us/step - loss: 0.0157 - acc: 0.8043 - val_loss: 0.0111 - val_acc: 0.8879\n",
      "Epoch 6/10\n",
      "7997/7997 [==============================] - 2s 279us/step - loss: 0.0137 - acc: 0.8306 - val_loss: 0.0100 - val_acc: 0.9019\n",
      "Epoch 7/10\n",
      "7997/7997 [==============================] - 2s 281us/step - loss: 0.0125 - acc: 0.8492 - val_loss: 0.0091 - val_acc: 0.9139\n",
      "Epoch 8/10\n",
      "7997/7997 [==============================] - 2s 283us/step - loss: 0.0114 - acc: 0.8676 - val_loss: 0.0084 - val_acc: 0.9149\n",
      "Epoch 9/10\n",
      "7997/7997 [==============================] - 2s 279us/step - loss: 0.0106 - acc: 0.8755 - val_loss: 0.0078 - val_acc: 0.9289\n",
      "Epoch 10/10\n",
      "7997/7997 [==============================] - 2s 282us/step - loss: 0.0098 - acc: 0.8923 - val_loss: 0.0072 - val_acc: 0.9299\n",
      "Dropout: 0.09\n",
      "Train on 7997 samples, validate on 999 samples\n",
      "Epoch 1/10\n",
      "7997/7997 [==============================] - 5s 570us/step - loss: 0.0439 - acc: 0.2380 - val_loss: 0.0336 - val_acc: 0.4825\n",
      "Epoch 2/10\n",
      "7997/7997 [==============================] - 2s 281us/step - loss: 0.0302 - acc: 0.5395 - val_loss: 0.0229 - val_acc: 0.6867\n",
      "Epoch 3/10\n",
      "7997/7997 [==============================] - 2s 282us/step - loss: 0.0230 - acc: 0.6709 - val_loss: 0.0165 - val_acc: 0.8138\n",
      "Epoch 4/10\n",
      "7997/7997 [==============================] - 2s 279us/step - loss: 0.0187 - acc: 0.7532 - val_loss: 0.0131 - val_acc: 0.8639\n",
      "Epoch 5/10\n",
      "7997/7997 [==============================] - 2s 274us/step - loss: 0.0160 - acc: 0.8013 - val_loss: 0.0110 - val_acc: 0.8829\n",
      "Epoch 6/10\n",
      "7997/7997 [==============================] - 2s 279us/step - loss: 0.0140 - acc: 0.8288 - val_loss: 0.0098 - val_acc: 0.8949\n",
      "Epoch 7/10\n",
      "7997/7997 [==============================] - 2s 295us/step - loss: 0.0127 - acc: 0.8457 - val_loss: 0.0089 - val_acc: 0.9049\n",
      "Epoch 8/10\n",
      "7997/7997 [==============================] - 2s 281us/step - loss: 0.0116 - acc: 0.8613 - val_loss: 0.0083 - val_acc: 0.9079\n",
      "Epoch 9/10\n",
      "7997/7997 [==============================] - 2s 288us/step - loss: 0.0107 - acc: 0.8740 - val_loss: 0.0078 - val_acc: 0.9169\n",
      "Epoch 10/10\n",
      "7997/7997 [==============================] - 2s 290us/step - loss: 0.0100 - acc: 0.8861 - val_loss: 0.0072 - val_acc: 0.9239\n",
      "Dropout: 0.1\n",
      "Train on 7997 samples, validate on 999 samples\n",
      "Epoch 1/10\n",
      "7997/7997 [==============================] - 5s 587us/step - loss: 0.0442 - acc: 0.2321 - val_loss: 0.0336 - val_acc: 0.4745\n",
      "Epoch 2/10\n",
      "7997/7997 [==============================] - 2s 297us/step - loss: 0.0304 - acc: 0.5373 - val_loss: 0.0231 - val_acc: 0.6827\n",
      "Epoch 3/10\n",
      "7997/7997 [==============================] - 2s 298us/step - loss: 0.0233 - acc: 0.6656 - val_loss: 0.0167 - val_acc: 0.7968\n",
      "Epoch 4/10\n",
      "7997/7997 [==============================] - 2s 298us/step - loss: 0.0189 - acc: 0.7508 - val_loss: 0.0132 - val_acc: 0.8589\n",
      "Epoch 5/10\n",
      "7997/7997 [==============================] - 2s 291us/step - loss: 0.0163 - acc: 0.7950 - val_loss: 0.0111 - val_acc: 0.8819\n",
      "Epoch 6/10\n",
      "7997/7997 [==============================] - 2s 297us/step - loss: 0.0143 - acc: 0.8244 - val_loss: 0.0100 - val_acc: 0.8979\n",
      "Epoch 7/10\n",
      "7997/7997 [==============================] - 2s 284us/step - loss: 0.0130 - acc: 0.8417 - val_loss: 0.0090 - val_acc: 0.9079\n",
      "Epoch 8/10\n",
      "7997/7997 [==============================] - 2s 294us/step - loss: 0.0120 - acc: 0.8589 - val_loss: 0.0084 - val_acc: 0.9089\n",
      "Epoch 9/10\n",
      "7997/7997 [==============================] - 2s 295us/step - loss: 0.0111 - acc: 0.8681 - val_loss: 0.0078 - val_acc: 0.9179\n",
      "Epoch 10/10\n",
      "7997/7997 [==============================] - 2s 292us/step - loss: 0.0103 - acc: 0.8810 - val_loss: 0.0073 - val_acc: 0.9209\n",
      "Dropout: 0.11\n",
      "Train on 7997 samples, validate on 999 samples\n",
      "Epoch 1/10\n",
      "7997/7997 [==============================] - 5s 580us/step - loss: 0.0443 - acc: 0.2337 - val_loss: 0.0338 - val_acc: 0.4715\n",
      "Epoch 2/10\n",
      "7997/7997 [==============================] - 2s 283us/step - loss: 0.0309 - acc: 0.5214 - val_loss: 0.0234 - val_acc: 0.6727\n",
      "Epoch 3/10\n",
      "7997/7997 [==============================] - 2s 287us/step - loss: 0.0237 - acc: 0.6597 - val_loss: 0.0170 - val_acc: 0.7988\n",
      "Epoch 4/10\n",
      "7997/7997 [==============================] - 2s 288us/step - loss: 0.0194 - acc: 0.7408 - val_loss: 0.0134 - val_acc: 0.8488\n",
      "Epoch 5/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7997/7997 [==============================] - 2s 283us/step - loss: 0.0165 - acc: 0.7858 - val_loss: 0.0112 - val_acc: 0.8769\n",
      "Epoch 6/10\n",
      "7997/7997 [==============================] - 2s 280us/step - loss: 0.0145 - acc: 0.8178 - val_loss: 0.0100 - val_acc: 0.8929\n",
      "Epoch 7/10\n",
      "7997/7997 [==============================] - 2s 279us/step - loss: 0.0132 - acc: 0.8381 - val_loss: 0.0090 - val_acc: 0.9039\n",
      "Epoch 8/10\n",
      "7997/7997 [==============================] - 2s 280us/step - loss: 0.0121 - acc: 0.8571 - val_loss: 0.0084 - val_acc: 0.9089\n",
      "Epoch 9/10\n",
      "7997/7997 [==============================] - 2s 275us/step - loss: 0.0112 - acc: 0.8676 - val_loss: 0.0077 - val_acc: 0.9199\n",
      "Epoch 10/10\n",
      "7997/7997 [==============================] - 2s 284us/step - loss: 0.0104 - acc: 0.8783 - val_loss: 0.0072 - val_acc: 0.9259\n",
      "Dropout: 0.12\n",
      "Train on 7997 samples, validate on 999 samples\n",
      "Epoch 1/10\n",
      "7997/7997 [==============================] - 5s 582us/step - loss: 0.0445 - acc: 0.2271 - val_loss: 0.0341 - val_acc: 0.4645\n",
      "Epoch 2/10\n",
      "7997/7997 [==============================] - 2s 279us/step - loss: 0.0311 - acc: 0.5153 - val_loss: 0.0235 - val_acc: 0.6707\n",
      "Epoch 3/10\n",
      "7997/7997 [==============================] - 2s 278us/step - loss: 0.0239 - acc: 0.6562 - val_loss: 0.0168 - val_acc: 0.8068\n",
      "Epoch 4/10\n",
      "7997/7997 [==============================] - 2s 275us/step - loss: 0.0194 - acc: 0.7433 - val_loss: 0.0135 - val_acc: 0.8448\n",
      "Epoch 5/10\n",
      "7997/7997 [==============================] - 2s 274us/step - loss: 0.0167 - acc: 0.7830 - val_loss: 0.0115 - val_acc: 0.8699\n",
      "Epoch 6/10\n",
      "7997/7997 [==============================] - 2s 275us/step - loss: 0.0147 - acc: 0.8183 - val_loss: 0.0100 - val_acc: 0.8929\n",
      "Epoch 7/10\n",
      "7997/7997 [==============================] - 2s 288us/step - loss: 0.0135 - acc: 0.8314 - val_loss: 0.0092 - val_acc: 0.8929\n",
      "Epoch 8/10\n",
      "7997/7997 [==============================] - 2s 278us/step - loss: 0.0122 - acc: 0.8517 - val_loss: 0.0084 - val_acc: 0.9089\n",
      "Epoch 9/10\n",
      "7997/7997 [==============================] - 2s 273us/step - loss: 0.0114 - acc: 0.8634 - val_loss: 0.0078 - val_acc: 0.9219\n",
      "Epoch 10/10\n",
      "7997/7997 [==============================] - 2s 279us/step - loss: 0.0107 - acc: 0.8727 - val_loss: 0.0072 - val_acc: 0.9239\n",
      "Dropout: 0.13\n",
      "Train on 7997 samples, validate on 999 samples\n",
      "Epoch 1/10\n",
      "7997/7997 [==============================] - 5s 570us/step - loss: 0.0446 - acc: 0.2212 - val_loss: 0.0345 - val_acc: 0.4555\n",
      "Epoch 2/10\n",
      "7997/7997 [==============================] - 2s 295us/step - loss: 0.0315 - acc: 0.5113 - val_loss: 0.0238 - val_acc: 0.6637\n",
      "Epoch 3/10\n",
      "7997/7997 [==============================] - 2s 283us/step - loss: 0.0243 - acc: 0.6512 - val_loss: 0.0171 - val_acc: 0.7888\n",
      "Epoch 4/10\n",
      "7997/7997 [==============================] - 2s 276us/step - loss: 0.0199 - acc: 0.7295 - val_loss: 0.0138 - val_acc: 0.8378\n",
      "Epoch 5/10\n",
      "7997/7997 [==============================] - 2s 286us/step - loss: 0.0171 - acc: 0.7814 - val_loss: 0.0116 - val_acc: 0.8739\n",
      "Epoch 6/10\n",
      "7997/7997 [==============================] - 2s 291us/step - loss: 0.0150 - acc: 0.8104 - val_loss: 0.0101 - val_acc: 0.8889\n",
      "Epoch 7/10\n",
      "7997/7997 [==============================] - 2s 293us/step - loss: 0.0137 - acc: 0.8303 - val_loss: 0.0093 - val_acc: 0.8999\n",
      "Epoch 8/10\n",
      "7997/7997 [==============================] - 2s 294us/step - loss: 0.0125 - acc: 0.8463 - val_loss: 0.0085 - val_acc: 0.9069\n",
      "Epoch 9/10\n",
      "7997/7997 [==============================] - 2s 294us/step - loss: 0.0116 - acc: 0.8636 - val_loss: 0.0077 - val_acc: 0.9229\n",
      "Epoch 10/10\n",
      "7997/7997 [==============================] - 2s 290us/step - loss: 0.0109 - acc: 0.8736 - val_loss: 0.0072 - val_acc: 0.9229\n",
      "Dropout: 0.14\n",
      "Train on 7997 samples, validate on 999 samples\n",
      "Epoch 1/10\n",
      "7997/7997 [==============================] - 5s 593us/step - loss: 0.0446 - acc: 0.2232 - val_loss: 0.0344 - val_acc: 0.4615\n",
      "Epoch 2/10\n",
      "7997/7997 [==============================] - 2s 282us/step - loss: 0.0318 - acc: 0.5051 - val_loss: 0.0238 - val_acc: 0.6707\n",
      "Epoch 3/10\n",
      "7997/7997 [==============================] - 2s 282us/step - loss: 0.0245 - acc: 0.6457 - val_loss: 0.0172 - val_acc: 0.7908\n",
      "Epoch 4/10\n",
      "7997/7997 [==============================] - 2s 284us/step - loss: 0.0201 - acc: 0.7264 - val_loss: 0.0136 - val_acc: 0.8438\n",
      "Epoch 5/10\n",
      "7997/7997 [==============================] - 2s 284us/step - loss: 0.0173 - acc: 0.7732 - val_loss: 0.0113 - val_acc: 0.8779\n",
      "Epoch 6/10\n",
      "7997/7997 [==============================] - 2s 286us/step - loss: 0.0152 - acc: 0.8088 - val_loss: 0.0098 - val_acc: 0.8959\n",
      "Epoch 7/10\n",
      "7997/7997 [==============================] - 2s 282us/step - loss: 0.0139 - acc: 0.8244 - val_loss: 0.0090 - val_acc: 0.8999\n",
      "Epoch 8/10\n",
      "7997/7997 [==============================] - 2s 284us/step - loss: 0.0127 - acc: 0.8456 - val_loss: 0.0081 - val_acc: 0.9079\n",
      "Epoch 9/10\n",
      "7997/7997 [==============================] - 2s 286us/step - loss: 0.0118 - acc: 0.8574 - val_loss: 0.0075 - val_acc: 0.9249\n",
      "Epoch 10/10\n",
      "7997/7997 [==============================] - 2s 281us/step - loss: 0.0110 - acc: 0.8701 - val_loss: 0.0070 - val_acc: 0.9319\n",
      "Dropout: 0.15\n",
      "Train on 7997 samples, validate on 999 samples\n",
      "Epoch 1/10\n",
      "7997/7997 [==============================] - 5s 570us/step - loss: 0.0448 - acc: 0.2187 - val_loss: 0.0348 - val_acc: 0.4505\n",
      "Epoch 2/10\n",
      "7997/7997 [==============================] - 2s 281us/step - loss: 0.0321 - acc: 0.4954 - val_loss: 0.0238 - val_acc: 0.6647\n",
      "Epoch 3/10\n",
      "7997/7997 [==============================] - 2s 282us/step - loss: 0.0249 - acc: 0.6386 - val_loss: 0.0174 - val_acc: 0.7818\n",
      "Epoch 4/10\n",
      "7997/7997 [==============================] - 2s 284us/step - loss: 0.0206 - acc: 0.7178 - val_loss: 0.0138 - val_acc: 0.8418\n",
      "Epoch 5/10\n",
      "7997/7997 [==============================] - 2s 283us/step - loss: 0.0176 - acc: 0.7725 - val_loss: 0.0115 - val_acc: 0.8669\n",
      "Epoch 6/10\n",
      "7997/7997 [==============================] - 2s 283us/step - loss: 0.0154 - acc: 0.8038 - val_loss: 0.0101 - val_acc: 0.8839\n",
      "Epoch 7/10\n",
      "7997/7997 [==============================] - 2s 282us/step - loss: 0.0141 - acc: 0.8239 - val_loss: 0.0091 - val_acc: 0.9039\n",
      "Epoch 8/10\n",
      "7997/7997 [==============================] - 2s 284us/step - loss: 0.0129 - acc: 0.8422 - val_loss: 0.0084 - val_acc: 0.9089\n",
      "Epoch 9/10\n",
      "7997/7997 [==============================] - 2s 281us/step - loss: 0.0120 - acc: 0.8583 - val_loss: 0.0078 - val_acc: 0.9149\n",
      "Epoch 10/10\n",
      "7997/7997 [==============================] - 2s 311us/step - loss: 0.0112 - acc: 0.8675 - val_loss: 0.0071 - val_acc: 0.9239\n",
      "Dropout: 0.16\n",
      "Train on 7997 samples, validate on 999 samples\n",
      "Epoch 1/10\n",
      "7997/7997 [==============================] - 5s 574us/step - loss: 0.0449 - acc: 0.2207 - val_loss: 0.0349 - val_acc: 0.4364\n",
      "Epoch 2/10\n",
      "7997/7997 [==============================] - 2s 285us/step - loss: 0.0324 - acc: 0.4892 - val_loss: 0.0240 - val_acc: 0.6597\n",
      "Epoch 3/10\n",
      "7997/7997 [==============================] - 2s 293us/step - loss: 0.0251 - acc: 0.6351 - val_loss: 0.0175 - val_acc: 0.7778\n",
      "Epoch 4/10\n",
      "7997/7997 [==============================] - 2s 296us/step - loss: 0.0208 - acc: 0.7153 - val_loss: 0.0138 - val_acc: 0.8458\n",
      "Epoch 5/10\n",
      "7997/7997 [==============================] - 2s 300us/step - loss: 0.0179 - acc: 0.7655 - val_loss: 0.0114 - val_acc: 0.8759\n",
      "Epoch 6/10\n",
      "7997/7997 [==============================] - 2s 277us/step - loss: 0.0157 - acc: 0.7997 - val_loss: 0.0101 - val_acc: 0.8899\n",
      "Epoch 7/10\n",
      "7997/7997 [==============================] - 3s 322us/step - loss: 0.0143 - acc: 0.8179 - val_loss: 0.0091 - val_acc: 0.9029\n",
      "Epoch 8/10\n",
      "7997/7997 [==============================] - 3s 313us/step - loss: 0.0132 - acc: 0.8359 - val_loss: 0.0083 - val_acc: 0.9099\n",
      "Epoch 9/10\n",
      "7997/7997 [==============================] - 2s 295us/step - loss: 0.0122 - acc: 0.8508 - val_loss: 0.0077 - val_acc: 0.9139\n",
      "Epoch 10/10\n",
      "7997/7997 [==============================] - 2s 279us/step - loss: 0.0114 - acc: 0.8611 - val_loss: 0.0072 - val_acc: 0.9309\n",
      "Dropout: 0.17\n",
      "Train on 7997 samples, validate on 999 samples\n",
      "Epoch 1/10\n",
      "7997/7997 [==============================] - 5s 582us/step - loss: 0.0451 - acc: 0.2117 - val_loss: 0.0350 - val_acc: 0.4434\n",
      "Epoch 2/10\n",
      "7997/7997 [==============================] - 2s 286us/step - loss: 0.0327 - acc: 0.4826 - val_loss: 0.0242 - val_acc: 0.6547\n",
      "Epoch 3/10\n",
      "7997/7997 [==============================] - 2s 303us/step - loss: 0.0256 - acc: 0.6222 - val_loss: 0.0176 - val_acc: 0.7798\n",
      "Epoch 4/10\n",
      "7997/7997 [==============================] - 2s 312us/step - loss: 0.0214 - acc: 0.7061 - val_loss: 0.0139 - val_acc: 0.8378\n",
      "Epoch 5/10\n",
      "7997/7997 [==============================] - 2s 289us/step - loss: 0.0184 - acc: 0.7515 - val_loss: 0.0114 - val_acc: 0.8759\n",
      "Epoch 6/10\n",
      "7997/7997 [==============================] - 2s 292us/step - loss: 0.0161 - acc: 0.7907 - val_loss: 0.0099 - val_acc: 0.8939\n",
      "Epoch 7/10\n",
      "7997/7997 [==============================] - 2s 295us/step - loss: 0.0147 - acc: 0.8122 - val_loss: 0.0090 - val_acc: 0.8999\n",
      "Epoch 8/10\n",
      "7997/7997 [==============================] - 2s 277us/step - loss: 0.0134 - acc: 0.8329 - val_loss: 0.0081 - val_acc: 0.9119\n",
      "Epoch 9/10\n",
      "7997/7997 [==============================] - 2s 291us/step - loss: 0.0125 - acc: 0.8462 - val_loss: 0.0076 - val_acc: 0.9189\n",
      "Epoch 10/10\n",
      "7997/7997 [==============================] - 2s 287us/step - loss: 0.0116 - acc: 0.8603 - val_loss: 0.0070 - val_acc: 0.9229\n",
      "Dropout: 0.18\n",
      "Train on 7997 samples, validate on 999 samples\n",
      "Epoch 1/10\n",
      "7997/7997 [==============================] - 5s 597us/step - loss: 0.0452 - acc: 0.2113 - val_loss: 0.0354 - val_acc: 0.4254\n",
      "Epoch 2/10\n",
      "7997/7997 [==============================] - 2s 297us/step - loss: 0.0331 - acc: 0.4773 - val_loss: 0.0242 - val_acc: 0.6587\n",
      "Epoch 3/10\n",
      "7997/7997 [==============================] - 3s 318us/step - loss: 0.0259 - acc: 0.6202 - val_loss: 0.0177 - val_acc: 0.7768\n",
      "Epoch 4/10\n",
      "7997/7997 [==============================] - 3s 320us/step - loss: 0.0216 - acc: 0.7005 - val_loss: 0.0139 - val_acc: 0.8438\n",
      "Epoch 5/10\n",
      "7997/7997 [==============================] - 3s 361us/step - loss: 0.0186 - acc: 0.7508 - val_loss: 0.0116 - val_acc: 0.8719\n",
      "Epoch 6/10\n",
      "7997/7997 [==============================] - 2s 283us/step - loss: 0.0162 - acc: 0.7908 - val_loss: 0.0102 - val_acc: 0.8859\n",
      "Epoch 7/10\n",
      "7997/7997 [==============================] - 2s 283us/step - loss: 0.0148 - acc: 0.8128 - val_loss: 0.0091 - val_acc: 0.9059\n",
      "Epoch 8/10\n",
      "7997/7997 [==============================] - 3s 314us/step - loss: 0.0136 - acc: 0.8316 - val_loss: 0.0085 - val_acc: 0.9039\n",
      "Epoch 9/10\n",
      "7997/7997 [==============================] - 2s 292us/step - loss: 0.0127 - acc: 0.8436 - val_loss: 0.0077 - val_acc: 0.9149\n",
      "Epoch 10/10\n",
      "7997/7997 [==============================] - 2s 305us/step - loss: 0.0118 - acc: 0.8573 - val_loss: 0.0071 - val_acc: 0.9239\n",
      "Dropout: 0.19\n",
      "Train on 7997 samples, validate on 999 samples\n",
      "Epoch 1/10\n",
      "7997/7997 [==============================] - 5s 671us/step - loss: 0.0454 - acc: 0.2073 - val_loss: 0.0355 - val_acc: 0.4344\n",
      "Epoch 2/10\n",
      "7997/7997 [==============================] - 2s 305us/step - loss: 0.0334 - acc: 0.4665 - val_loss: 0.0246 - val_acc: 0.6567\n",
      "Epoch 3/10\n",
      "7997/7997 [==============================] - 2s 298us/step - loss: 0.0263 - acc: 0.6096 - val_loss: 0.0183 - val_acc: 0.7508\n",
      "Epoch 4/10\n",
      "7997/7997 [==============================] - 2s 308us/step - loss: 0.0220 - acc: 0.6951 - val_loss: 0.0143 - val_acc: 0.8348\n",
      "Epoch 5/10\n",
      "7997/7997 [==============================] - 2s 294us/step - loss: 0.0190 - acc: 0.7477 - val_loss: 0.0119 - val_acc: 0.8639\n",
      "Epoch 6/10\n",
      "7997/7997 [==============================] - 2s 288us/step - loss: 0.0166 - acc: 0.7828 - val_loss: 0.0104 - val_acc: 0.8849\n",
      "Epoch 7/10\n",
      "7997/7997 [==============================] - 2s 298us/step - loss: 0.0152 - acc: 0.8074 - val_loss: 0.0094 - val_acc: 0.8989\n",
      "Epoch 8/10\n",
      "7997/7997 [==============================] - 2s 309us/step - loss: 0.0140 - acc: 0.8257 - val_loss: 0.0086 - val_acc: 0.8999\n",
      "Epoch 9/10\n",
      "7997/7997 [==============================] - 2s 306us/step - loss: 0.0131 - acc: 0.8389 - val_loss: 0.0078 - val_acc: 0.9139\n",
      "Epoch 10/10\n",
      "7997/7997 [==============================] - 2s 308us/step - loss: 0.0122 - acc: 0.8501 - val_loss: 0.0072 - val_acc: 0.9219\n",
      "Dropout: 0.2\n",
      "Train on 7997 samples, validate on 999 samples\n",
      "Epoch 1/10\n",
      "7997/7997 [==============================] - 5s 596us/step - loss: 0.0455 - acc: 0.2030 - val_loss: 0.0359 - val_acc: 0.4284\n",
      "Epoch 2/10\n",
      "7997/7997 [==============================] - 2s 282us/step - loss: 0.0339 - acc: 0.4504 - val_loss: 0.0249 - val_acc: 0.6456\n",
      "Epoch 3/10\n",
      "7997/7997 [==============================] - 2s 285us/step - loss: 0.0267 - acc: 0.6061 - val_loss: 0.0183 - val_acc: 0.7708\n",
      "Epoch 4/10\n",
      "7997/7997 [==============================] - 2s 284us/step - loss: 0.0223 - acc: 0.6855 - val_loss: 0.0142 - val_acc: 0.8368\n",
      "Epoch 5/10\n",
      "7997/7997 [==============================] - 2s 281us/step - loss: 0.0192 - acc: 0.7392 - val_loss: 0.0117 - val_acc: 0.8729\n",
      "Epoch 6/10\n",
      "7997/7997 [==============================] - 2s 279us/step - loss: 0.0168 - acc: 0.7793 - val_loss: 0.0102 - val_acc: 0.8859\n",
      "Epoch 7/10\n",
      "7997/7997 [==============================] - 2s 279us/step - loss: 0.0154 - acc: 0.8059 - val_loss: 0.0090 - val_acc: 0.9029\n",
      "Epoch 8/10\n",
      "7997/7997 [==============================] - 2s 280us/step - loss: 0.0141 - acc: 0.8236 - val_loss: 0.0083 - val_acc: 0.9059\n",
      "Epoch 9/10\n",
      "7997/7997 [==============================] - 2s 280us/step - loss: 0.0133 - acc: 0.8371 - val_loss: 0.0076 - val_acc: 0.9169\n",
      "Epoch 10/10\n",
      "7997/7997 [==============================] - 2s 281us/step - loss: 0.0123 - acc: 0.8488 - val_loss: 0.0069 - val_acc: 0.9299\n",
      "Dropout: 0.21\n",
      "Train on 7997 samples, validate on 999 samples\n",
      "Epoch 1/10\n",
      "7997/7997 [==============================] - 5s 606us/step - loss: 0.0456 - acc: 0.2002 - val_loss: 0.0358 - val_acc: 0.4204\n",
      "Epoch 2/10\n",
      "7997/7997 [==============================] - 2s 285us/step - loss: 0.0341 - acc: 0.4484 - val_loss: 0.0248 - val_acc: 0.6416\n",
      "Epoch 3/10\n",
      "7997/7997 [==============================] - 2s 294us/step - loss: 0.0270 - acc: 0.5891 - val_loss: 0.0185 - val_acc: 0.7618\n",
      "Epoch 4/10\n",
      "7997/7997 [==============================] - 2s 291us/step - loss: 0.0227 - acc: 0.6803 - val_loss: 0.0144 - val_acc: 0.8248\n",
      "Epoch 5/10\n",
      "7997/7997 [==============================] - 2s 290us/step - loss: 0.0196 - acc: 0.7343 - val_loss: 0.0120 - val_acc: 0.8539\n",
      "Epoch 6/10\n",
      "7997/7997 [==============================] - 2s 283us/step - loss: 0.0173 - acc: 0.7690 - val_loss: 0.0103 - val_acc: 0.8769\n",
      "Epoch 7/10\n",
      "7997/7997 [==============================] - 3s 313us/step - loss: 0.0157 - acc: 0.7984 - val_loss: 0.0092 - val_acc: 0.8869\n",
      "Epoch 8/10\n",
      "7997/7997 [==============================] - 2s 296us/step - loss: 0.0144 - acc: 0.8149 - val_loss: 0.0084 - val_acc: 0.9029\n",
      "Epoch 9/10\n",
      "7997/7997 [==============================] - 2s 289us/step - loss: 0.0136 - acc: 0.8306 - val_loss: 0.0078 - val_acc: 0.9059\n",
      "Epoch 10/10\n",
      "7997/7997 [==============================] - 2s 289us/step - loss: 0.0126 - acc: 0.8444 - val_loss: 0.0072 - val_acc: 0.9149\n",
      "Dropout: 0.22\n",
      "Train on 7997 samples, validate on 999 samples\n",
      "Epoch 1/10\n",
      "7997/7997 [==============================] - 5s 596us/step - loss: 0.0458 - acc: 0.1933 - val_loss: 0.0361 - val_acc: 0.4174\n",
      "Epoch 2/10\n",
      "7997/7997 [==============================] - 2s 278us/step - loss: 0.0346 - acc: 0.4389 - val_loss: 0.0254 - val_acc: 0.6216\n",
      "Epoch 3/10\n",
      "7997/7997 [==============================] - 2s 280us/step - loss: 0.0274 - acc: 0.5888 - val_loss: 0.0186 - val_acc: 0.7598\n",
      "Epoch 4/10\n",
      "7997/7997 [==============================] - 2s 280us/step - loss: 0.0230 - acc: 0.6750 - val_loss: 0.0146 - val_acc: 0.8198\n",
      "Epoch 5/10\n",
      "7997/7997 [==============================] - 2s 279us/step - loss: 0.0199 - acc: 0.7305 - val_loss: 0.0121 - val_acc: 0.8539\n",
      "Epoch 6/10\n",
      "7997/7997 [==============================] - 2s 279us/step - loss: 0.0176 - acc: 0.7665 - val_loss: 0.0106 - val_acc: 0.8759\n",
      "Epoch 7/10\n",
      "7997/7997 [==============================] - 2s 280us/step - loss: 0.0160 - acc: 0.7910 - val_loss: 0.0095 - val_acc: 0.8819\n",
      "Epoch 8/10\n",
      "7997/7997 [==============================] - 2s 278us/step - loss: 0.0147 - acc: 0.8107 - val_loss: 0.0086 - val_acc: 0.8979\n",
      "Epoch 9/10\n",
      "7997/7997 [==============================] - 2s 279us/step - loss: 0.0138 - acc: 0.8272 - val_loss: 0.0079 - val_acc: 0.9049\n",
      "Epoch 10/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7997/7997 [==============================] - 2s 279us/step - loss: 0.0129 - acc: 0.8389 - val_loss: 0.0073 - val_acc: 0.9139\n",
      "Dropout: 0.23\n",
      "Train on 7997 samples, validate on 999 samples\n",
      "Epoch 1/10\n",
      "7997/7997 [==============================] - 5s 606us/step - loss: 0.0459 - acc: 0.1901 - val_loss: 0.0364 - val_acc: 0.4154\n",
      "Epoch 2/10\n",
      "7997/7997 [==============================] - 2s 282us/step - loss: 0.0350 - acc: 0.4280 - val_loss: 0.0257 - val_acc: 0.6176\n",
      "Epoch 3/10\n",
      "7997/7997 [==============================] - 2s 281us/step - loss: 0.0280 - acc: 0.5718 - val_loss: 0.0188 - val_acc: 0.7497\n",
      "Epoch 4/10\n",
      "7997/7997 [==============================] - 2s 284us/step - loss: 0.0234 - acc: 0.6650 - val_loss: 0.0148 - val_acc: 0.8138\n",
      "Epoch 5/10\n",
      "7997/7997 [==============================] - 2s 282us/step - loss: 0.0203 - acc: 0.7248 - val_loss: 0.0121 - val_acc: 0.8689\n",
      "Epoch 6/10\n",
      "7997/7997 [==============================] - 2s 282us/step - loss: 0.0179 - acc: 0.7642 - val_loss: 0.0103 - val_acc: 0.8879\n",
      "Epoch 7/10\n",
      "7997/7997 [==============================] - 2s 282us/step - loss: 0.0162 - acc: 0.7928 - val_loss: 0.0093 - val_acc: 0.8989\n",
      "Epoch 8/10\n",
      "7997/7997 [==============================] - 2s 281us/step - loss: 0.0149 - acc: 0.8049 - val_loss: 0.0085 - val_acc: 0.8989\n",
      "Epoch 9/10\n",
      "7997/7997 [==============================] - 2s 281us/step - loss: 0.0139 - acc: 0.8277 - val_loss: 0.0077 - val_acc: 0.9149\n",
      "Epoch 10/10\n",
      "7997/7997 [==============================] - 2s 283us/step - loss: 0.0130 - acc: 0.8389 - val_loss: 0.0072 - val_acc: 0.9179\n",
      "Dropout: 0.24\n",
      "Train on 7997 samples, validate on 999 samples\n",
      "Epoch 1/10\n",
      "7997/7997 [==============================] - 5s 598us/step - loss: 0.0461 - acc: 0.1894 - val_loss: 0.0366 - val_acc: 0.4234\n",
      "Epoch 2/10\n",
      "7997/7997 [==============================] - 2s 282us/step - loss: 0.0353 - acc: 0.4224 - val_loss: 0.0259 - val_acc: 0.6296\n",
      "Epoch 3/10\n",
      "7997/7997 [==============================] - 2s 281us/step - loss: 0.0283 - acc: 0.5698 - val_loss: 0.0195 - val_acc: 0.7327\n",
      "Epoch 4/10\n",
      "7997/7997 [==============================] - 2s 283us/step - loss: 0.0238 - acc: 0.6579 - val_loss: 0.0152 - val_acc: 0.8088\n",
      "Epoch 5/10\n",
      "7997/7997 [==============================] - 2s 287us/step - loss: 0.0206 - acc: 0.7179 - val_loss: 0.0123 - val_acc: 0.8529\n",
      "Epoch 6/10\n",
      "7997/7997 [==============================] - 2s 303us/step - loss: 0.0182 - acc: 0.7562 - val_loss: 0.0105 - val_acc: 0.8819\n",
      "Epoch 7/10\n",
      "7997/7997 [==============================] - 2s 308us/step - loss: 0.0165 - acc: 0.7815 - val_loss: 0.0095 - val_acc: 0.8949\n",
      "Epoch 8/10\n",
      "7997/7997 [==============================] - 3s 314us/step - loss: 0.0152 - acc: 0.8051 - val_loss: 0.0086 - val_acc: 0.9029\n",
      "Epoch 9/10\n",
      "7997/7997 [==============================] - 2s 299us/step - loss: 0.0142 - acc: 0.8209 - val_loss: 0.0079 - val_acc: 0.9109\n",
      "Epoch 10/10\n",
      "7997/7997 [==============================] - 2s 298us/step - loss: 0.0133 - acc: 0.8356 - val_loss: 0.0073 - val_acc: 0.9199\n",
      "Dropout: 0.25\n",
      "Train on 7997 samples, validate on 999 samples\n",
      "Epoch 1/10\n",
      "7997/7997 [==============================] - 5s 654us/step - loss: 0.0463 - acc: 0.1831 - val_loss: 0.0369 - val_acc: 0.4014\n",
      "Epoch 2/10\n",
      "7997/7997 [==============================] - 2s 284us/step - loss: 0.0357 - acc: 0.4145 - val_loss: 0.0263 - val_acc: 0.6016\n",
      "Epoch 3/10\n",
      "7997/7997 [==============================] - 2s 297us/step - loss: 0.0288 - acc: 0.5622 - val_loss: 0.0196 - val_acc: 0.7357\n",
      "Epoch 4/10\n",
      "7997/7997 [==============================] - 2s 297us/step - loss: 0.0243 - acc: 0.6474 - val_loss: 0.0154 - val_acc: 0.8088\n",
      "Epoch 5/10\n",
      "7997/7997 [==============================] - 2s 280us/step - loss: 0.0211 - acc: 0.7063 - val_loss: 0.0125 - val_acc: 0.8468\n",
      "Epoch 6/10\n",
      "7997/7997 [==============================] - 2s 279us/step - loss: 0.0185 - acc: 0.7495 - val_loss: 0.0108 - val_acc: 0.8689\n",
      "Epoch 7/10\n",
      "7997/7997 [==============================] - 2s 281us/step - loss: 0.0169 - acc: 0.7807 - val_loss: 0.0095 - val_acc: 0.8849\n",
      "Epoch 8/10\n",
      "7997/7997 [==============================] - 2s 277us/step - loss: 0.0154 - acc: 0.8027 - val_loss: 0.0088 - val_acc: 0.8969\n",
      "Epoch 9/10\n",
      "7997/7997 [==============================] - 2s 277us/step - loss: 0.0144 - acc: 0.8158 - val_loss: 0.0080 - val_acc: 0.9089\n",
      "Epoch 10/10\n",
      "7997/7997 [==============================] - 2s 285us/step - loss: 0.0135 - acc: 0.8304 - val_loss: 0.0076 - val_acc: 0.9119\n",
      "Dropout: 0.26\n",
      "Train on 7997 samples, validate on 999 samples\n",
      "Epoch 1/10\n",
      "7997/7997 [==============================] - 5s 664us/step - loss: 0.0463 - acc: 0.1813 - val_loss: 0.0369 - val_acc: 0.4094\n",
      "Epoch 2/10\n",
      "7997/7997 [==============================] - 2s 278us/step - loss: 0.0357 - acc: 0.4123 - val_loss: 0.0265 - val_acc: 0.6046\n",
      "Epoch 3/10\n",
      "7997/7997 [==============================] - 2s 277us/step - loss: 0.0290 - acc: 0.5613 - val_loss: 0.0195 - val_acc: 0.7357\n",
      "Epoch 4/10\n",
      "7997/7997 [==============================] - 2s 277us/step - loss: 0.0244 - acc: 0.6427 - val_loss: 0.0154 - val_acc: 0.8028\n",
      "Epoch 5/10\n",
      "7997/7997 [==============================] - 2s 276us/step - loss: 0.0212 - acc: 0.7045 - val_loss: 0.0124 - val_acc: 0.8468\n",
      "Epoch 6/10\n",
      "7997/7997 [==============================] - 2s 275us/step - loss: 0.0187 - acc: 0.7470 - val_loss: 0.0108 - val_acc: 0.8669\n",
      "Epoch 7/10\n",
      "7997/7997 [==============================] - 2s 295us/step - loss: 0.0171 - acc: 0.7755 - val_loss: 0.0095 - val_acc: 0.8969\n",
      "Epoch 8/10\n",
      "7997/7997 [==============================] - 2s 285us/step - loss: 0.0157 - acc: 0.7967 - val_loss: 0.0088 - val_acc: 0.8989\n",
      "Epoch 9/10\n",
      "7997/7997 [==============================] - 2s 285us/step - loss: 0.0147 - acc: 0.8141 - val_loss: 0.0079 - val_acc: 0.9019\n",
      "Epoch 10/10\n",
      "7997/7997 [==============================] - 2s 283us/step - loss: 0.0137 - acc: 0.8273 - val_loss: 0.0075 - val_acc: 0.9129\n",
      "Dropout: 0.27\n",
      "Train on 7997 samples, validate on 999 samples\n",
      "Epoch 1/10\n",
      "7997/7997 [==============================] - 5s 638us/step - loss: 0.0465 - acc: 0.1763 - val_loss: 0.0372 - val_acc: 0.3864\n",
      "Epoch 2/10\n",
      "7997/7997 [==============================] - 2s 282us/step - loss: 0.0363 - acc: 0.4005 - val_loss: 0.0267 - val_acc: 0.6026\n",
      "Epoch 3/10\n",
      "7997/7997 [==============================] - 2s 289us/step - loss: 0.0294 - acc: 0.5466 - val_loss: 0.0200 - val_acc: 0.7217\n",
      "Epoch 4/10\n",
      "7997/7997 [==============================] - 2s 289us/step - loss: 0.0250 - acc: 0.6357 - val_loss: 0.0158 - val_acc: 0.8008\n",
      "Epoch 5/10\n",
      "7997/7997 [==============================] - 2s 280us/step - loss: 0.0218 - acc: 0.6955 - val_loss: 0.0130 - val_acc: 0.8438\n",
      "Epoch 6/10\n",
      "7997/7997 [==============================] - 2s 277us/step - loss: 0.0194 - acc: 0.7365 - val_loss: 0.0110 - val_acc: 0.8719\n",
      "Epoch 7/10\n",
      "7997/7997 [==============================] - 2s 278us/step - loss: 0.0176 - acc: 0.7670 - val_loss: 0.0096 - val_acc: 0.8899\n",
      "Epoch 8/10\n",
      "7997/7997 [==============================] - 2s 279us/step - loss: 0.0162 - acc: 0.7878 - val_loss: 0.0088 - val_acc: 0.8939\n",
      "Epoch 9/10\n",
      "7997/7997 [==============================] - 2s 285us/step - loss: 0.0151 - acc: 0.8072 - val_loss: 0.0079 - val_acc: 0.9149\n",
      "Epoch 10/10\n",
      "7997/7997 [==============================] - 2s 280us/step - loss: 0.0142 - acc: 0.8232 - val_loss: 0.0072 - val_acc: 0.9259\n",
      "Dropout: 0.28\n",
      "Train on 7997 samples, validate on 999 samples\n",
      "Epoch 1/10\n",
      "7997/7997 [==============================] - 5s 643us/step - loss: 0.0466 - acc: 0.1724 - val_loss: 0.0374 - val_acc: 0.3904\n",
      "Epoch 2/10\n",
      "7997/7997 [==============================] - 2s 294us/step - loss: 0.0366 - acc: 0.3921 - val_loss: 0.0272 - val_acc: 0.5856\n",
      "Epoch 3/10\n",
      "7997/7997 [==============================] - 2s 282us/step - loss: 0.0300 - acc: 0.5360 - val_loss: 0.0200 - val_acc: 0.7367\n",
      "Epoch 4/10\n",
      "7997/7997 [==============================] - 2s 280us/step - loss: 0.0253 - acc: 0.6302 - val_loss: 0.0156 - val_acc: 0.8088\n",
      "Epoch 5/10\n",
      "7997/7997 [==============================] - 2s 281us/step - loss: 0.0221 - acc: 0.6901 - val_loss: 0.0127 - val_acc: 0.8549\n",
      "Epoch 6/10\n",
      "7997/7997 [==============================] - 2s 283us/step - loss: 0.0195 - acc: 0.7373 - val_loss: 0.0109 - val_acc: 0.8719\n",
      "Epoch 7/10\n",
      "7997/7997 [==============================] - 2s 288us/step - loss: 0.0178 - acc: 0.7629 - val_loss: 0.0098 - val_acc: 0.8849\n",
      "Epoch 8/10\n",
      "7997/7997 [==============================] - 2s 281us/step - loss: 0.0165 - acc: 0.7808 - val_loss: 0.0088 - val_acc: 0.8929\n",
      "Epoch 9/10\n",
      "7997/7997 [==============================] - 2s 280us/step - loss: 0.0153 - acc: 0.8046 - val_loss: 0.0082 - val_acc: 0.9019\n",
      "Epoch 10/10\n",
      "7997/7997 [==============================] - 2s 281us/step - loss: 0.0144 - acc: 0.8263 - val_loss: 0.0077 - val_acc: 0.9099\n",
      "Dropout: 0.29\n",
      "Train on 7997 samples, validate on 999 samples\n",
      "Epoch 1/10\n",
      "7997/7997 [==============================] - 5s 673us/step - loss: 0.0467 - acc: 0.1697 - val_loss: 0.0375 - val_acc: 0.3874\n",
      "Epoch 2/10\n",
      "7997/7997 [==============================] - 2s 294us/step - loss: 0.0369 - acc: 0.3871 - val_loss: 0.0271 - val_acc: 0.6016\n",
      "Epoch 3/10\n",
      "7997/7997 [==============================] - 2s 281us/step - loss: 0.0302 - acc: 0.5288 - val_loss: 0.0203 - val_acc: 0.7117\n",
      "Epoch 4/10\n",
      "7997/7997 [==============================] - 2s 284us/step - loss: 0.0257 - acc: 0.6226 - val_loss: 0.0159 - val_acc: 0.7968\n",
      "Epoch 5/10\n",
      "7997/7997 [==============================] - 2s 293us/step - loss: 0.0223 - acc: 0.6848 - val_loss: 0.0127 - val_acc: 0.8438\n",
      "Epoch 6/10\n",
      "7997/7997 [==============================] - 2s 285us/step - loss: 0.0198 - acc: 0.7286 - val_loss: 0.0107 - val_acc: 0.8729\n",
      "Epoch 7/10\n",
      "7997/7997 [==============================] - 2s 285us/step - loss: 0.0181 - acc: 0.7585 - val_loss: 0.0097 - val_acc: 0.8929\n",
      "Epoch 8/10\n",
      "7997/7997 [==============================] - 2s 284us/step - loss: 0.0167 - acc: 0.7800 - val_loss: 0.0090 - val_acc: 0.8929\n",
      "Epoch 9/10\n",
      "7997/7997 [==============================] - 2s 301us/step - loss: 0.0156 - acc: 0.7984 - val_loss: 0.0081 - val_acc: 0.9119\n",
      "Epoch 10/10\n",
      "7997/7997 [==============================] - 2s 295us/step - loss: 0.0147 - acc: 0.8129 - val_loss: 0.0077 - val_acc: 0.9109\n",
      "Dropout: 0.3\n",
      "Train on 7997 samples, validate on 999 samples\n",
      "Epoch 1/10\n",
      "7997/7997 [==============================] - 5s 658us/step - loss: 0.0467 - acc: 0.1707 - val_loss: 0.0378 - val_acc: 0.3764\n",
      "Epoch 2/10\n",
      "7997/7997 [==============================] - 2s 303us/step - loss: 0.0373 - acc: 0.3756 - val_loss: 0.0268 - val_acc: 0.6086\n",
      "Epoch 3/10\n",
      "7997/7997 [==============================] - 2s 297us/step - loss: 0.0306 - acc: 0.5226 - val_loss: 0.0203 - val_acc: 0.7127\n",
      "Epoch 4/10\n",
      "7997/7997 [==============================] - 2s 292us/step - loss: 0.0260 - acc: 0.6145 - val_loss: 0.0160 - val_acc: 0.7938\n",
      "Epoch 5/10\n",
      "7997/7997 [==============================] - 2s 291us/step - loss: 0.0227 - acc: 0.6819 - val_loss: 0.0129 - val_acc: 0.8468\n",
      "Epoch 6/10\n",
      "7997/7997 [==============================] - 2s 287us/step - loss: 0.0201 - acc: 0.7256 - val_loss: 0.0111 - val_acc: 0.8739\n",
      "Epoch 7/10\n",
      "7997/7997 [==============================] - 2s 283us/step - loss: 0.0184 - acc: 0.7518 - val_loss: 0.0097 - val_acc: 0.8879\n",
      "Epoch 8/10\n",
      "7997/7997 [==============================] - 2s 282us/step - loss: 0.0170 - acc: 0.7803 - val_loss: 0.0088 - val_acc: 0.8929\n",
      "Epoch 9/10\n",
      "7997/7997 [==============================] - 2s 285us/step - loss: 0.0158 - acc: 0.7993 - val_loss: 0.0079 - val_acc: 0.9069\n",
      "Epoch 10/10\n",
      "7997/7997 [==============================] - 2s 282us/step - loss: 0.0149 - acc: 0.8102 - val_loss: 0.0073 - val_acc: 0.9119\n",
      "Dropout: 0.31\n",
      "Train on 7997 samples, validate on 999 samples\n",
      "Epoch 1/10\n",
      "7997/7997 [==============================] - 5s 677us/step - loss: 0.0468 - acc: 0.1678 - val_loss: 0.0380 - val_acc: 0.3714\n",
      "Epoch 2/10\n",
      "7997/7997 [==============================] - 3s 313us/step - loss: 0.0375 - acc: 0.3686 - val_loss: 0.0273 - val_acc: 0.5926\n",
      "Epoch 3/10\n",
      "7997/7997 [==============================] - 2s 280us/step - loss: 0.0309 - acc: 0.5176 - val_loss: 0.0209 - val_acc: 0.7167\n",
      "Epoch 4/10\n",
      "7997/7997 [==============================] - 2s 284us/step - loss: 0.0265 - acc: 0.6065 - val_loss: 0.0163 - val_acc: 0.7918\n",
      "Epoch 5/10\n",
      "7997/7997 [==============================] - 2s 288us/step - loss: 0.0232 - acc: 0.6725 - val_loss: 0.0132 - val_acc: 0.8388\n",
      "Epoch 6/10\n",
      "7997/7997 [==============================] - 2s 297us/step - loss: 0.0206 - acc: 0.7186 - val_loss: 0.0110 - val_acc: 0.8689\n",
      "Epoch 7/10\n",
      "7997/7997 [==============================] - 2s 292us/step - loss: 0.0187 - acc: 0.7475 - val_loss: 0.0096 - val_acc: 0.8959\n",
      "Epoch 8/10\n",
      "7997/7997 [==============================] - 2s 294us/step - loss: 0.0173 - acc: 0.7742 - val_loss: 0.0087 - val_acc: 0.9029\n",
      "Epoch 9/10\n",
      "7997/7997 [==============================] - 2s 312us/step - loss: 0.0161 - acc: 0.7960 - val_loss: 0.0079 - val_acc: 0.9049\n",
      "Epoch 10/10\n",
      "7997/7997 [==============================] - 2s 307us/step - loss: 0.0151 - acc: 0.8062 - val_loss: 0.0073 - val_acc: 0.9119\n",
      "Dropout: 0.32\n",
      "Train on 7997 samples, validate on 999 samples\n",
      "Epoch 1/10\n",
      "7997/7997 [==============================] - 5s 657us/step - loss: 0.0469 - acc: 0.1647 - val_loss: 0.0381 - val_acc: 0.3694\n",
      "Epoch 2/10\n",
      "7997/7997 [==============================] - 2s 292us/step - loss: 0.0379 - acc: 0.3579 - val_loss: 0.0277 - val_acc: 0.5706\n",
      "Epoch 3/10\n",
      "7997/7997 [==============================] - 2s 299us/step - loss: 0.0314 - acc: 0.5088 - val_loss: 0.0212 - val_acc: 0.7067\n",
      "Epoch 4/10\n",
      "7997/7997 [==============================] - 2s 293us/step - loss: 0.0269 - acc: 0.5961 - val_loss: 0.0164 - val_acc: 0.7928\n",
      "Epoch 5/10\n",
      "7997/7997 [==============================] - 2s 290us/step - loss: 0.0236 - acc: 0.6630 - val_loss: 0.0134 - val_acc: 0.8298\n",
      "Epoch 6/10\n",
      "7997/7997 [==============================] - 2s 285us/step - loss: 0.0210 - acc: 0.7081 - val_loss: 0.0115 - val_acc: 0.8639\n",
      "Epoch 7/10\n",
      "7997/7997 [==============================] - 2s 298us/step - loss: 0.0193 - acc: 0.7372 - val_loss: 0.0102 - val_acc: 0.8849\n",
      "Epoch 8/10\n",
      "7997/7997 [==============================] - 2s 305us/step - loss: 0.0178 - acc: 0.7639 - val_loss: 0.0094 - val_acc: 0.8879\n",
      "Epoch 9/10\n",
      "7997/7997 [==============================] - 2s 293us/step - loss: 0.0165 - acc: 0.7849 - val_loss: 0.0084 - val_acc: 0.8929\n",
      "Epoch 10/10\n",
      "7997/7997 [==============================] - 2s 288us/step - loss: 0.0156 - acc: 0.8017 - val_loss: 0.0078 - val_acc: 0.9039\n",
      "Dropout: 0.33\n",
      "Train on 7997 samples, validate on 999 samples\n",
      "Epoch 1/10\n",
      "7997/7997 [==============================] - 5s 657us/step - loss: 0.0469 - acc: 0.1636 - val_loss: 0.0384 - val_acc: 0.3664\n",
      "Epoch 2/10\n",
      "7997/7997 [==============================] - 2s 282us/step - loss: 0.0381 - acc: 0.3541 - val_loss: 0.0278 - val_acc: 0.5786\n",
      "Epoch 3/10\n",
      "7997/7997 [==============================] - 2s 283us/step - loss: 0.0317 - acc: 0.5008 - val_loss: 0.0213 - val_acc: 0.7017\n",
      "Epoch 4/10\n",
      "7997/7997 [==============================] - 2s 281us/step - loss: 0.0273 - acc: 0.5875 - val_loss: 0.0167 - val_acc: 0.7748\n",
      "Epoch 5/10\n",
      "7997/7997 [==============================] - 2s 281us/step - loss: 0.0239 - acc: 0.6565 - val_loss: 0.0135 - val_acc: 0.8308\n",
      "Epoch 6/10\n",
      "7997/7997 [==============================] - 2s 280us/step - loss: 0.0212 - acc: 0.7079 - val_loss: 0.0118 - val_acc: 0.8589\n",
      "Epoch 7/10\n",
      "7997/7997 [==============================] - 2s 283us/step - loss: 0.0195 - acc: 0.7344 - val_loss: 0.0104 - val_acc: 0.8719\n",
      "Epoch 8/10\n",
      "7997/7997 [==============================] - 2s 284us/step - loss: 0.0180 - acc: 0.7569 - val_loss: 0.0093 - val_acc: 0.8859\n",
      "Epoch 9/10\n",
      "7997/7997 [==============================] - 2s 282us/step - loss: 0.0167 - acc: 0.7805 - val_loss: 0.0083 - val_acc: 0.9009\n",
      "Epoch 10/10\n",
      "7997/7997 [==============================] - 2s 283us/step - loss: 0.0158 - acc: 0.7948 - val_loss: 0.0078 - val_acc: 0.8999\n",
      "Dropout: 0.34\n",
      "Train on 7997 samples, validate on 999 samples\n",
      "Epoch 1/10\n",
      "7997/7997 [==============================] - 5s 662us/step - loss: 0.0471 - acc: 0.1594 - val_loss: 0.0386 - val_acc: 0.3674\n",
      "Epoch 2/10\n",
      "7997/7997 [==============================] - 2s 288us/step - loss: 0.0384 - acc: 0.3485 - val_loss: 0.0285 - val_acc: 0.5666\n",
      "Epoch 3/10\n",
      "7997/7997 [==============================] - 2s 311us/step - loss: 0.0321 - acc: 0.4902 - val_loss: 0.0220 - val_acc: 0.6777\n",
      "Epoch 4/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7997/7997 [==============================] - 2s 305us/step - loss: 0.0276 - acc: 0.5812 - val_loss: 0.0172 - val_acc: 0.7768\n",
      "Epoch 5/10\n",
      "7997/7997 [==============================] - 2s 291us/step - loss: 0.0243 - acc: 0.6501 - val_loss: 0.0140 - val_acc: 0.8258\n",
      "Epoch 6/10\n",
      "7997/7997 [==============================] - 2s 297us/step - loss: 0.0217 - acc: 0.6998 - val_loss: 0.0119 - val_acc: 0.8529\n",
      "Epoch 7/10\n",
      "7997/7997 [==============================] - 2s 297us/step - loss: 0.0198 - acc: 0.7311 - val_loss: 0.0104 - val_acc: 0.8719\n",
      "Epoch 8/10\n",
      "7997/7997 [==============================] - 2s 295us/step - loss: 0.0184 - acc: 0.7549 - val_loss: 0.0095 - val_acc: 0.8839\n",
      "Epoch 9/10\n",
      "7997/7997 [==============================] - 2s 285us/step - loss: 0.0170 - acc: 0.7814 - val_loss: 0.0087 - val_acc: 0.8919\n",
      "Epoch 10/10\n",
      "7997/7997 [==============================] - 2s 294us/step - loss: 0.0161 - acc: 0.7918 - val_loss: 0.0080 - val_acc: 0.8999\n",
      "Dropout: 0.35\n",
      "Train on 7997 samples, validate on 999 samples\n",
      "Epoch 1/10\n",
      "7997/7997 [==============================] - 5s 682us/step - loss: 0.0471 - acc: 0.1573 - val_loss: 0.0387 - val_acc: 0.3524\n",
      "Epoch 2/10\n",
      "7997/7997 [==============================] - 2s 300us/step - loss: 0.0386 - acc: 0.3441 - val_loss: 0.0283 - val_acc: 0.5636\n",
      "Epoch 3/10\n",
      "7997/7997 [==============================] - 2s 293us/step - loss: 0.0323 - acc: 0.4887 - val_loss: 0.0217 - val_acc: 0.6837\n",
      "Epoch 4/10\n",
      "7997/7997 [==============================] - 2s 293us/step - loss: 0.0279 - acc: 0.5793 - val_loss: 0.0173 - val_acc: 0.7698\n",
      "Epoch 5/10\n",
      "7997/7997 [==============================] - 2s 296us/step - loss: 0.0246 - acc: 0.6401 - val_loss: 0.0140 - val_acc: 0.8248\n",
      "Epoch 6/10\n",
      "7997/7997 [==============================] - 2s 305us/step - loss: 0.0219 - acc: 0.6963 - val_loss: 0.0117 - val_acc: 0.8589\n",
      "Epoch 7/10\n",
      "7997/7997 [==============================] - 2s 294us/step - loss: 0.0200 - acc: 0.7244 - val_loss: 0.0103 - val_acc: 0.8679\n",
      "Epoch 8/10\n",
      "7997/7997 [==============================] - 2s 294us/step - loss: 0.0187 - acc: 0.7519 - val_loss: 0.0093 - val_acc: 0.8819\n",
      "Epoch 9/10\n",
      "7997/7997 [==============================] - 2s 305us/step - loss: 0.0173 - acc: 0.7742 - val_loss: 0.0086 - val_acc: 0.8969\n",
      "Epoch 10/10\n",
      "7997/7997 [==============================] - 2s 285us/step - loss: 0.0164 - acc: 0.7853 - val_loss: 0.0079 - val_acc: 0.9089\n",
      "Dropout: 0.36\n",
      "Train on 7997 samples, validate on 999 samples\n",
      "Epoch 1/10\n",
      "7997/7997 [==============================] - 5s 658us/step - loss: 0.0472 - acc: 0.1553 - val_loss: 0.0389 - val_acc: 0.3524\n",
      "Epoch 2/10\n",
      "7997/7997 [==============================] - 2s 289us/step - loss: 0.0388 - acc: 0.3404 - val_loss: 0.0277 - val_acc: 0.5826\n",
      "Epoch 3/10\n",
      "7997/7997 [==============================] - 2s 303us/step - loss: 0.0325 - acc: 0.4822 - val_loss: 0.0215 - val_acc: 0.7047\n",
      "Epoch 4/10\n",
      "7997/7997 [==============================] - 2s 287us/step - loss: 0.0282 - acc: 0.5690 - val_loss: 0.0174 - val_acc: 0.7648\n",
      "Epoch 5/10\n",
      "7997/7997 [==============================] - 2s 292us/step - loss: 0.0250 - acc: 0.6325 - val_loss: 0.0137 - val_acc: 0.8268\n",
      "Epoch 6/10\n",
      "7997/7997 [==============================] - 2s 283us/step - loss: 0.0221 - acc: 0.6919 - val_loss: 0.0117 - val_acc: 0.8599\n",
      "Epoch 7/10\n",
      "7997/7997 [==============================] - 2s 274us/step - loss: 0.0204 - acc: 0.7234 - val_loss: 0.0103 - val_acc: 0.8789\n",
      "Epoch 8/10\n",
      "7997/7997 [==============================] - 2s 289us/step - loss: 0.0190 - acc: 0.7507 - val_loss: 0.0094 - val_acc: 0.8889\n",
      "Epoch 9/10\n",
      "7997/7997 [==============================] - 2s 284us/step - loss: 0.0176 - acc: 0.7670 - val_loss: 0.0085 - val_acc: 0.8949\n",
      "Epoch 10/10\n",
      "7997/7997 [==============================] - 2s 285us/step - loss: 0.0167 - acc: 0.7809 - val_loss: 0.0078 - val_acc: 0.8979\n",
      "Dropout: 0.37\n",
      "Train on 7997 samples, validate on 999 samples\n",
      "Epoch 1/10\n",
      "7997/7997 [==============================] - 5s 668us/step - loss: 0.0473 - acc: 0.1512 - val_loss: 0.0392 - val_acc: 0.3333\n",
      "Epoch 2/10\n",
      "7997/7997 [==============================] - 2s 285us/step - loss: 0.0392 - acc: 0.3267 - val_loss: 0.0285 - val_acc: 0.5566\n",
      "Epoch 3/10\n",
      "7997/7997 [==============================] - 2s 301us/step - loss: 0.0331 - acc: 0.4719 - val_loss: 0.0227 - val_acc: 0.6637\n",
      "Epoch 4/10\n",
      "7997/7997 [==============================] - 2s 291us/step - loss: 0.0289 - acc: 0.5553 - val_loss: 0.0179 - val_acc: 0.7548\n",
      "Epoch 5/10\n",
      "7997/7997 [==============================] - 2s 287us/step - loss: 0.0256 - acc: 0.6200 - val_loss: 0.0144 - val_acc: 0.8208\n",
      "Epoch 6/10\n",
      "7997/7997 [==============================] - 2s 283us/step - loss: 0.0227 - acc: 0.6881 - val_loss: 0.0123 - val_acc: 0.8529\n",
      "Epoch 7/10\n",
      "7997/7997 [==============================] - 2s 297us/step - loss: 0.0210 - acc: 0.7148 - val_loss: 0.0107 - val_acc: 0.8669\n",
      "Epoch 8/10\n",
      "7997/7997 [==============================] - 2s 296us/step - loss: 0.0194 - acc: 0.7367 - val_loss: 0.0096 - val_acc: 0.8799\n",
      "Epoch 9/10\n",
      "7997/7997 [==============================] - 3s 319us/step - loss: 0.0181 - acc: 0.7607 - val_loss: 0.0086 - val_acc: 0.8949\n",
      "Epoch 10/10\n",
      "7997/7997 [==============================] - 2s 310us/step - loss: 0.0172 - acc: 0.7773 - val_loss: 0.0080 - val_acc: 0.8959\n",
      "Dropout: 0.38\n",
      "Train on 7997 samples, validate on 999 samples\n",
      "Epoch 1/10\n",
      "7997/7997 [==============================] - 6s 721us/step - loss: 0.0474 - acc: 0.1501 - val_loss: 0.0395 - val_acc: 0.3273\n",
      "Epoch 2/10\n",
      "7997/7997 [==============================] - 2s 303us/step - loss: 0.0394 - acc: 0.3221 - val_loss: 0.0288 - val_acc: 0.5646\n",
      "Epoch 3/10\n",
      "7997/7997 [==============================] - 2s 289us/step - loss: 0.0334 - acc: 0.4603 - val_loss: 0.0228 - val_acc: 0.6727\n",
      "Epoch 4/10\n",
      "7997/7997 [==============================] - 2s 284us/step - loss: 0.0292 - acc: 0.5547 - val_loss: 0.0181 - val_acc: 0.7457\n",
      "Epoch 5/10\n",
      "7997/7997 [==============================] - 2s 289us/step - loss: 0.0259 - acc: 0.6210 - val_loss: 0.0147 - val_acc: 0.8098\n",
      "Epoch 6/10\n",
      "7997/7997 [==============================] - 2s 286us/step - loss: 0.0231 - acc: 0.6721 - val_loss: 0.0124 - val_acc: 0.8509\n",
      "Epoch 7/10\n",
      "7997/7997 [==============================] - 2s 302us/step - loss: 0.0213 - acc: 0.7070 - val_loss: 0.0108 - val_acc: 0.8629\n",
      "Epoch 8/10\n",
      "7997/7997 [==============================] - 2s 309us/step - loss: 0.0198 - acc: 0.7311 - val_loss: 0.0098 - val_acc: 0.8809\n",
      "Epoch 9/10\n",
      "7997/7997 [==============================] - 2s 287us/step - loss: 0.0183 - acc: 0.7578 - val_loss: 0.0087 - val_acc: 0.8979\n",
      "Epoch 10/10\n",
      "7997/7997 [==============================] - 2s 282us/step - loss: 0.0174 - acc: 0.7694 - val_loss: 0.0080 - val_acc: 0.9069\n",
      "Dropout: 0.39\n",
      "Train on 7997 samples, validate on 999 samples\n",
      "Epoch 1/10\n",
      "7997/7997 [==============================] - 5s 659us/step - loss: 0.0475 - acc: 0.1427 - val_loss: 0.0398 - val_acc: 0.3293\n",
      "Epoch 2/10\n",
      "7997/7997 [==============================] - 2s 287us/step - loss: 0.0398 - acc: 0.3105 - val_loss: 0.0293 - val_acc: 0.5435\n",
      "Epoch 3/10\n",
      "7997/7997 [==============================] - 2s 285us/step - loss: 0.0337 - acc: 0.4577 - val_loss: 0.0230 - val_acc: 0.6597\n",
      "Epoch 4/10\n",
      "7997/7997 [==============================] - 2s 288us/step - loss: 0.0297 - acc: 0.5410 - val_loss: 0.0185 - val_acc: 0.7377\n",
      "Epoch 5/10\n",
      "7997/7997 [==============================] - 2s 285us/step - loss: 0.0263 - acc: 0.6077 - val_loss: 0.0147 - val_acc: 0.8138\n",
      "Epoch 6/10\n",
      "7997/7997 [==============================] - 2s 288us/step - loss: 0.0236 - acc: 0.6665 - val_loss: 0.0126 - val_acc: 0.8378\n",
      "Epoch 7/10\n",
      "7997/7997 [==============================] - 2s 286us/step - loss: 0.0218 - acc: 0.7008 - val_loss: 0.0112 - val_acc: 0.8529\n",
      "Epoch 8/10\n",
      "7997/7997 [==============================] - 2s 284us/step - loss: 0.0203 - acc: 0.7205 - val_loss: 0.0103 - val_acc: 0.8619\n",
      "Epoch 9/10\n",
      "7997/7997 [==============================] - 2s 288us/step - loss: 0.0189 - acc: 0.7475 - val_loss: 0.0094 - val_acc: 0.8719\n",
      "Epoch 10/10\n",
      "7997/7997 [==============================] - 2s 286us/step - loss: 0.0178 - acc: 0.7625 - val_loss: 0.0086 - val_acc: 0.8869\n",
      "Dropout: 0.4\n",
      "Train on 7997 samples, validate on 999 samples\n",
      "Epoch 1/10\n",
      "7997/7997 [==============================] - 5s 686us/step - loss: 0.0475 - acc: 0.1421 - val_loss: 0.0401 - val_acc: 0.3163\n",
      "Epoch 2/10\n",
      "7997/7997 [==============================] - 2s 298us/step - loss: 0.0401 - acc: 0.3072 - val_loss: 0.0295 - val_acc: 0.5536\n",
      "Epoch 3/10\n",
      "7997/7997 [==============================] - 2s 286us/step - loss: 0.0343 - acc: 0.4473 - val_loss: 0.0236 - val_acc: 0.6476\n",
      "Epoch 4/10\n",
      "7997/7997 [==============================] - 2s 286us/step - loss: 0.0301 - acc: 0.5373 - val_loss: 0.0185 - val_acc: 0.7417\n",
      "Epoch 5/10\n",
      "7997/7997 [==============================] - 2s 286us/step - loss: 0.0268 - acc: 0.6006 - val_loss: 0.0151 - val_acc: 0.7978\n",
      "Epoch 6/10\n",
      "7997/7997 [==============================] - 2s 286us/step - loss: 0.0240 - acc: 0.6560 - val_loss: 0.0129 - val_acc: 0.8288\n",
      "Epoch 7/10\n",
      "7997/7997 [==============================] - 2s 286us/step - loss: 0.0221 - acc: 0.6904 - val_loss: 0.0111 - val_acc: 0.8579\n",
      "Epoch 8/10\n",
      "7997/7997 [==============================] - 2s 286us/step - loss: 0.0205 - acc: 0.7188 - val_loss: 0.0099 - val_acc: 0.8769\n",
      "Epoch 9/10\n",
      "7997/7997 [==============================] - 2s 289us/step - loss: 0.0190 - acc: 0.7480 - val_loss: 0.0090 - val_acc: 0.8829\n",
      "Epoch 10/10\n",
      "7997/7997 [==============================] - 2s 287us/step - loss: 0.0180 - acc: 0.7620 - val_loss: 0.0082 - val_acc: 0.8969\n",
      "Dropout: 0.41\n",
      "Train on 7997 samples, validate on 999 samples\n",
      "Epoch 1/10\n",
      "7997/7997 [==============================] - 5s 658us/step - loss: 0.0476 - acc: 0.1402 - val_loss: 0.0405 - val_acc: 0.3053\n",
      "Epoch 2/10\n",
      "7997/7997 [==============================] - 2s 286us/step - loss: 0.0404 - acc: 0.2977 - val_loss: 0.0298 - val_acc: 0.5425\n",
      "Epoch 3/10\n",
      "7997/7997 [==============================] - 2s 286us/step - loss: 0.0345 - acc: 0.4360 - val_loss: 0.0239 - val_acc: 0.6386\n",
      "Epoch 4/10\n",
      "7997/7997 [==============================] - 2s 293us/step - loss: 0.0305 - acc: 0.5243 - val_loss: 0.0193 - val_acc: 0.7297\n",
      "Epoch 5/10\n",
      "7997/7997 [==============================] - 2s 290us/step - loss: 0.0272 - acc: 0.5940 - val_loss: 0.0158 - val_acc: 0.7898\n",
      "Epoch 6/10\n",
      "7997/7997 [==============================] - 2s 288us/step - loss: 0.0244 - acc: 0.6494 - val_loss: 0.0132 - val_acc: 0.8328\n",
      "Epoch 7/10\n",
      "7997/7997 [==============================] - 2s 285us/step - loss: 0.0226 - acc: 0.6849 - val_loss: 0.0114 - val_acc: 0.8539\n",
      "Epoch 8/10\n",
      "7997/7997 [==============================] - 2s 285us/step - loss: 0.0209 - acc: 0.7138 - val_loss: 0.0102 - val_acc: 0.8729\n",
      "Epoch 9/10\n",
      "7997/7997 [==============================] - 2s 288us/step - loss: 0.0194 - acc: 0.7379 - val_loss: 0.0090 - val_acc: 0.8829\n",
      "Epoch 10/10\n",
      "7997/7997 [==============================] - 2s 284us/step - loss: 0.0185 - acc: 0.7514 - val_loss: 0.0083 - val_acc: 0.8909\n",
      "Dropout: 0.42\n",
      "Train on 7997 samples, validate on 999 samples\n",
      "Epoch 1/10\n",
      "7997/7997 [==============================] - 6s 699us/step - loss: 0.0476 - acc: 0.1408 - val_loss: 0.0404 - val_acc: 0.3143\n",
      "Epoch 2/10\n",
      "7997/7997 [==============================] - 2s 289us/step - loss: 0.0406 - acc: 0.2935 - val_loss: 0.0301 - val_acc: 0.5305\n",
      "Epoch 3/10\n",
      "7997/7997 [==============================] - 2s 284us/step - loss: 0.0349 - acc: 0.4320 - val_loss: 0.0239 - val_acc: 0.6436\n",
      "Epoch 4/10\n",
      "7997/7997 [==============================] - 2s 285us/step - loss: 0.0308 - acc: 0.5169 - val_loss: 0.0196 - val_acc: 0.7287\n",
      "Epoch 5/10\n",
      "7997/7997 [==============================] - 2s 281us/step - loss: 0.0275 - acc: 0.5877 - val_loss: 0.0156 - val_acc: 0.7948\n",
      "Epoch 6/10\n",
      "7997/7997 [==============================] - 2s 282us/step - loss: 0.0248 - acc: 0.6434 - val_loss: 0.0132 - val_acc: 0.8348\n",
      "Epoch 7/10\n",
      "7997/7997 [==============================] - 2s 286us/step - loss: 0.0228 - acc: 0.6840 - val_loss: 0.0116 - val_acc: 0.8488\n",
      "Epoch 8/10\n",
      "7997/7997 [==============================] - 2s 285us/step - loss: 0.0213 - acc: 0.7046 - val_loss: 0.0105 - val_acc: 0.8689\n",
      "Epoch 9/10\n",
      "7997/7997 [==============================] - 2s 282us/step - loss: 0.0198 - acc: 0.7349 - val_loss: 0.0094 - val_acc: 0.8799\n",
      "Epoch 10/10\n",
      "7997/7997 [==============================] - 2s 280us/step - loss: 0.0188 - acc: 0.7485 - val_loss: 0.0085 - val_acc: 0.8929\n",
      "Dropout: 0.43\n",
      "Train on 7997 samples, validate on 999 samples\n",
      "Epoch 1/10\n",
      "7997/7997 [==============================] - 5s 679us/step - loss: 0.0476 - acc: 0.1387 - val_loss: 0.0406 - val_acc: 0.2933\n",
      "Epoch 2/10\n",
      "7997/7997 [==============================] - 2s 283us/step - loss: 0.0408 - acc: 0.2900 - val_loss: 0.0303 - val_acc: 0.5305\n",
      "Epoch 3/10\n",
      "7997/7997 [==============================] - 2s 283us/step - loss: 0.0352 - acc: 0.4220 - val_loss: 0.0240 - val_acc: 0.6486\n",
      "Epoch 4/10\n",
      "7997/7997 [==============================] - 2s 285us/step - loss: 0.0311 - acc: 0.5157 - val_loss: 0.0196 - val_acc: 0.7277\n",
      "Epoch 5/10\n",
      "7997/7997 [==============================] - 2s 286us/step - loss: 0.0279 - acc: 0.5792 - val_loss: 0.0162 - val_acc: 0.7708\n",
      "Epoch 6/10\n",
      "7997/7997 [==============================] - 2s 283us/step - loss: 0.0250 - acc: 0.6416 - val_loss: 0.0139 - val_acc: 0.8218\n",
      "Epoch 7/10\n",
      "7997/7997 [==============================] - 2s 285us/step - loss: 0.0232 - acc: 0.6721 - val_loss: 0.0117 - val_acc: 0.8609\n",
      "Epoch 8/10\n",
      "7997/7997 [==============================] - 2s 286us/step - loss: 0.0217 - acc: 0.6990 - val_loss: 0.0106 - val_acc: 0.8629\n",
      "Epoch 9/10\n",
      "7997/7997 [==============================] - 2s 286us/step - loss: 0.0201 - acc: 0.7221 - val_loss: 0.0097 - val_acc: 0.8759\n",
      "Epoch 10/10\n",
      "7997/7997 [==============================] - 2s 284us/step - loss: 0.0191 - acc: 0.7442 - val_loss: 0.0089 - val_acc: 0.8909\n",
      "Dropout: 0.44\n",
      "Train on 7997 samples, validate on 999 samples\n",
      "Epoch 1/10\n",
      "7997/7997 [==============================] - 5s 668us/step - loss: 0.0478 - acc: 0.1338 - val_loss: 0.0411 - val_acc: 0.2803\n",
      "Epoch 2/10\n",
      "7997/7997 [==============================] - 2s 280us/step - loss: 0.0412 - acc: 0.2787 - val_loss: 0.0313 - val_acc: 0.5105\n",
      "Epoch 3/10\n",
      "7997/7997 [==============================] - 2s 284us/step - loss: 0.0359 - acc: 0.4050 - val_loss: 0.0247 - val_acc: 0.6336\n",
      "Epoch 4/10\n",
      "7997/7997 [==============================] - 2s 308us/step - loss: 0.0317 - acc: 0.4981 - val_loss: 0.0200 - val_acc: 0.7117\n",
      "Epoch 5/10\n",
      "7997/7997 [==============================] - 2s 299us/step - loss: 0.0285 - acc: 0.5620 - val_loss: 0.0163 - val_acc: 0.7758\n",
      "Epoch 6/10\n",
      "7997/7997 [==============================] - 2s 299us/step - loss: 0.0258 - acc: 0.6255 - val_loss: 0.0137 - val_acc: 0.8298\n",
      "Epoch 7/10\n",
      "7997/7997 [==============================] - 2s 301us/step - loss: 0.0237 - acc: 0.6661 - val_loss: 0.0118 - val_acc: 0.8519\n",
      "Epoch 8/10\n",
      "7997/7997 [==============================] - 2s 295us/step - loss: 0.0221 - acc: 0.6930 - val_loss: 0.0107 - val_acc: 0.8589\n",
      "Epoch 9/10\n",
      "7997/7997 [==============================] - 2s 290us/step - loss: 0.0205 - acc: 0.7198 - val_loss: 0.0096 - val_acc: 0.8729\n",
      "Epoch 10/10\n",
      "7997/7997 [==============================] - 2s 286us/step - loss: 0.0195 - acc: 0.7375 - val_loss: 0.0086 - val_acc: 0.8869\n",
      "Dropout: 0.45\n",
      "Train on 7997 samples, validate on 999 samples\n",
      "Epoch 1/10\n",
      "7997/7997 [==============================] - 6s 690us/step - loss: 0.0476 - acc: 0.1322 - val_loss: 0.0407 - val_acc: 0.2913\n",
      "Epoch 2/10\n",
      "7997/7997 [==============================] - 2s 299us/step - loss: 0.0412 - acc: 0.2762 - val_loss: 0.0312 - val_acc: 0.5145\n",
      "Epoch 3/10\n",
      "7997/7997 [==============================] - 2s 297us/step - loss: 0.0359 - acc: 0.4052 - val_loss: 0.0249 - val_acc: 0.6316\n",
      "Epoch 4/10\n",
      "7997/7997 [==============================] - 2s 298us/step - loss: 0.0321 - acc: 0.4949 - val_loss: 0.0204 - val_acc: 0.7157\n",
      "Epoch 5/10\n",
      "7997/7997 [==============================] - 2s 302us/step - loss: 0.0289 - acc: 0.5643 - val_loss: 0.0166 - val_acc: 0.7778\n",
      "Epoch 6/10\n",
      "7997/7997 [==============================] - 2s 288us/step - loss: 0.0262 - acc: 0.6184 - val_loss: 0.0143 - val_acc: 0.8128\n",
      "Epoch 7/10\n",
      "7997/7997 [==============================] - 2s 292us/step - loss: 0.0242 - acc: 0.6560 - val_loss: 0.0122 - val_acc: 0.8448\n",
      "Epoch 8/10\n",
      "7997/7997 [==============================] - 2s 292us/step - loss: 0.0225 - acc: 0.6884 - val_loss: 0.0109 - val_acc: 0.8599\n",
      "Epoch 9/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7997/7997 [==============================] - 2s 290us/step - loss: 0.0210 - acc: 0.7128 - val_loss: 0.0098 - val_acc: 0.8719\n",
      "Epoch 10/10\n",
      "7997/7997 [==============================] - 2s 294us/step - loss: 0.0199 - acc: 0.7286 - val_loss: 0.0090 - val_acc: 0.8799\n",
      "Dropout: 0.46\n",
      "Train on 7997 samples, validate on 999 samples\n",
      "Epoch 1/10\n",
      "7997/7997 [==============================] - 6s 698us/step - loss: 0.0478 - acc: 0.1300 - val_loss: 0.0414 - val_acc: 0.2773\n",
      "Epoch 2/10\n",
      "7997/7997 [==============================] - 2s 285us/step - loss: 0.0415 - acc: 0.2676 - val_loss: 0.0314 - val_acc: 0.5095\n",
      "Epoch 3/10\n",
      "7997/7997 [==============================] - 2s 289us/step - loss: 0.0363 - acc: 0.4004 - val_loss: 0.0255 - val_acc: 0.6176\n",
      "Epoch 4/10\n",
      "7997/7997 [==============================] - 2s 288us/step - loss: 0.0325 - acc: 0.4827 - val_loss: 0.0209 - val_acc: 0.6997\n",
      "Epoch 5/10\n",
      "7997/7997 [==============================] - 2s 288us/step - loss: 0.0293 - acc: 0.5538 - val_loss: 0.0172 - val_acc: 0.7578\n",
      "Epoch 6/10\n",
      "7997/7997 [==============================] - 2s 287us/step - loss: 0.0266 - acc: 0.6084 - val_loss: 0.0146 - val_acc: 0.8078\n",
      "Epoch 7/10\n",
      "7997/7997 [==============================] - 2s 287us/step - loss: 0.0247 - acc: 0.6465 - val_loss: 0.0128 - val_acc: 0.8248\n",
      "Epoch 8/10\n",
      "7997/7997 [==============================] - 2s 289us/step - loss: 0.0232 - acc: 0.6718 - val_loss: 0.0111 - val_acc: 0.8569\n",
      "Epoch 9/10\n",
      "7997/7997 [==============================] - 2s 288us/step - loss: 0.0216 - acc: 0.7018 - val_loss: 0.0102 - val_acc: 0.8669\n",
      "Epoch 10/10\n",
      "7997/7997 [==============================] - 2s 295us/step - loss: 0.0204 - acc: 0.7166 - val_loss: 0.0091 - val_acc: 0.8769\n",
      "Dropout: 0.47\n",
      "Train on 7997 samples, validate on 999 samples\n",
      "Epoch 1/10\n",
      "7997/7997 [==============================] - 6s 713us/step - loss: 0.0478 - acc: 0.1262 - val_loss: 0.0414 - val_acc: 0.2663\n",
      "Epoch 2/10\n",
      "7997/7997 [==============================] - 2s 292us/step - loss: 0.0417 - acc: 0.2671 - val_loss: 0.0318 - val_acc: 0.5005\n",
      "Epoch 3/10\n",
      "7997/7997 [==============================] - 2s 287us/step - loss: 0.0366 - acc: 0.3903 - val_loss: 0.0257 - val_acc: 0.6216\n",
      "Epoch 4/10\n",
      "7997/7997 [==============================] - 2s 303us/step - loss: 0.0329 - acc: 0.4767 - val_loss: 0.0216 - val_acc: 0.6797\n",
      "Epoch 5/10\n",
      "7997/7997 [==============================] - 2s 293us/step - loss: 0.0298 - acc: 0.5412 - val_loss: 0.0177 - val_acc: 0.7528\n",
      "Epoch 6/10\n",
      "7997/7997 [==============================] - 2s 293us/step - loss: 0.0273 - acc: 0.5983 - val_loss: 0.0154 - val_acc: 0.7948\n",
      "Epoch 7/10\n",
      "7997/7997 [==============================] - 2s 293us/step - loss: 0.0252 - acc: 0.6350 - val_loss: 0.0130 - val_acc: 0.8298\n",
      "Epoch 8/10\n",
      "7997/7997 [==============================] - 2s 291us/step - loss: 0.0236 - acc: 0.6657 - val_loss: 0.0114 - val_acc: 0.8569\n",
      "Epoch 9/10\n",
      "7997/7997 [==============================] - 2s 288us/step - loss: 0.0220 - acc: 0.6944 - val_loss: 0.0102 - val_acc: 0.8689\n",
      "Epoch 10/10\n",
      "7997/7997 [==============================] - 2s 306us/step - loss: 0.0209 - acc: 0.7091 - val_loss: 0.0093 - val_acc: 0.8749\n",
      "Dropout: 0.48\n",
      "Train on 7997 samples, validate on 999 samples\n",
      "Epoch 1/10\n",
      "7997/7997 [==============================] - 6s 732us/step - loss: 0.0478 - acc: 0.1317 - val_loss: 0.0419 - val_acc: 0.2553\n",
      "Epoch 2/10\n",
      "7997/7997 [==============================] - 2s 286us/step - loss: 0.0421 - acc: 0.2566 - val_loss: 0.0318 - val_acc: 0.5025\n",
      "Epoch 3/10\n",
      "7997/7997 [==============================] - 2s 285us/step - loss: 0.0371 - acc: 0.3739 - val_loss: 0.0259 - val_acc: 0.6046\n",
      "Epoch 4/10\n",
      "7997/7997 [==============================] - 2s 286us/step - loss: 0.0335 - acc: 0.4610 - val_loss: 0.0216 - val_acc: 0.6937\n",
      "Epoch 5/10\n",
      "7997/7997 [==============================] - 2s 286us/step - loss: 0.0303 - acc: 0.5284 - val_loss: 0.0181 - val_acc: 0.7407\n",
      "Epoch 6/10\n",
      "7997/7997 [==============================] - 2s 286us/step - loss: 0.0277 - acc: 0.5927 - val_loss: 0.0158 - val_acc: 0.7818\n",
      "Epoch 7/10\n",
      "7997/7997 [==============================] - 2s 286us/step - loss: 0.0258 - acc: 0.6257 - val_loss: 0.0133 - val_acc: 0.8268\n",
      "Epoch 8/10\n",
      "7997/7997 [==============================] - 2s 286us/step - loss: 0.0242 - acc: 0.6585 - val_loss: 0.0120 - val_acc: 0.8368\n",
      "Epoch 9/10\n",
      "7997/7997 [==============================] - 2s 286us/step - loss: 0.0225 - acc: 0.6875 - val_loss: 0.0106 - val_acc: 0.8669\n",
      "Epoch 10/10\n",
      "7997/7997 [==============================] - 2s 285us/step - loss: 0.0214 - acc: 0.7018 - val_loss: 0.0097 - val_acc: 0.8679\n",
      "Dropout: 0.49\n",
      "Train on 7997 samples, validate on 999 samples\n",
      "Epoch 1/10\n",
      "7997/7997 [==============================] - 6s 698us/step - loss: 0.0478 - acc: 0.1253 - val_loss: 0.0425 - val_acc: 0.2402\n",
      "Epoch 2/10\n",
      "7997/7997 [==============================] - 2s 288us/step - loss: 0.0423 - acc: 0.2478 - val_loss: 0.0322 - val_acc: 0.5015\n",
      "Epoch 3/10\n",
      "7997/7997 [==============================] - 2s 285us/step - loss: 0.0375 - acc: 0.3703 - val_loss: 0.0262 - val_acc: 0.6006\n",
      "Epoch 4/10\n",
      "7997/7997 [==============================] - 2s 289us/step - loss: 0.0338 - acc: 0.4540 - val_loss: 0.0217 - val_acc: 0.6837\n",
      "Epoch 5/10\n",
      "7997/7997 [==============================] - 2s 289us/step - loss: 0.0307 - acc: 0.5198 - val_loss: 0.0178 - val_acc: 0.7508\n",
      "Epoch 6/10\n",
      "7997/7997 [==============================] - 2s 288us/step - loss: 0.0282 - acc: 0.5803 - val_loss: 0.0156 - val_acc: 0.7898\n",
      "Epoch 7/10\n",
      "7997/7997 [==============================] - 2s 288us/step - loss: 0.0262 - acc: 0.6230 - val_loss: 0.0134 - val_acc: 0.8148\n",
      "Epoch 8/10\n",
      "7997/7997 [==============================] - 2s 288us/step - loss: 0.0245 - acc: 0.6530 - val_loss: 0.0117 - val_acc: 0.8529\n",
      "Epoch 9/10\n",
      "7997/7997 [==============================] - 2s 287us/step - loss: 0.0229 - acc: 0.6835 - val_loss: 0.0104 - val_acc: 0.8709\n",
      "Epoch 10/10\n",
      "7997/7997 [==============================] - 2s 288us/step - loss: 0.0217 - acc: 0.6980 - val_loss: 0.0096 - val_acc: 0.8769\n",
      "Dropout: 0.5\n",
      "Train on 7997 samples, validate on 999 samples\n",
      "Epoch 1/10\n",
      "7997/7997 [==============================] - 6s 703us/step - loss: 0.0479 - acc: 0.1219 - val_loss: 0.0426 - val_acc: 0.2302\n",
      "Epoch 2/10\n",
      "7997/7997 [==============================] - 2s 289us/step - loss: 0.0427 - acc: 0.2387 - val_loss: 0.0326 - val_acc: 0.4935\n",
      "Epoch 3/10\n",
      "7997/7997 [==============================] - 2s 290us/step - loss: 0.0380 - acc: 0.3553 - val_loss: 0.0263 - val_acc: 0.6116\n",
      "Epoch 4/10\n",
      "7997/7997 [==============================] - 2s 295us/step - loss: 0.0343 - acc: 0.4447 - val_loss: 0.0220 - val_acc: 0.6597\n",
      "Epoch 5/10\n",
      "7997/7997 [==============================] - 2s 294us/step - loss: 0.0312 - acc: 0.5097 - val_loss: 0.0184 - val_acc: 0.7307\n",
      "Epoch 6/10\n",
      "7997/7997 [==============================] - 2s 287us/step - loss: 0.0287 - acc: 0.5678 - val_loss: 0.0160 - val_acc: 0.7708\n",
      "Epoch 7/10\n",
      "7997/7997 [==============================] - 2s 289us/step - loss: 0.0266 - acc: 0.6107 - val_loss: 0.0139 - val_acc: 0.8028\n",
      "Epoch 8/10\n",
      "7997/7997 [==============================] - 2s 287us/step - loss: 0.0250 - acc: 0.6406 - val_loss: 0.0121 - val_acc: 0.8428\n",
      "Epoch 9/10\n",
      "7997/7997 [==============================] - 2s 302us/step - loss: 0.0234 - acc: 0.6714 - val_loss: 0.0109 - val_acc: 0.8569\n",
      "Epoch 10/10\n",
      "7997/7997 [==============================] - 3s 324us/step - loss: 0.0222 - acc: 0.6928 - val_loss: 0.0099 - val_acc: 0.8679\n",
      "Dropout: 0.51\n",
      "Train on 7997 samples, validate on 999 samples\n",
      "Epoch 1/10\n",
      "7997/7997 [==============================] - 6s 753us/step - loss: 0.0478 - acc: 0.1224 - val_loss: 0.0428 - val_acc: 0.2322\n",
      "Epoch 2/10\n",
      "7997/7997 [==============================] - 3s 352us/step - loss: 0.0428 - acc: 0.2390 - val_loss: 0.0328 - val_acc: 0.4865\n",
      "Epoch 3/10\n",
      "7997/7997 [==============================] - 3s 324us/step - loss: 0.0382 - acc: 0.3474 - val_loss: 0.0264 - val_acc: 0.6066\n",
      "Epoch 4/10\n",
      "7997/7997 [==============================] - 3s 357us/step - loss: 0.0345 - acc: 0.4445 - val_loss: 0.0222 - val_acc: 0.6737\n",
      "Epoch 5/10\n",
      "7997/7997 [==============================] - 3s 349us/step - loss: 0.0314 - acc: 0.5101 - val_loss: 0.0185 - val_acc: 0.7387\n",
      "Epoch 6/10\n",
      "7997/7997 [==============================] - 3s 317us/step - loss: 0.0290 - acc: 0.5655 - val_loss: 0.0157 - val_acc: 0.7858\n",
      "Epoch 7/10\n",
      "7997/7997 [==============================] - 2s 310us/step - loss: 0.0270 - acc: 0.6072 - val_loss: 0.0137 - val_acc: 0.8228\n",
      "Epoch 8/10\n",
      "7997/7997 [==============================] - 2s 309us/step - loss: 0.0252 - acc: 0.6394 - val_loss: 0.0117 - val_acc: 0.8519\n",
      "Epoch 9/10\n",
      "7997/7997 [==============================] - 3s 319us/step - loss: 0.0237 - acc: 0.6684 - val_loss: 0.0104 - val_acc: 0.8729\n",
      "Epoch 10/10\n",
      "7997/7997 [==============================] - 2s 297us/step - loss: 0.0224 - acc: 0.6905 - val_loss: 0.0097 - val_acc: 0.8799\n",
      "Dropout: 0.52\n",
      "Train on 7997 samples, validate on 999 samples\n",
      "Epoch 1/10\n",
      "7997/7997 [==============================] - 6s 756us/step - loss: 0.0479 - acc: 0.1199 - val_loss: 0.0431 - val_acc: 0.2352\n",
      "Epoch 2/10\n",
      "7997/7997 [==============================] - 2s 298us/step - loss: 0.0430 - acc: 0.2275 - val_loss: 0.0329 - val_acc: 0.4955\n",
      "Epoch 3/10\n",
      "7997/7997 [==============================] - 2s 297us/step - loss: 0.0386 - acc: 0.3404 - val_loss: 0.0270 - val_acc: 0.5866\n",
      "Epoch 4/10\n",
      "7997/7997 [==============================] - 2s 298us/step - loss: 0.0350 - acc: 0.4327 - val_loss: 0.0227 - val_acc: 0.6597\n",
      "Epoch 5/10\n",
      "7997/7997 [==============================] - 2s 298us/step - loss: 0.0319 - acc: 0.4969 - val_loss: 0.0189 - val_acc: 0.7277\n",
      "Epoch 6/10\n",
      "7997/7997 [==============================] - 2s 299us/step - loss: 0.0293 - acc: 0.5570 - val_loss: 0.0164 - val_acc: 0.7618\n",
      "Epoch 7/10\n",
      "7997/7997 [==============================] - 2s 300us/step - loss: 0.0274 - acc: 0.5962 - val_loss: 0.0141 - val_acc: 0.8008\n",
      "Epoch 8/10\n",
      "7997/7997 [==============================] - 2s 297us/step - loss: 0.0257 - acc: 0.6282 - val_loss: 0.0122 - val_acc: 0.8348\n",
      "Epoch 9/10\n",
      "7997/7997 [==============================] - 2s 299us/step - loss: 0.0242 - acc: 0.6545 - val_loss: 0.0113 - val_acc: 0.8478\n",
      "Epoch 10/10\n",
      "7997/7997 [==============================] - 2s 300us/step - loss: 0.0231 - acc: 0.6775 - val_loss: 0.0103 - val_acc: 0.8599\n",
      "Dropout: 0.53\n",
      "Train on 7997 samples, validate on 999 samples\n",
      "Epoch 1/10\n",
      "7997/7997 [==============================] - 6s 757us/step - loss: 0.0478 - acc: 0.1225 - val_loss: 0.0432 - val_acc: 0.2312\n",
      "Epoch 2/10\n",
      "7997/7997 [==============================] - 2s 296us/step - loss: 0.0432 - acc: 0.2266 - val_loss: 0.0339 - val_acc: 0.4705\n",
      "Epoch 3/10\n",
      "7997/7997 [==============================] - 2s 301us/step - loss: 0.0389 - acc: 0.3332 - val_loss: 0.0272 - val_acc: 0.5846\n",
      "Epoch 4/10\n",
      "7997/7997 [==============================] - 2s 294us/step - loss: 0.0354 - acc: 0.4222 - val_loss: 0.0228 - val_acc: 0.6607\n",
      "Epoch 5/10\n",
      "7997/7997 [==============================] - 2s 306us/step - loss: 0.0323 - acc: 0.4911 - val_loss: 0.0193 - val_acc: 0.7157\n",
      "Epoch 6/10\n",
      "7997/7997 [==============================] - 2s 301us/step - loss: 0.0300 - acc: 0.5415 - val_loss: 0.0167 - val_acc: 0.7678\n",
      "Epoch 7/10\n",
      "7997/7997 [==============================] - 2s 306us/step - loss: 0.0280 - acc: 0.5851 - val_loss: 0.0144 - val_acc: 0.8028\n",
      "Epoch 8/10\n",
      "7997/7997 [==============================] - 2s 305us/step - loss: 0.0264 - acc: 0.6146 - val_loss: 0.0124 - val_acc: 0.8368\n",
      "Epoch 9/10\n",
      "7997/7997 [==============================] - 2s 301us/step - loss: 0.0249 - acc: 0.6387 - val_loss: 0.0112 - val_acc: 0.8569\n",
      "Epoch 10/10\n",
      "7997/7997 [==============================] - 2s 302us/step - loss: 0.0236 - acc: 0.6699 - val_loss: 0.0102 - val_acc: 0.8749\n",
      "Dropout: 0.54\n",
      "Train on 7997 samples, validate on 999 samples\n",
      "Epoch 1/10\n",
      "7997/7997 [==============================] - 6s 767us/step - loss: 0.0478 - acc: 0.1182 - val_loss: 0.0441 - val_acc: 0.2162\n",
      "Epoch 2/10\n",
      "7997/7997 [==============================] - 2s 302us/step - loss: 0.0435 - acc: 0.2191 - val_loss: 0.0342 - val_acc: 0.4515\n",
      "Epoch 3/10\n",
      "7997/7997 [==============================] - 2s 312us/step - loss: 0.0393 - acc: 0.3204 - val_loss: 0.0278 - val_acc: 0.5676\n",
      "Epoch 4/10\n",
      "7997/7997 [==============================] - 2s 305us/step - loss: 0.0360 - acc: 0.4062 - val_loss: 0.0237 - val_acc: 0.6486\n",
      "Epoch 5/10\n",
      "7997/7997 [==============================] - 2s 301us/step - loss: 0.0330 - acc: 0.4744 - val_loss: 0.0199 - val_acc: 0.6987\n",
      "Epoch 6/10\n",
      "7997/7997 [==============================] - 2s 303us/step - loss: 0.0305 - acc: 0.5307 - val_loss: 0.0171 - val_acc: 0.7538\n",
      "Epoch 7/10\n",
      "7997/7997 [==============================] - 2s 299us/step - loss: 0.0285 - acc: 0.5727 - val_loss: 0.0150 - val_acc: 0.7958\n",
      "Epoch 8/10\n",
      "7997/7997 [==============================] - 2s 299us/step - loss: 0.0268 - acc: 0.6070 - val_loss: 0.0132 - val_acc: 0.8208\n",
      "Epoch 9/10\n",
      "7997/7997 [==============================] - 2s 303us/step - loss: 0.0253 - acc: 0.6362 - val_loss: 0.0117 - val_acc: 0.8448\n",
      "Epoch 10/10\n",
      "7997/7997 [==============================] - 2s 309us/step - loss: 0.0239 - acc: 0.6569 - val_loss: 0.0105 - val_acc: 0.8709\n",
      "Dropout: 0.55\n",
      "Train on 7997 samples, validate on 999 samples\n",
      "Epoch 1/10\n",
      "7997/7997 [==============================] - 6s 761us/step - loss: 0.0478 - acc: 0.1162 - val_loss: 0.0440 - val_acc: 0.2242\n",
      "Epoch 2/10\n",
      "7997/7997 [==============================] - 2s 302us/step - loss: 0.0436 - acc: 0.2133 - val_loss: 0.0343 - val_acc: 0.4515\n",
      "Epoch 3/10\n",
      "7997/7997 [==============================] - 2s 299us/step - loss: 0.0394 - acc: 0.3206 - val_loss: 0.0283 - val_acc: 0.5696\n",
      "Epoch 4/10\n",
      "7997/7997 [==============================] - 2s 299us/step - loss: 0.0363 - acc: 0.3968 - val_loss: 0.0238 - val_acc: 0.6366\n",
      "Epoch 5/10\n",
      "7997/7997 [==============================] - 2s 304us/step - loss: 0.0333 - acc: 0.4734 - val_loss: 0.0202 - val_acc: 0.6977\n",
      "Epoch 6/10\n",
      "7997/7997 [==============================] - 2s 297us/step - loss: 0.0308 - acc: 0.5301 - val_loss: 0.0182 - val_acc: 0.7197\n",
      "Epoch 7/10\n",
      "7997/7997 [==============================] - 3s 313us/step - loss: 0.0290 - acc: 0.5642 - val_loss: 0.0156 - val_acc: 0.7798\n",
      "Epoch 8/10\n",
      "7997/7997 [==============================] - 2s 307us/step - loss: 0.0273 - acc: 0.5923 - val_loss: 0.0137 - val_acc: 0.8078\n",
      "Epoch 9/10\n",
      "7997/7997 [==============================] - 2s 303us/step - loss: 0.0257 - acc: 0.6287 - val_loss: 0.0121 - val_acc: 0.8398\n",
      "Epoch 10/10\n",
      "7997/7997 [==============================] - 2s 301us/step - loss: 0.0245 - acc: 0.6530 - val_loss: 0.0110 - val_acc: 0.8569\n",
      "Dropout: 0.56\n",
      "Train on 7997 samples, validate on 999 samples\n",
      "Epoch 1/10\n",
      "7997/7997 [==============================] - 6s 779us/step - loss: 0.0478 - acc: 0.1140 - val_loss: 0.0446 - val_acc: 0.2072\n",
      "Epoch 2/10\n",
      "7997/7997 [==============================] - 2s 301us/step - loss: 0.0439 - acc: 0.2075 - val_loss: 0.0350 - val_acc: 0.4404\n",
      "Epoch 3/10\n",
      "7997/7997 [==============================] - 2s 302us/step - loss: 0.0400 - acc: 0.3029 - val_loss: 0.0286 - val_acc: 0.5666\n",
      "Epoch 4/10\n",
      "7997/7997 [==============================] - 2s 301us/step - loss: 0.0368 - acc: 0.3831 - val_loss: 0.0244 - val_acc: 0.6426\n",
      "Epoch 5/10\n",
      "7997/7997 [==============================] - 2s 304us/step - loss: 0.0339 - acc: 0.4573 - val_loss: 0.0212 - val_acc: 0.6817\n",
      "Epoch 6/10\n",
      "7997/7997 [==============================] - 2s 300us/step - loss: 0.0316 - acc: 0.5151 - val_loss: 0.0187 - val_acc: 0.7317\n",
      "Epoch 7/10\n",
      "7997/7997 [==============================] - 2s 301us/step - loss: 0.0297 - acc: 0.5520 - val_loss: 0.0162 - val_acc: 0.7718\n",
      "Epoch 8/10\n",
      "7997/7997 [==============================] - 2s 303us/step - loss: 0.0282 - acc: 0.5795 - val_loss: 0.0141 - val_acc: 0.8068\n",
      "Epoch 9/10\n",
      "7997/7997 [==============================] - 2s 301us/step - loss: 0.0266 - acc: 0.6074 - val_loss: 0.0123 - val_acc: 0.8348\n",
      "Epoch 10/10\n",
      "7997/7997 [==============================] - 2s 305us/step - loss: 0.0252 - acc: 0.6419 - val_loss: 0.0113 - val_acc: 0.8529\n",
      "Dropout: 0.57\n",
      "Train on 7997 samples, validate on 999 samples\n",
      "Epoch 1/10\n",
      "7997/7997 [==============================] - 6s 777us/step - loss: 0.0478 - acc: 0.1153 - val_loss: 0.0450 - val_acc: 0.2002\n",
      "Epoch 2/10\n",
      "7997/7997 [==============================] - 2s 304us/step - loss: 0.0439 - acc: 0.2076 - val_loss: 0.0353 - val_acc: 0.4284\n",
      "Epoch 3/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7997/7997 [==============================] - 2s 304us/step - loss: 0.0402 - acc: 0.2960 - val_loss: 0.0289 - val_acc: 0.5636\n",
      "Epoch 4/10\n",
      "7997/7997 [==============================] - 2s 312us/step - loss: 0.0371 - acc: 0.3766 - val_loss: 0.0247 - val_acc: 0.6166\n",
      "Epoch 5/10\n",
      "7997/7997 [==============================] - 3s 322us/step - loss: 0.0345 - acc: 0.4368 - val_loss: 0.0214 - val_acc: 0.6807\n",
      "Epoch 6/10\n",
      "7997/7997 [==============================] - 3s 313us/step - loss: 0.0320 - acc: 0.5061 - val_loss: 0.0186 - val_acc: 0.7297\n",
      "Epoch 7/10\n",
      "7997/7997 [==============================] - 3s 322us/step - loss: 0.0301 - acc: 0.5407 - val_loss: 0.0161 - val_acc: 0.7788\n",
      "Epoch 8/10\n",
      "7997/7997 [==============================] - 2s 312us/step - loss: 0.0284 - acc: 0.5785 - val_loss: 0.0141 - val_acc: 0.8108\n",
      "Epoch 9/10\n",
      "7997/7997 [==============================] - 3s 320us/step - loss: 0.0269 - acc: 0.6057 - val_loss: 0.0127 - val_acc: 0.8238\n",
      "Epoch 10/10\n",
      "7997/7997 [==============================] - 3s 327us/step - loss: 0.0256 - acc: 0.6286 - val_loss: 0.0117 - val_acc: 0.8448\n",
      "Dropout: 0.58\n",
      "Train on 7997 samples, validate on 999 samples\n",
      "Epoch 1/10\n",
      "7997/7997 [==============================] - 6s 799us/step - loss: 0.0478 - acc: 0.1099 - val_loss: 0.0454 - val_acc: 0.1972\n",
      "Epoch 2/10\n",
      "7997/7997 [==============================] - 2s 312us/step - loss: 0.0443 - acc: 0.1911 - val_loss: 0.0355 - val_acc: 0.4194\n",
      "Epoch 3/10\n",
      "7997/7997 [==============================] - 2s 307us/step - loss: 0.0406 - acc: 0.2886 - val_loss: 0.0290 - val_acc: 0.5606\n",
      "Epoch 4/10\n",
      "7997/7997 [==============================] - 2s 306us/step - loss: 0.0374 - acc: 0.3695 - val_loss: 0.0245 - val_acc: 0.6356\n",
      "Epoch 5/10\n",
      "7997/7997 [==============================] - 2s 304us/step - loss: 0.0348 - acc: 0.4334 - val_loss: 0.0210 - val_acc: 0.6857\n",
      "Epoch 6/10\n",
      "7997/7997 [==============================] - 2s 304us/step - loss: 0.0323 - acc: 0.4992 - val_loss: 0.0185 - val_acc: 0.7267\n",
      "Epoch 7/10\n",
      "7997/7997 [==============================] - 2s 305us/step - loss: 0.0304 - acc: 0.5381 - val_loss: 0.0162 - val_acc: 0.7638\n",
      "Epoch 8/10\n",
      "7997/7997 [==============================] - 2s 307us/step - loss: 0.0289 - acc: 0.5697 - val_loss: 0.0140 - val_acc: 0.8108\n",
      "Epoch 9/10\n",
      "7997/7997 [==============================] - 3s 314us/step - loss: 0.0273 - acc: 0.5975 - val_loss: 0.0128 - val_acc: 0.8268\n",
      "Epoch 10/10\n",
      "7997/7997 [==============================] - 2s 310us/step - loss: 0.0261 - acc: 0.6267 - val_loss: 0.0119 - val_acc: 0.8408\n",
      "Dropout: 0.59\n",
      "Train on 7997 samples, validate on 999 samples\n",
      "Epoch 1/10\n",
      "7997/7997 [==============================] - 7s 819us/step - loss: 0.0478 - acc: 0.1097 - val_loss: 0.0456 - val_acc: 0.1882\n",
      "Epoch 2/10\n",
      "7997/7997 [==============================] - 2s 308us/step - loss: 0.0444 - acc: 0.1872 - val_loss: 0.0360 - val_acc: 0.4074\n",
      "Epoch 3/10\n",
      "7997/7997 [==============================] - 2s 310us/step - loss: 0.0410 - acc: 0.2801 - val_loss: 0.0295 - val_acc: 0.5485\n",
      "Epoch 4/10\n",
      "7997/7997 [==============================] - 2s 310us/step - loss: 0.0380 - acc: 0.3543 - val_loss: 0.0256 - val_acc: 0.6246\n",
      "Epoch 5/10\n",
      "7997/7997 [==============================] - 3s 320us/step - loss: 0.0354 - acc: 0.4237 - val_loss: 0.0222 - val_acc: 0.6697\n",
      "Epoch 6/10\n",
      "7997/7997 [==============================] - 2s 311us/step - loss: 0.0331 - acc: 0.4829 - val_loss: 0.0200 - val_acc: 0.6997\n",
      "Epoch 7/10\n",
      "7997/7997 [==============================] - 2s 313us/step - loss: 0.0313 - acc: 0.5189 - val_loss: 0.0176 - val_acc: 0.7528\n",
      "Epoch 8/10\n",
      "7997/7997 [==============================] - 3s 344us/step - loss: 0.0295 - acc: 0.5570 - val_loss: 0.0154 - val_acc: 0.7828\n",
      "Epoch 9/10\n",
      "7997/7997 [==============================] - 3s 319us/step - loss: 0.0281 - acc: 0.5838 - val_loss: 0.0139 - val_acc: 0.8098\n",
      "Epoch 10/10\n",
      "7997/7997 [==============================] - 3s 315us/step - loss: 0.0267 - acc: 0.6101 - val_loss: 0.0124 - val_acc: 0.8378\n",
      "Dropout: 0.6\n",
      "Train on 7997 samples, validate on 999 samples\n",
      "Epoch 1/10\n",
      "7997/7997 [==============================] - 7s 840us/step - loss: 0.0477 - acc: 0.1089 - val_loss: 0.0459 - val_acc: 0.1852\n",
      "Epoch 2/10\n",
      "7997/7997 [==============================] - 3s 333us/step - loss: 0.0446 - acc: 0.1823 - val_loss: 0.0368 - val_acc: 0.3764\n",
      "Epoch 3/10\n",
      "7997/7997 [==============================] - 3s 335us/step - loss: 0.0412 - acc: 0.2716 - val_loss: 0.0306 - val_acc: 0.5145\n",
      "Epoch 4/10\n",
      "7997/7997 [==============================] - 3s 327us/step - loss: 0.0385 - acc: 0.3454 - val_loss: 0.0265 - val_acc: 0.5986\n",
      "Epoch 5/10\n",
      "7997/7997 [==============================] - 3s 323us/step - loss: 0.0358 - acc: 0.4072 - val_loss: 0.0233 - val_acc: 0.6456\n",
      "Epoch 6/10\n",
      "7997/7997 [==============================] - 3s 343us/step - loss: 0.0335 - acc: 0.4701 - val_loss: 0.0200 - val_acc: 0.6987\n",
      "Epoch 7/10\n",
      "7997/7997 [==============================] - 3s 347us/step - loss: 0.0317 - acc: 0.5073 - val_loss: 0.0174 - val_acc: 0.7427\n",
      "Epoch 8/10\n",
      "7997/7997 [==============================] - 3s 335us/step - loss: 0.0301 - acc: 0.5462 - val_loss: 0.0156 - val_acc: 0.7778\n",
      "Epoch 9/10\n",
      "7997/7997 [==============================] - 3s 313us/step - loss: 0.0285 - acc: 0.5763 - val_loss: 0.0139 - val_acc: 0.8028\n",
      "Epoch 10/10\n",
      "7997/7997 [==============================] - 2s 309us/step - loss: 0.0272 - acc: 0.6034 - val_loss: 0.0125 - val_acc: 0.8308\n",
      "Dropout: 0.61\n",
      "Train on 7997 samples, validate on 999 samples\n",
      "Epoch 1/10\n",
      "7997/7997 [==============================] - 6s 807us/step - loss: 0.0478 - acc: 0.1102 - val_loss: 0.0464 - val_acc: 0.1752\n",
      "Epoch 2/10\n",
      "7997/7997 [==============================] - 2s 308us/step - loss: 0.0446 - acc: 0.1816 - val_loss: 0.0367 - val_acc: 0.3834\n",
      "Epoch 3/10\n",
      "7997/7997 [==============================] - 2s 311us/step - loss: 0.0414 - acc: 0.2627 - val_loss: 0.0304 - val_acc: 0.5265\n",
      "Epoch 4/10\n",
      "7997/7997 [==============================] - 2s 312us/step - loss: 0.0388 - acc: 0.3380 - val_loss: 0.0268 - val_acc: 0.5906\n",
      "Epoch 5/10\n",
      "7997/7997 [==============================] - 2s 310us/step - loss: 0.0363 - acc: 0.3978 - val_loss: 0.0237 - val_acc: 0.6336\n",
      "Epoch 6/10\n",
      "7997/7997 [==============================] - 3s 323us/step - loss: 0.0342 - acc: 0.4524 - val_loss: 0.0209 - val_acc: 0.6827\n",
      "Epoch 7/10\n",
      "7997/7997 [==============================] - 3s 325us/step - loss: 0.0325 - acc: 0.4896 - val_loss: 0.0185 - val_acc: 0.7257\n",
      "Epoch 8/10\n",
      "7997/7997 [==============================] - 2s 311us/step - loss: 0.0307 - acc: 0.5287 - val_loss: 0.0162 - val_acc: 0.7648\n",
      "Epoch 9/10\n",
      "7997/7997 [==============================] - 3s 315us/step - loss: 0.0294 - acc: 0.5581 - val_loss: 0.0152 - val_acc: 0.7798\n",
      "Epoch 10/10\n",
      "7997/7997 [==============================] - 3s 313us/step - loss: 0.0279 - acc: 0.5928 - val_loss: 0.0136 - val_acc: 0.8068\n",
      "Dropout: 0.62\n",
      "Train on 7997 samples, validate on 999 samples\n",
      "Epoch 1/10\n",
      "7997/7997 [==============================] - 7s 829us/step - loss: 0.0477 - acc: 0.1089 - val_loss: 0.0469 - val_acc: 0.1642\n",
      "Epoch 2/10\n",
      "7997/7997 [==============================] - 3s 317us/step - loss: 0.0447 - acc: 0.1787 - val_loss: 0.0373 - val_acc: 0.3734\n",
      "Epoch 3/10\n",
      "7997/7997 [==============================] - 2s 309us/step - loss: 0.0416 - acc: 0.2612 - val_loss: 0.0308 - val_acc: 0.5245\n",
      "Epoch 4/10\n",
      "7997/7997 [==============================] - 2s 309us/step - loss: 0.0390 - acc: 0.3281 - val_loss: 0.0270 - val_acc: 0.5876\n",
      "Epoch 5/10\n",
      "7997/7997 [==============================] - 3s 315us/step - loss: 0.0366 - acc: 0.3910 - val_loss: 0.0235 - val_acc: 0.6517\n",
      "Epoch 6/10\n",
      "7997/7997 [==============================] - 3s 313us/step - loss: 0.0346 - acc: 0.4424 - val_loss: 0.0207 - val_acc: 0.6977\n",
      "Epoch 7/10\n",
      "7997/7997 [==============================] - 2s 306us/step - loss: 0.0328 - acc: 0.4856 - val_loss: 0.0185 - val_acc: 0.7247\n",
      "Epoch 8/10\n",
      "7997/7997 [==============================] - 2s 311us/step - loss: 0.0312 - acc: 0.5213 - val_loss: 0.0163 - val_acc: 0.7538\n",
      "Epoch 9/10\n",
      "7997/7997 [==============================] - 3s 315us/step - loss: 0.0297 - acc: 0.5505 - val_loss: 0.0146 - val_acc: 0.7928\n",
      "Epoch 10/10\n",
      "7997/7997 [==============================] - 2s 305us/step - loss: 0.0283 - acc: 0.5843 - val_loss: 0.0133 - val_acc: 0.8218\n",
      "Dropout: 0.63\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7997 samples, validate on 999 samples\n",
      "Epoch 1/10\n",
      "7997/7997 [==============================] - 6s 798us/step - loss: 0.0476 - acc: 0.1102 - val_loss: 0.0469 - val_acc: 0.1662\n",
      "Epoch 2/10\n",
      "7997/7997 [==============================] - 2s 300us/step - loss: 0.0448 - acc: 0.1738 - val_loss: 0.0375 - val_acc: 0.3664\n",
      "Epoch 3/10\n",
      "7997/7997 [==============================] - 2s 304us/step - loss: 0.0419 - acc: 0.2455 - val_loss: 0.0312 - val_acc: 0.5255\n",
      "Epoch 4/10\n",
      "7997/7997 [==============================] - 2s 299us/step - loss: 0.0394 - acc: 0.3201 - val_loss: 0.0264 - val_acc: 0.5936\n",
      "Epoch 5/10\n",
      "7997/7997 [==============================] - 2s 302us/step - loss: 0.0371 - acc: 0.3794 - val_loss: 0.0231 - val_acc: 0.6476\n",
      "Epoch 6/10\n",
      "7997/7997 [==============================] - 2s 300us/step - loss: 0.0351 - acc: 0.4338 - val_loss: 0.0210 - val_acc: 0.6847\n",
      "Epoch 7/10\n",
      "7997/7997 [==============================] - 2s 307us/step - loss: 0.0333 - acc: 0.4728 - val_loss: 0.0187 - val_acc: 0.7307\n",
      "Epoch 8/10\n",
      "7997/7997 [==============================] - 2s 303us/step - loss: 0.0318 - acc: 0.5063 - val_loss: 0.0171 - val_acc: 0.7518\n",
      "Epoch 9/10\n",
      "7997/7997 [==============================] - 2s 300us/step - loss: 0.0302 - acc: 0.5376 - val_loss: 0.0151 - val_acc: 0.7858\n",
      "Epoch 10/10\n",
      "7997/7997 [==============================] - 2s 303us/step - loss: 0.0289 - acc: 0.5685 - val_loss: 0.0136 - val_acc: 0.8128\n",
      "Dropout: 0.64\n",
      "Train on 7997 samples, validate on 999 samples\n",
      "Epoch 1/10\n",
      "7997/7997 [==============================] - 6s 803us/step - loss: 0.0475 - acc: 0.1050 - val_loss: 0.0469 - val_acc: 0.1562\n",
      "Epoch 2/10\n",
      "7997/7997 [==============================] - 2s 297us/step - loss: 0.0449 - acc: 0.1666 - val_loss: 0.0380 - val_acc: 0.3714\n",
      "Epoch 3/10\n",
      "7997/7997 [==============================] - 2s 309us/step - loss: 0.0422 - acc: 0.2456 - val_loss: 0.0319 - val_acc: 0.4855\n",
      "Epoch 4/10\n",
      "7997/7997 [==============================] - 3s 326us/step - loss: 0.0398 - acc: 0.3106 - val_loss: 0.0276 - val_acc: 0.5736\n",
      "Epoch 5/10\n",
      "7997/7997 [==============================] - 3s 324us/step - loss: 0.0375 - acc: 0.3683 - val_loss: 0.0245 - val_acc: 0.6246\n",
      "Epoch 6/10\n",
      "7997/7997 [==============================] - 2s 307us/step - loss: 0.0356 - acc: 0.4187 - val_loss: 0.0223 - val_acc: 0.6567\n",
      "Epoch 7/10\n",
      "7997/7997 [==============================] - 2s 300us/step - loss: 0.0339 - acc: 0.4600 - val_loss: 0.0198 - val_acc: 0.7047\n",
      "Epoch 8/10\n",
      "7997/7997 [==============================] - 2s 306us/step - loss: 0.0324 - acc: 0.4912 - val_loss: 0.0173 - val_acc: 0.7538\n",
      "Epoch 9/10\n",
      "7997/7997 [==============================] - 2s 298us/step - loss: 0.0310 - acc: 0.5209 - val_loss: 0.0159 - val_acc: 0.7828\n",
      "Epoch 10/10\n",
      "7997/7997 [==============================] - 2s 302us/step - loss: 0.0297 - acc: 0.5582 - val_loss: 0.0144 - val_acc: 0.8088\n",
      "Dropout: 0.65\n",
      "Train on 7997 samples, validate on 999 samples\n",
      "Epoch 1/10\n",
      "7997/7997 [==============================] - 7s 821us/step - loss: 0.0475 - acc: 0.1060 - val_loss: 0.0471 - val_acc: 0.1481\n",
      "Epoch 2/10\n",
      "7997/7997 [==============================] - 3s 316us/step - loss: 0.0450 - acc: 0.1623 - val_loss: 0.0384 - val_acc: 0.3644\n",
      "Epoch 3/10\n",
      "7997/7997 [==============================] - 3s 323us/step - loss: 0.0424 - acc: 0.2380 - val_loss: 0.0320 - val_acc: 0.4835\n",
      "Epoch 4/10\n",
      "7997/7997 [==============================] - 3s 315us/step - loss: 0.0401 - acc: 0.3026 - val_loss: 0.0280 - val_acc: 0.5495\n",
      "Epoch 5/10\n",
      "7997/7997 [==============================] - 3s 314us/step - loss: 0.0379 - acc: 0.3599 - val_loss: 0.0243 - val_acc: 0.6336\n",
      "Epoch 6/10\n",
      "7997/7997 [==============================] - 3s 319us/step - loss: 0.0359 - acc: 0.4138 - val_loss: 0.0217 - val_acc: 0.6727\n",
      "Epoch 7/10\n",
      "7997/7997 [==============================] - 2s 310us/step - loss: 0.0344 - acc: 0.4524 - val_loss: 0.0192 - val_acc: 0.7137\n",
      "Epoch 8/10\n",
      "7997/7997 [==============================] - 2s 308us/step - loss: 0.0329 - acc: 0.4853 - val_loss: 0.0169 - val_acc: 0.7558\n",
      "Epoch 9/10\n",
      "7997/7997 [==============================] - 2s 304us/step - loss: 0.0315 - acc: 0.5164 - val_loss: 0.0154 - val_acc: 0.7858\n",
      "Epoch 10/10\n",
      "7997/7997 [==============================] - 2s 303us/step - loss: 0.0300 - acc: 0.5532 - val_loss: 0.0143 - val_acc: 0.8008\n",
      "Dropout: 0.66\n",
      "Train on 7997 samples, validate on 999 samples\n",
      "Epoch 1/10\n",
      "7997/7997 [==============================] - 7s 826us/step - loss: 0.0474 - acc: 0.1069 - val_loss: 0.0479 - val_acc: 0.1491\n",
      "Epoch 2/10\n",
      "7997/7997 [==============================] - 2s 302us/step - loss: 0.0450 - acc: 0.1639 - val_loss: 0.0395 - val_acc: 0.3153\n",
      "Epoch 3/10\n",
      "7997/7997 [==============================] - 2s 311us/step - loss: 0.0427 - acc: 0.2257 - val_loss: 0.0331 - val_acc: 0.4645\n",
      "Epoch 4/10\n",
      "7997/7997 [==============================] - 3s 313us/step - loss: 0.0405 - acc: 0.2889 - val_loss: 0.0284 - val_acc: 0.5666\n",
      "Epoch 5/10\n",
      "7997/7997 [==============================] - 3s 324us/step - loss: 0.0385 - acc: 0.3430 - val_loss: 0.0244 - val_acc: 0.6296\n",
      "Epoch 6/10\n",
      "7997/7997 [==============================] - 3s 315us/step - loss: 0.0365 - acc: 0.4004 - val_loss: 0.0221 - val_acc: 0.6647\n",
      "Epoch 7/10\n",
      "7997/7997 [==============================] - 3s 335us/step - loss: 0.0351 - acc: 0.4349 - val_loss: 0.0194 - val_acc: 0.7157\n",
      "Epoch 8/10\n",
      "7997/7997 [==============================] - 3s 329us/step - loss: 0.0335 - acc: 0.4702 - val_loss: 0.0180 - val_acc: 0.7347\n",
      "Epoch 9/10\n",
      "7997/7997 [==============================] - 2s 307us/step - loss: 0.0321 - acc: 0.5068 - val_loss: 0.0166 - val_acc: 0.7568\n",
      "Epoch 10/10\n",
      "7997/7997 [==============================] - 3s 313us/step - loss: 0.0307 - acc: 0.5336 - val_loss: 0.0151 - val_acc: 0.7838\n",
      "Dropout: 0.67\n",
      "Train on 7997 samples, validate on 999 samples\n",
      "Epoch 1/10\n",
      "7997/7997 [==============================] - 7s 843us/step - loss: 0.0474 - acc: 0.1053 - val_loss: 0.0482 - val_acc: 0.1321\n",
      "Epoch 2/10\n",
      "7997/7997 [==============================] - 3s 330us/step - loss: 0.0452 - acc: 0.1574 - val_loss: 0.0392 - val_acc: 0.3243\n",
      "Epoch 3/10\n",
      "7997/7997 [==============================] - 3s 347us/step - loss: 0.0431 - acc: 0.2173 - val_loss: 0.0335 - val_acc: 0.4675\n",
      "Epoch 4/10\n",
      "7997/7997 [==============================] - 3s 325us/step - loss: 0.0410 - acc: 0.2765 - val_loss: 0.0293 - val_acc: 0.5455\n",
      "Epoch 5/10\n",
      "7997/7997 [==============================] - 2s 309us/step - loss: 0.0389 - acc: 0.3353 - val_loss: 0.0254 - val_acc: 0.6106\n",
      "Epoch 6/10\n",
      "7997/7997 [==============================] - 3s 322us/step - loss: 0.0370 - acc: 0.3824 - val_loss: 0.0230 - val_acc: 0.6527\n",
      "Epoch 7/10\n",
      "7997/7997 [==============================] - 2s 311us/step - loss: 0.0356 - acc: 0.4222 - val_loss: 0.0209 - val_acc: 0.6907\n",
      "Epoch 8/10\n",
      "7997/7997 [==============================] - 3s 318us/step - loss: 0.0341 - acc: 0.4603 - val_loss: 0.0191 - val_acc: 0.7127\n",
      "Epoch 9/10\n",
      "7997/7997 [==============================] - 2s 307us/step - loss: 0.0328 - acc: 0.4868 - val_loss: 0.0178 - val_acc: 0.7327\n",
      "Epoch 10/10\n",
      "7997/7997 [==============================] - 2s 311us/step - loss: 0.0314 - acc: 0.5162 - val_loss: 0.0161 - val_acc: 0.7628\n",
      "Dropout: 0.68\n",
      "Train on 7997 samples, validate on 999 samples\n",
      "Epoch 1/10\n",
      "7997/7997 [==============================] - 7s 842us/step - loss: 0.0473 - acc: 0.1045 - val_loss: 0.0487 - val_acc: 0.1231\n",
      "Epoch 2/10\n",
      "7997/7997 [==============================] - 2s 309us/step - loss: 0.0454 - acc: 0.1508 - val_loss: 0.0400 - val_acc: 0.3083\n",
      "Epoch 3/10\n",
      "7997/7997 [==============================] - 3s 314us/step - loss: 0.0433 - acc: 0.2105 - val_loss: 0.0340 - val_acc: 0.4454\n",
      "Epoch 4/10\n",
      "7997/7997 [==============================] - 2s 310us/step - loss: 0.0414 - acc: 0.2697 - val_loss: 0.0301 - val_acc: 0.5155\n",
      "Epoch 5/10\n",
      "7997/7997 [==============================] - 3s 320us/step - loss: 0.0394 - acc: 0.3206 - val_loss: 0.0261 - val_acc: 0.6026\n",
      "Epoch 6/10\n",
      "7997/7997 [==============================] - 3s 313us/step - loss: 0.0378 - acc: 0.3664 - val_loss: 0.0232 - val_acc: 0.6476\n",
      "Epoch 7/10\n",
      "7997/7997 [==============================] - 2s 310us/step - loss: 0.0364 - acc: 0.4002 - val_loss: 0.0210 - val_acc: 0.6807\n",
      "Epoch 8/10\n",
      "7997/7997 [==============================] - 2s 307us/step - loss: 0.0350 - acc: 0.4397 - val_loss: 0.0190 - val_acc: 0.7157\n",
      "Epoch 9/10\n",
      "7997/7997 [==============================] - 2s 302us/step - loss: 0.0336 - acc: 0.4686 - val_loss: 0.0178 - val_acc: 0.7347\n",
      "Epoch 10/10\n",
      "7997/7997 [==============================] - 2s 305us/step - loss: 0.0322 - acc: 0.5008 - val_loss: 0.0161 - val_acc: 0.7638\n",
      "Dropout: 0.69\n",
      "Train on 7997 samples, validate on 999 samples\n",
      "Epoch 1/10\n",
      "7997/7997 [==============================] - 7s 837us/step - loss: 0.0473 - acc: 0.1058 - val_loss: 0.0487 - val_acc: 0.1391\n",
      "Epoch 2/10\n",
      "7997/7997 [==============================] - 2s 310us/step - loss: 0.0454 - acc: 0.1469 - val_loss: 0.0407 - val_acc: 0.2883\n",
      "Epoch 3/10\n",
      "7997/7997 [==============================] - 2s 302us/step - loss: 0.0433 - acc: 0.2077 - val_loss: 0.0336 - val_acc: 0.4615\n",
      "Epoch 4/10\n",
      "7997/7997 [==============================] - 3s 316us/step - loss: 0.0414 - acc: 0.2637 - val_loss: 0.0297 - val_acc: 0.5225\n",
      "Epoch 5/10\n",
      "7997/7997 [==============================] - 3s 325us/step - loss: 0.0396 - acc: 0.3156 - val_loss: 0.0271 - val_acc: 0.5756\n",
      "Epoch 6/10\n",
      "7997/7997 [==============================] - 2s 305us/step - loss: 0.0381 - acc: 0.3589 - val_loss: 0.0246 - val_acc: 0.6176\n",
      "Epoch 7/10\n",
      "7997/7997 [==============================] - 2s 304us/step - loss: 0.0367 - acc: 0.4000 - val_loss: 0.0230 - val_acc: 0.6356\n",
      "Epoch 8/10\n",
      "7997/7997 [==============================] - 3s 322us/step - loss: 0.0354 - acc: 0.4294 - val_loss: 0.0218 - val_acc: 0.6627\n",
      "Epoch 9/10\n",
      "7997/7997 [==============================] - 3s 314us/step - loss: 0.0341 - acc: 0.4564 - val_loss: 0.0201 - val_acc: 0.6847\n",
      "Epoch 10/10\n",
      "7997/7997 [==============================] - 2s 304us/step - loss: 0.0328 - acc: 0.4858 - val_loss: 0.0186 - val_acc: 0.7217\n",
      "Dropout: 0.7\n",
      "Train on 7997 samples, validate on 999 samples\n",
      "Epoch 1/10\n",
      "7997/7997 [==============================] - 7s 833us/step - loss: 0.0472 - acc: 0.1072 - val_loss: 0.0495 - val_acc: 0.1201\n",
      "Epoch 2/10\n",
      "7997/7997 [==============================] - 2s 306us/step - loss: 0.0455 - acc: 0.1444 - val_loss: 0.0412 - val_acc: 0.2743\n",
      "Epoch 3/10\n",
      "7997/7997 [==============================] - 2s 306us/step - loss: 0.0436 - acc: 0.1912 - val_loss: 0.0356 - val_acc: 0.4064\n",
      "Epoch 4/10\n",
      "7997/7997 [==============================] - 2s 309us/step - loss: 0.0419 - acc: 0.2465 - val_loss: 0.0311 - val_acc: 0.4925\n",
      "Epoch 5/10\n",
      "7997/7997 [==============================] - 2s 309us/step - loss: 0.0402 - acc: 0.2952 - val_loss: 0.0277 - val_acc: 0.5726\n",
      "Epoch 6/10\n",
      "7997/7997 [==============================] - 2s 304us/step - loss: 0.0387 - acc: 0.3409 - val_loss: 0.0251 - val_acc: 0.6216\n",
      "Epoch 7/10\n",
      "7997/7997 [==============================] - 2s 305us/step - loss: 0.0373 - acc: 0.3823 - val_loss: 0.0224 - val_acc: 0.6557\n",
      "Epoch 8/10\n",
      "7997/7997 [==============================] - 2s 307us/step - loss: 0.0360 - acc: 0.4109 - val_loss: 0.0204 - val_acc: 0.6947\n",
      "Epoch 9/10\n",
      "7997/7997 [==============================] - 2s 309us/step - loss: 0.0346 - acc: 0.4484 - val_loss: 0.0189 - val_acc: 0.7187\n",
      "Epoch 10/10\n",
      "7997/7997 [==============================] - 2s 312us/step - loss: 0.0334 - acc: 0.4741 - val_loss: 0.0173 - val_acc: 0.7367\n",
      "Dropout: 0.71\n",
      "Train on 7997 samples, validate on 999 samples\n",
      "Epoch 1/10\n",
      "7997/7997 [==============================] - 7s 839us/step - loss: 0.0471 - acc: 0.1058 - val_loss: 0.0491 - val_acc: 0.1151\n",
      "Epoch 2/10\n",
      "7997/7997 [==============================] - 2s 303us/step - loss: 0.0455 - acc: 0.1438 - val_loss: 0.0408 - val_acc: 0.2703\n",
      "Epoch 3/10\n",
      "7997/7997 [==============================] - 2s 304us/step - loss: 0.0436 - acc: 0.1959 - val_loss: 0.0347 - val_acc: 0.4344\n",
      "Epoch 4/10\n",
      "7997/7997 [==============================] - 2s 306us/step - loss: 0.0420 - acc: 0.2455 - val_loss: 0.0305 - val_acc: 0.5105\n",
      "Epoch 5/10\n",
      "7997/7997 [==============================] - 2s 311us/step - loss: 0.0402 - acc: 0.2981 - val_loss: 0.0273 - val_acc: 0.5766\n",
      "Epoch 6/10\n",
      "7997/7997 [==============================] - 3s 317us/step - loss: 0.0387 - acc: 0.3421 - val_loss: 0.0249 - val_acc: 0.6196\n",
      "Epoch 7/10\n",
      "7997/7997 [==============================] - 2s 313us/step - loss: 0.0376 - acc: 0.3709 - val_loss: 0.0232 - val_acc: 0.6496\n",
      "Epoch 8/10\n",
      "7997/7997 [==============================] - 2s 310us/step - loss: 0.0363 - acc: 0.4074 - val_loss: 0.0212 - val_acc: 0.6797\n",
      "Epoch 9/10\n",
      "7997/7997 [==============================] - 2s 311us/step - loss: 0.0350 - acc: 0.4378 - val_loss: 0.0197 - val_acc: 0.7067\n",
      "Epoch 10/10\n",
      "7997/7997 [==============================] - 3s 317us/step - loss: 0.0338 - acc: 0.4632 - val_loss: 0.0187 - val_acc: 0.7237\n",
      "Dropout: 0.72\n",
      "Train on 7997 samples, validate on 999 samples\n",
      "Epoch 1/10\n",
      "7997/7997 [==============================] - 7s 849us/step - loss: 0.0470 - acc: 0.1059 - val_loss: 0.0496 - val_acc: 0.1061\n",
      "Epoch 2/10\n",
      "7997/7997 [==============================] - 2s 304us/step - loss: 0.0455 - acc: 0.1398 - val_loss: 0.0421 - val_acc: 0.2462\n",
      "Epoch 3/10\n",
      "7997/7997 [==============================] - 2s 309us/step - loss: 0.0440 - acc: 0.1833 - val_loss: 0.0359 - val_acc: 0.4094\n",
      "Epoch 4/10\n",
      "7997/7997 [==============================] - 2s 306us/step - loss: 0.0424 - acc: 0.2286 - val_loss: 0.0309 - val_acc: 0.5075\n",
      "Epoch 5/10\n",
      "7997/7997 [==============================] - 2s 307us/step - loss: 0.0408 - acc: 0.2789 - val_loss: 0.0273 - val_acc: 0.5886\n",
      "Epoch 6/10\n",
      "7997/7997 [==============================] - 2s 308us/step - loss: 0.0393 - acc: 0.3284 - val_loss: 0.0252 - val_acc: 0.6226\n",
      "Epoch 7/10\n",
      "7997/7997 [==============================] - 2s 304us/step - loss: 0.0383 - acc: 0.3501 - val_loss: 0.0231 - val_acc: 0.6406\n",
      "Epoch 8/10\n",
      "7997/7997 [==============================] - 2s 306us/step - loss: 0.0371 - acc: 0.3845 - val_loss: 0.0215 - val_acc: 0.6747\n",
      "Epoch 9/10\n",
      "7997/7997 [==============================] - 2s 305us/step - loss: 0.0358 - acc: 0.4184 - val_loss: 0.0199 - val_acc: 0.7047\n",
      "Epoch 10/10\n",
      "7997/7997 [==============================] - 2s 312us/step - loss: 0.0347 - acc: 0.4487 - val_loss: 0.0191 - val_acc: 0.7147\n",
      "Dropout: 0.73\n",
      "Train on 7997 samples, validate on 999 samples\n",
      "Epoch 1/10\n",
      "7997/7997 [==============================] - 7s 856us/step - loss: 0.0470 - acc: 0.1078 - val_loss: 0.0506 - val_acc: 0.1071\n",
      "Epoch 2/10\n",
      "7997/7997 [==============================] - 2s 308us/step - loss: 0.0457 - acc: 0.1310 - val_loss: 0.0425 - val_acc: 0.2553\n",
      "Epoch 3/10\n",
      "7997/7997 [==============================] - 2s 307us/step - loss: 0.0442 - acc: 0.1758 - val_loss: 0.0368 - val_acc: 0.3954\n",
      "Epoch 4/10\n",
      "7997/7997 [==============================] - 2s 306us/step - loss: 0.0428 - acc: 0.2192 - val_loss: 0.0320 - val_acc: 0.4885\n",
      "Epoch 5/10\n",
      "7997/7997 [==============================] - 2s 308us/step - loss: 0.0414 - acc: 0.2590 - val_loss: 0.0288 - val_acc: 0.5455\n",
      "Epoch 6/10\n",
      "7997/7997 [==============================] - 2s 306us/step - loss: 0.0400 - acc: 0.3086 - val_loss: 0.0266 - val_acc: 0.5786\n",
      "Epoch 7/10\n",
      "7997/7997 [==============================] - 2s 307us/step - loss: 0.0390 - acc: 0.3326 - val_loss: 0.0250 - val_acc: 0.5946\n",
      "Epoch 8/10\n",
      "7997/7997 [==============================] - 2s 307us/step - loss: 0.0379 - acc: 0.3660 - val_loss: 0.0230 - val_acc: 0.6517\n",
      "Epoch 9/10\n",
      "7997/7997 [==============================] - 3s 347us/step - loss: 0.0366 - acc: 0.4042 - val_loss: 0.0212 - val_acc: 0.6837\n",
      "Epoch 10/10\n",
      "7997/7997 [==============================] - 3s 344us/step - loss: 0.0354 - acc: 0.4257 - val_loss: 0.0199 - val_acc: 0.7007\n",
      "Dropout: 0.74\n",
      "Train on 7997 samples, validate on 999 samples\n",
      "Epoch 1/10\n",
      "7997/7997 [==============================] - 7s 856us/step - loss: 0.0469 - acc: 0.1082 - val_loss: 0.0499 - val_acc: 0.1121\n",
      "Epoch 2/10\n",
      "7997/7997 [==============================] - 2s 303us/step - loss: 0.0456 - acc: 0.1322 - val_loss: 0.0429 - val_acc: 0.2432\n",
      "Epoch 3/10\n",
      "7997/7997 [==============================] - 2s 311us/step - loss: 0.0442 - acc: 0.1743 - val_loss: 0.0371 - val_acc: 0.3624\n",
      "Epoch 4/10\n",
      "7997/7997 [==============================] - 2s 308us/step - loss: 0.0430 - acc: 0.2108 - val_loss: 0.0328 - val_acc: 0.4755\n",
      "Epoch 5/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7997/7997 [==============================] - 2s 307us/step - loss: 0.0415 - acc: 0.2575 - val_loss: 0.0294 - val_acc: 0.5375\n",
      "Epoch 6/10\n",
      "7997/7997 [==============================] - 2s 302us/step - loss: 0.0402 - acc: 0.3050 - val_loss: 0.0269 - val_acc: 0.5856\n",
      "Epoch 7/10\n",
      "7997/7997 [==============================] - 2s 304us/step - loss: 0.0392 - acc: 0.3262 - val_loss: 0.0248 - val_acc: 0.6136\n",
      "Epoch 8/10\n",
      "7997/7997 [==============================] - 2s 302us/step - loss: 0.0382 - acc: 0.3563 - val_loss: 0.0236 - val_acc: 0.6276\n",
      "Epoch 9/10\n",
      "7997/7997 [==============================] - 2s 301us/step - loss: 0.0369 - acc: 0.3916 - val_loss: 0.0221 - val_acc: 0.6617\n",
      "Epoch 10/10\n",
      "7997/7997 [==============================] - 2s 301us/step - loss: 0.0359 - acc: 0.4173 - val_loss: 0.0214 - val_acc: 0.6697\n",
      "Dropout: 0.75\n",
      "Train on 7997 samples, validate on 999 samples\n",
      "Epoch 1/10\n",
      "7997/7997 [==============================] - 7s 856us/step - loss: 0.0468 - acc: 0.1083 - val_loss: 0.0507 - val_acc: 0.1071\n",
      "Epoch 2/10\n",
      "7997/7997 [==============================] - 2s 301us/step - loss: 0.0457 - acc: 0.1243 - val_loss: 0.0439 - val_acc: 0.2162\n",
      "Epoch 3/10\n",
      "7997/7997 [==============================] - 2s 297us/step - loss: 0.0444 - acc: 0.1649 - val_loss: 0.0379 - val_acc: 0.3624\n",
      "Epoch 4/10\n",
      "7997/7997 [==============================] - 2s 299us/step - loss: 0.0431 - acc: 0.2043 - val_loss: 0.0334 - val_acc: 0.4605\n",
      "Epoch 5/10\n",
      "7997/7997 [==============================] - 2s 301us/step - loss: 0.0418 - acc: 0.2438 - val_loss: 0.0296 - val_acc: 0.5395\n",
      "Epoch 6/10\n",
      "7997/7997 [==============================] - 2s 297us/step - loss: 0.0407 - acc: 0.2879 - val_loss: 0.0267 - val_acc: 0.5826\n",
      "Epoch 7/10\n",
      "7997/7997 [==============================] - 2s 299us/step - loss: 0.0397 - acc: 0.3091 - val_loss: 0.0244 - val_acc: 0.6296\n",
      "Epoch 8/10\n",
      "7997/7997 [==============================] - 2s 303us/step - loss: 0.0387 - acc: 0.3388 - val_loss: 0.0230 - val_acc: 0.6647\n",
      "Epoch 9/10\n",
      "7997/7997 [==============================] - 3s 317us/step - loss: 0.0375 - acc: 0.3724 - val_loss: 0.0218 - val_acc: 0.6727\n",
      "Epoch 10/10\n",
      "7997/7997 [==============================] - 3s 317us/step - loss: 0.0365 - acc: 0.4044 - val_loss: 0.0209 - val_acc: 0.6747\n",
      "Dropout: 0.76\n",
      "Train on 7997 samples, validate on 999 samples\n",
      "Epoch 1/10\n",
      "7997/7997 [==============================] - 7s 853us/step - loss: 0.0467 - acc: 0.1055 - val_loss: 0.0513 - val_acc: 0.1041\n",
      "Epoch 2/10\n",
      "7997/7997 [==============================] - 3s 336us/step - loss: 0.0458 - acc: 0.1229 - val_loss: 0.0444 - val_acc: 0.1942\n",
      "Epoch 3/10\n",
      "7997/7997 [==============================] - 3s 315us/step - loss: 0.0446 - acc: 0.1632 - val_loss: 0.0388 - val_acc: 0.3393\n",
      "Epoch 4/10\n",
      "7997/7997 [==============================] - 3s 315us/step - loss: 0.0435 - acc: 0.1948 - val_loss: 0.0340 - val_acc: 0.4464\n",
      "Epoch 5/10\n",
      "7997/7997 [==============================] - 3s 319us/step - loss: 0.0423 - acc: 0.2246 - val_loss: 0.0300 - val_acc: 0.5305\n",
      "Epoch 6/10\n",
      "7997/7997 [==============================] - 2s 308us/step - loss: 0.0411 - acc: 0.2752 - val_loss: 0.0270 - val_acc: 0.5886\n",
      "Epoch 7/10\n",
      "7997/7997 [==============================] - 2s 308us/step - loss: 0.0403 - acc: 0.2967 - val_loss: 0.0251 - val_acc: 0.6216\n",
      "Epoch 8/10\n",
      "7997/7997 [==============================] - 2s 307us/step - loss: 0.0393 - acc: 0.3262 - val_loss: 0.0231 - val_acc: 0.6396\n",
      "Epoch 9/10\n",
      "7997/7997 [==============================] - 2s 309us/step - loss: 0.0381 - acc: 0.3619 - val_loss: 0.0223 - val_acc: 0.6426\n",
      "Epoch 10/10\n",
      "7997/7997 [==============================] - 2s 310us/step - loss: 0.0371 - acc: 0.3883 - val_loss: 0.0211 - val_acc: 0.6787\n",
      "Dropout: 0.77\n",
      "Train on 7997 samples, validate on 999 samples\n",
      "Epoch 1/10\n",
      "7997/7997 [==============================] - 7s 868us/step - loss: 0.0466 - acc: 0.1069 - val_loss: 0.0514 - val_acc: 0.0921\n",
      "Epoch 2/10\n",
      "7997/7997 [==============================] - 2s 309us/step - loss: 0.0457 - acc: 0.1247 - val_loss: 0.0448 - val_acc: 0.1992\n",
      "Epoch 3/10\n",
      "7997/7997 [==============================] - 2s 305us/step - loss: 0.0446 - acc: 0.1618 - val_loss: 0.0389 - val_acc: 0.3373\n",
      "Epoch 4/10\n",
      "7997/7997 [==============================] - 3s 314us/step - loss: 0.0435 - acc: 0.1908 - val_loss: 0.0345 - val_acc: 0.4354\n",
      "Epoch 5/10\n",
      "7997/7997 [==============================] - 2s 310us/step - loss: 0.0426 - acc: 0.2205 - val_loss: 0.0311 - val_acc: 0.5135\n",
      "Epoch 6/10\n",
      "7997/7997 [==============================] - 2s 311us/step - loss: 0.0415 - acc: 0.2555 - val_loss: 0.0283 - val_acc: 0.5666\n",
      "Epoch 7/10\n",
      "7997/7997 [==============================] - 2s 306us/step - loss: 0.0406 - acc: 0.2899 - val_loss: 0.0263 - val_acc: 0.5836\n",
      "Epoch 8/10\n",
      "7997/7997 [==============================] - 2s 309us/step - loss: 0.0398 - acc: 0.3102 - val_loss: 0.0249 - val_acc: 0.6046\n",
      "Epoch 9/10\n",
      "7997/7997 [==============================] - 2s 309us/step - loss: 0.0387 - acc: 0.3479 - val_loss: 0.0232 - val_acc: 0.6296\n",
      "Epoch 10/10\n",
      "7997/7997 [==============================] - 2s 307us/step - loss: 0.0379 - acc: 0.3728 - val_loss: 0.0225 - val_acc: 0.6366\n",
      "Dropout: 0.78\n",
      "Train on 7997 samples, validate on 999 samples\n",
      "Epoch 1/10\n",
      "7997/7997 [==============================] - 7s 929us/step - loss: 0.0465 - acc: 0.1065 - val_loss: 0.0515 - val_acc: 0.0961\n",
      "Epoch 2/10\n",
      "7997/7997 [==============================] - 2s 307us/step - loss: 0.0457 - acc: 0.1208 - val_loss: 0.0459 - val_acc: 0.1882\n",
      "Epoch 3/10\n",
      "7997/7997 [==============================] - 2s 304us/step - loss: 0.0447 - acc: 0.1536 - val_loss: 0.0394 - val_acc: 0.3133\n",
      "Epoch 4/10\n",
      "7997/7997 [==============================] - 2s 304us/step - loss: 0.0437 - acc: 0.1809 - val_loss: 0.0347 - val_acc: 0.4294\n",
      "Epoch 5/10\n",
      "7997/7997 [==============================] - 2s 308us/step - loss: 0.0428 - acc: 0.2108 - val_loss: 0.0313 - val_acc: 0.5105\n",
      "Epoch 6/10\n",
      "7997/7997 [==============================] - 2s 304us/step - loss: 0.0418 - acc: 0.2473 - val_loss: 0.0282 - val_acc: 0.5596\n",
      "Epoch 7/10\n",
      "7997/7997 [==============================] - 2s 305us/step - loss: 0.0410 - acc: 0.2749 - val_loss: 0.0261 - val_acc: 0.6036\n",
      "Epoch 8/10\n",
      "7997/7997 [==============================] - 2s 306us/step - loss: 0.0403 - acc: 0.2965 - val_loss: 0.0250 - val_acc: 0.6296\n",
      "Epoch 9/10\n",
      "7997/7997 [==============================] - 3s 353us/step - loss: 0.0391 - acc: 0.3338 - val_loss: 0.0235 - val_acc: 0.6426\n",
      "Epoch 10/10\n",
      "7997/7997 [==============================] - 3s 329us/step - loss: 0.0384 - acc: 0.3571 - val_loss: 0.0230 - val_acc: 0.6396\n",
      "Dropout: 0.79\n",
      "Train on 7997 samples, validate on 999 samples\n",
      "Epoch 1/10\n",
      "7997/7997 [==============================] - 7s 877us/step - loss: 0.0464 - acc: 0.1073 - val_loss: 0.0522 - val_acc: 0.0951\n",
      "Epoch 2/10\n",
      "7997/7997 [==============================] - 2s 308us/step - loss: 0.0457 - acc: 0.1175 - val_loss: 0.0466 - val_acc: 0.1632\n",
      "Epoch 3/10\n",
      "7997/7997 [==============================] - 2s 304us/step - loss: 0.0448 - acc: 0.1487 - val_loss: 0.0418 - val_acc: 0.2492\n",
      "Epoch 4/10\n",
      "7997/7997 [==============================] - 2s 310us/step - loss: 0.0439 - acc: 0.1722 - val_loss: 0.0366 - val_acc: 0.3914\n",
      "Epoch 5/10\n",
      "7997/7997 [==============================] - 3s 366us/step - loss: 0.0430 - acc: 0.2036 - val_loss: 0.0330 - val_acc: 0.4845\n",
      "Epoch 6/10\n",
      "7997/7997 [==============================] - 2s 312us/step - loss: 0.0422 - acc: 0.2331 - val_loss: 0.0304 - val_acc: 0.5305\n",
      "Epoch 7/10\n",
      "7997/7997 [==============================] - 3s 319us/step - loss: 0.0416 - acc: 0.2592 - val_loss: 0.0275 - val_acc: 0.5756\n",
      "Epoch 8/10\n",
      "7997/7997 [==============================] - 3s 346us/step - loss: 0.0409 - acc: 0.2784 - val_loss: 0.0258 - val_acc: 0.5946\n",
      "Epoch 9/10\n",
      "7997/7997 [==============================] - 3s 356us/step - loss: 0.0397 - acc: 0.3137 - val_loss: 0.0240 - val_acc: 0.6366\n",
      "Epoch 10/10\n",
      "7997/7997 [==============================] - 3s 343us/step - loss: 0.0390 - acc: 0.3374 - val_loss: 0.0234 - val_acc: 0.6336\n",
      "Dropout: 0.8\n",
      "Train on 7997 samples, validate on 999 samples\n",
      "Epoch 1/10\n",
      "7997/7997 [==============================] - 7s 928us/step - loss: 0.0463 - acc: 0.1040 - val_loss: 0.0534 - val_acc: 0.0911\n",
      "Epoch 2/10\n",
      "7997/7997 [==============================] - 3s 320us/step - loss: 0.0457 - acc: 0.1138 - val_loss: 0.0479 - val_acc: 0.1562\n",
      "Epoch 3/10\n",
      "7997/7997 [==============================] - 3s 317us/step - loss: 0.0448 - acc: 0.1439 - val_loss: 0.0436 - val_acc: 0.2372\n",
      "Epoch 4/10\n",
      "7997/7997 [==============================] - 2s 311us/step - loss: 0.0441 - acc: 0.1703 - val_loss: 0.0392 - val_acc: 0.3303\n",
      "Epoch 5/10\n",
      "7997/7997 [==============================] - 2s 301us/step - loss: 0.0433 - acc: 0.1923 - val_loss: 0.0349 - val_acc: 0.4174\n",
      "Epoch 6/10\n",
      "7997/7997 [==============================] - 2s 300us/step - loss: 0.0425 - acc: 0.2222 - val_loss: 0.0311 - val_acc: 0.5055\n",
      "Epoch 7/10\n",
      "7997/7997 [==============================] - 2s 301us/step - loss: 0.0419 - acc: 0.2460 - val_loss: 0.0281 - val_acc: 0.5726\n",
      "Epoch 8/10\n",
      "7997/7997 [==============================] - 2s 301us/step - loss: 0.0413 - acc: 0.2621 - val_loss: 0.0260 - val_acc: 0.6196\n",
      "Epoch 9/10\n",
      "7997/7997 [==============================] - 3s 317us/step - loss: 0.0404 - acc: 0.2886 - val_loss: 0.0243 - val_acc: 0.6266\n",
      "Epoch 10/10\n",
      "7997/7997 [==============================] - 3s 324us/step - loss: 0.0396 - acc: 0.3189 - val_loss: 0.0228 - val_acc: 0.6577\n",
      "Dropout: 0.81\n",
      "Train on 7997 samples, validate on 999 samples\n",
      "Epoch 1/10\n",
      "7997/7997 [==============================] - 7s 874us/step - loss: 0.0462 - acc: 0.1019 - val_loss: 0.0527 - val_acc: 0.0861\n",
      "Epoch 2/10\n",
      "7997/7997 [==============================] - 2s 310us/step - loss: 0.0456 - acc: 0.1148 - val_loss: 0.0482 - val_acc: 0.1401\n",
      "Epoch 3/10\n",
      "7997/7997 [==============================] - 2s 309us/step - loss: 0.0449 - acc: 0.1409 - val_loss: 0.0436 - val_acc: 0.2162\n",
      "Epoch 4/10\n",
      "7997/7997 [==============================] - 3s 316us/step - loss: 0.0442 - acc: 0.1618 - val_loss: 0.0384 - val_acc: 0.3303\n",
      "Epoch 5/10\n",
      "7997/7997 [==============================] - 2s 310us/step - loss: 0.0435 - acc: 0.1841 - val_loss: 0.0352 - val_acc: 0.4034\n",
      "Epoch 6/10\n",
      "7997/7997 [==============================] - 2s 309us/step - loss: 0.0428 - acc: 0.2153 - val_loss: 0.0318 - val_acc: 0.4945\n",
      "Epoch 7/10\n",
      "7997/7997 [==============================] - 2s 312us/step - loss: 0.0422 - acc: 0.2312 - val_loss: 0.0291 - val_acc: 0.5465\n",
      "Epoch 8/10\n",
      "7997/7997 [==============================] - 2s 309us/step - loss: 0.0416 - acc: 0.2560 - val_loss: 0.0273 - val_acc: 0.5776\n",
      "Epoch 9/10\n",
      "7997/7997 [==============================] - 3s 319us/step - loss: 0.0407 - acc: 0.2821 - val_loss: 0.0255 - val_acc: 0.6166\n",
      "Epoch 10/10\n",
      "7997/7997 [==============================] - 2s 311us/step - loss: 0.0401 - acc: 0.3012 - val_loss: 0.0244 - val_acc: 0.6216\n",
      "Dropout: 0.82\n",
      "Train on 7997 samples, validate on 999 samples\n",
      "Epoch 1/10\n",
      "7997/7997 [==============================] - 7s 894us/step - loss: 0.0461 - acc: 0.1052 - val_loss: 0.0525 - val_acc: 0.0921\n",
      "Epoch 2/10\n",
      "7997/7997 [==============================] - 2s 312us/step - loss: 0.0455 - acc: 0.1178 - val_loss: 0.0473 - val_acc: 0.1732\n",
      "Epoch 3/10\n",
      "7997/7997 [==============================] - 3s 326us/step - loss: 0.0448 - acc: 0.1367 - val_loss: 0.0432 - val_acc: 0.2503\n",
      "Epoch 4/10\n",
      "7997/7997 [==============================] - 3s 313us/step - loss: 0.0443 - acc: 0.1611 - val_loss: 0.0388 - val_acc: 0.3483\n",
      "Epoch 5/10\n",
      "7997/7997 [==============================] - 2s 308us/step - loss: 0.0438 - acc: 0.1763 - val_loss: 0.0353 - val_acc: 0.4104\n",
      "Epoch 6/10\n",
      "7997/7997 [==============================] - 2s 311us/step - loss: 0.0431 - acc: 0.1993 - val_loss: 0.0324 - val_acc: 0.4765\n",
      "Epoch 7/10\n",
      "7997/7997 [==============================] - 3s 322us/step - loss: 0.0426 - acc: 0.2168 - val_loss: 0.0296 - val_acc: 0.5345\n",
      "Epoch 8/10\n",
      "7997/7997 [==============================] - 2s 312us/step - loss: 0.0421 - acc: 0.2386 - val_loss: 0.0275 - val_acc: 0.5576\n",
      "Epoch 9/10\n",
      "7997/7997 [==============================] - 3s 321us/step - loss: 0.0413 - acc: 0.2695 - val_loss: 0.0256 - val_acc: 0.6096\n",
      "Epoch 10/10\n",
      "7997/7997 [==============================] - 3s 314us/step - loss: 0.0407 - acc: 0.2882 - val_loss: 0.0246 - val_acc: 0.6216\n",
      "Dropout: 0.83\n",
      "Train on 7997 samples, validate on 999 samples\n",
      "Epoch 1/10\n",
      "7997/7997 [==============================] - 7s 906us/step - loss: 0.0460 - acc: 0.1070 - val_loss: 0.0541 - val_acc: 0.0741\n",
      "Epoch 2/10\n",
      "7997/7997 [==============================] - 3s 318us/step - loss: 0.0456 - acc: 0.1062 - val_loss: 0.0492 - val_acc: 0.1191\n",
      "Epoch 3/10\n",
      "7997/7997 [==============================] - 2s 308us/step - loss: 0.0450 - acc: 0.1332 - val_loss: 0.0450 - val_acc: 0.1902\n",
      "Epoch 4/10\n",
      "7997/7997 [==============================] - 2s 308us/step - loss: 0.0445 - acc: 0.1486 - val_loss: 0.0403 - val_acc: 0.2873\n",
      "Epoch 5/10\n",
      "7997/7997 [==============================] - 2s 307us/step - loss: 0.0440 - acc: 0.1642 - val_loss: 0.0367 - val_acc: 0.3804\n",
      "Epoch 6/10\n",
      "7997/7997 [==============================] - 3s 318us/step - loss: 0.0434 - acc: 0.1883 - val_loss: 0.0331 - val_acc: 0.4535\n",
      "Epoch 7/10\n",
      "7997/7997 [==============================] - 2s 309us/step - loss: 0.0429 - acc: 0.2027 - val_loss: 0.0303 - val_acc: 0.5275\n",
      "Epoch 8/10\n",
      "7997/7997 [==============================] - 3s 327us/step - loss: 0.0425 - acc: 0.2232 - val_loss: 0.0284 - val_acc: 0.5586\n",
      "Epoch 9/10\n",
      "7997/7997 [==============================] - 3s 322us/step - loss: 0.0418 - acc: 0.2493 - val_loss: 0.0270 - val_acc: 0.5886\n",
      "Epoch 10/10\n",
      "7997/7997 [==============================] - 3s 319us/step - loss: 0.0412 - acc: 0.2623 - val_loss: 0.0259 - val_acc: 0.6076\n",
      "Dropout: 0.84\n",
      "Train on 7997 samples, validate on 999 samples\n",
      "Epoch 1/10\n",
      "7997/7997 [==============================] - 7s 932us/step - loss: 0.0459 - acc: 0.1045 - val_loss: 0.0546 - val_acc: 0.0721\n",
      "Epoch 2/10\n",
      "7997/7997 [==============================] - 2s 307us/step - loss: 0.0455 - acc: 0.1114 - val_loss: 0.0500 - val_acc: 0.1191\n",
      "Epoch 3/10\n",
      "7997/7997 [==============================] - 3s 338us/step - loss: 0.0449 - acc: 0.1344 - val_loss: 0.0455 - val_acc: 0.1932\n",
      "Epoch 4/10\n",
      "7997/7997 [==============================] - 3s 321us/step - loss: 0.0445 - acc: 0.1413 - val_loss: 0.0410 - val_acc: 0.2783\n",
      "Epoch 5/10\n",
      "7997/7997 [==============================] - 3s 320us/step - loss: 0.0441 - acc: 0.1576 - val_loss: 0.0374 - val_acc: 0.3674\n",
      "Epoch 6/10\n",
      "7997/7997 [==============================] - 3s 315us/step - loss: 0.0435 - acc: 0.1824 - val_loss: 0.0340 - val_acc: 0.4555\n",
      "Epoch 7/10\n",
      "7997/7997 [==============================] - 2s 310us/step - loss: 0.0432 - acc: 0.1876 - val_loss: 0.0312 - val_acc: 0.5095\n",
      "Epoch 8/10\n",
      "7997/7997 [==============================] - 2s 311us/step - loss: 0.0428 - acc: 0.2028 - val_loss: 0.0286 - val_acc: 0.5746\n",
      "Epoch 9/10\n",
      "7997/7997 [==============================] - 3s 314us/step - loss: 0.0421 - acc: 0.2288 - val_loss: 0.0268 - val_acc: 0.5946\n",
      "Epoch 10/10\n",
      "7997/7997 [==============================] - 2s 310us/step - loss: 0.0416 - acc: 0.2557 - val_loss: 0.0260 - val_acc: 0.5936\n",
      "Dropout: 0.85\n",
      "Train on 7997 samples, validate on 999 samples\n",
      "Epoch 1/10\n",
      "7997/7997 [==============================] - 7s 910us/step - loss: 0.0458 - acc: 0.1009 - val_loss: 0.0544 - val_acc: 0.0751\n",
      "Epoch 2/10\n",
      "7997/7997 [==============================] - 3s 318us/step - loss: 0.0455 - acc: 0.1059 - val_loss: 0.0511 - val_acc: 0.1191\n",
      "Epoch 3/10\n",
      "7997/7997 [==============================] - 2s 312us/step - loss: 0.0450 - acc: 0.1240 - val_loss: 0.0466 - val_acc: 0.1682\n",
      "Epoch 4/10\n",
      "7997/7997 [==============================] - 3s 316us/step - loss: 0.0447 - acc: 0.1308 - val_loss: 0.0431 - val_acc: 0.2262\n",
      "Epoch 5/10\n",
      "7997/7997 [==============================] - 2s 311us/step - loss: 0.0443 - acc: 0.1482 - val_loss: 0.0388 - val_acc: 0.3283\n",
      "Epoch 6/10\n",
      "7997/7997 [==============================] - 3s 313us/step - loss: 0.0438 - acc: 0.1669 - val_loss: 0.0360 - val_acc: 0.3914\n",
      "Epoch 7/10\n",
      "7997/7997 [==============================] - 3s 315us/step - loss: 0.0435 - acc: 0.1796 - val_loss: 0.0336 - val_acc: 0.4484\n",
      "Epoch 8/10\n",
      "7997/7997 [==============================] - 3s 318us/step - loss: 0.0432 - acc: 0.1961 - val_loss: 0.0306 - val_acc: 0.5305\n",
      "Epoch 9/10\n",
      "7997/7997 [==============================] - 3s 314us/step - loss: 0.0426 - acc: 0.2126 - val_loss: 0.0287 - val_acc: 0.5526\n",
      "Epoch 10/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7997/7997 [==============================] - 2s 311us/step - loss: 0.0421 - acc: 0.2273 - val_loss: 0.0274 - val_acc: 0.5706\n",
      "Dropout: 0.86\n",
      "Train on 7997 samples, validate on 999 samples\n",
      "Epoch 1/10\n",
      "7997/7997 [==============================] - 7s 915us/step - loss: 0.0457 - acc: 0.1029 - val_loss: 0.0547 - val_acc: 0.0721\n",
      "Epoch 2/10\n",
      "7997/7997 [==============================] - 2s 312us/step - loss: 0.0454 - acc: 0.1070 - val_loss: 0.0509 - val_acc: 0.1141\n",
      "Epoch 3/10\n",
      "7997/7997 [==============================] - 2s 308us/step - loss: 0.0449 - acc: 0.1240 - val_loss: 0.0466 - val_acc: 0.1742\n",
      "Epoch 4/10\n",
      "7997/7997 [==============================] - 2s 311us/step - loss: 0.0446 - acc: 0.1328 - val_loss: 0.0444 - val_acc: 0.2232\n",
      "Epoch 5/10\n",
      "7997/7997 [==============================] - 2s 310us/step - loss: 0.0444 - acc: 0.1439 - val_loss: 0.0415 - val_acc: 0.2743\n",
      "Epoch 6/10\n",
      "7997/7997 [==============================] - 2s 307us/step - loss: 0.0439 - acc: 0.1617 - val_loss: 0.0382 - val_acc: 0.3514\n",
      "Epoch 7/10\n",
      "7997/7997 [==============================] - 2s 309us/step - loss: 0.0437 - acc: 0.1709 - val_loss: 0.0354 - val_acc: 0.4184\n",
      "Epoch 8/10\n",
      "7997/7997 [==============================] - 2s 307us/step - loss: 0.0434 - acc: 0.1863 - val_loss: 0.0323 - val_acc: 0.4815\n",
      "Epoch 9/10\n",
      "7997/7997 [==============================] - 3s 316us/step - loss: 0.0429 - acc: 0.2008 - val_loss: 0.0300 - val_acc: 0.5205\n",
      "Epoch 10/10\n",
      "7997/7997 [==============================] - 3s 317us/step - loss: 0.0425 - acc: 0.2145 - val_loss: 0.0283 - val_acc: 0.5606\n",
      "Dropout: 0.87\n",
      "Train on 7997 samples, validate on 999 samples\n",
      "Epoch 1/10\n",
      "7997/7997 [==============================] - 8s 973us/step - loss: 0.0456 - acc: 0.1019 - val_loss: 0.0545 - val_acc: 0.0831\n",
      "Epoch 2/10\n",
      "7997/7997 [==============================] - 3s 320us/step - loss: 0.0453 - acc: 0.1087 - val_loss: 0.0517 - val_acc: 0.1141\n",
      "Epoch 3/10\n",
      "7997/7997 [==============================] - 3s 317us/step - loss: 0.0450 - acc: 0.1185 - val_loss: 0.0487 - val_acc: 0.1461\n",
      "Epoch 4/10\n",
      "7997/7997 [==============================] - 2s 313us/step - loss: 0.0447 - acc: 0.1280 - val_loss: 0.0460 - val_acc: 0.1902\n",
      "Epoch 5/10\n",
      "7997/7997 [==============================] - 2s 311us/step - loss: 0.0444 - acc: 0.1393 - val_loss: 0.0423 - val_acc: 0.2613\n",
      "Epoch 6/10\n",
      "7997/7997 [==============================] - 2s 309us/step - loss: 0.0441 - acc: 0.1509 - val_loss: 0.0391 - val_acc: 0.3213\n",
      "Epoch 7/10\n",
      "7997/7997 [==============================] - 2s 311us/step - loss: 0.0439 - acc: 0.1536 - val_loss: 0.0370 - val_acc: 0.3874\n",
      "Epoch 8/10\n",
      "7997/7997 [==============================] - 3s 313us/step - loss: 0.0437 - acc: 0.1678 - val_loss: 0.0338 - val_acc: 0.4494\n",
      "Epoch 9/10\n",
      "7997/7997 [==============================] - 3s 317us/step - loss: 0.0432 - acc: 0.1833 - val_loss: 0.0320 - val_acc: 0.4895\n",
      "Epoch 10/10\n",
      "7997/7997 [==============================] - 2s 312us/step - loss: 0.0429 - acc: 0.1964 - val_loss: 0.0307 - val_acc: 0.5135\n",
      "Dropout: 0.88\n",
      "Train on 7997 samples, validate on 999 samples\n",
      "Epoch 1/10\n",
      "7997/7997 [==============================] - 7s 926us/step - loss: 0.0455 - acc: 0.1054 - val_loss: 0.0551 - val_acc: 0.0691\n",
      "Epoch 2/10\n",
      "7997/7997 [==============================] - 2s 306us/step - loss: 0.0452 - acc: 0.1073 - val_loss: 0.0527 - val_acc: 0.0971\n",
      "Epoch 3/10\n",
      "7997/7997 [==============================] - 2s 307us/step - loss: 0.0450 - acc: 0.1147 - val_loss: 0.0502 - val_acc: 0.1261\n",
      "Epoch 4/10\n",
      "7997/7997 [==============================] - 2s 308us/step - loss: 0.0448 - acc: 0.1265 - val_loss: 0.0469 - val_acc: 0.1772\n",
      "Epoch 5/10\n",
      "7997/7997 [==============================] - 2s 310us/step - loss: 0.0445 - acc: 0.1329 - val_loss: 0.0437 - val_acc: 0.2182\n",
      "Epoch 6/10\n",
      "7997/7997 [==============================] - 2s 308us/step - loss: 0.0442 - acc: 0.1419 - val_loss: 0.0410 - val_acc: 0.2873\n",
      "Epoch 7/10\n",
      "7997/7997 [==============================] - 3s 319us/step - loss: 0.0440 - acc: 0.1548 - val_loss: 0.0387 - val_acc: 0.3353\n",
      "Epoch 8/10\n",
      "7997/7997 [==============================] - 3s 319us/step - loss: 0.0438 - acc: 0.1599 - val_loss: 0.0361 - val_acc: 0.3934\n",
      "Epoch 9/10\n",
      "7997/7997 [==============================] - 2s 312us/step - loss: 0.0434 - acc: 0.1732 - val_loss: 0.0333 - val_acc: 0.4474\n",
      "Epoch 10/10\n",
      "7997/7997 [==============================] - 2s 308us/step - loss: 0.0432 - acc: 0.1841 - val_loss: 0.0313 - val_acc: 0.5035\n",
      "Dropout: 0.89\n",
      "Train on 7997 samples, validate on 999 samples\n",
      "Epoch 1/10\n",
      "7997/7997 [==============================] - 7s 923us/step - loss: 0.0453 - acc: 0.1047 - val_loss: 0.0550 - val_acc: 0.0681\n",
      "Epoch 2/10\n",
      "7997/7997 [==============================] - 2s 308us/step - loss: 0.0452 - acc: 0.1060 - val_loss: 0.0525 - val_acc: 0.0901\n",
      "Epoch 3/10\n",
      "7997/7997 [==============================] - 3s 313us/step - loss: 0.0449 - acc: 0.1120 - val_loss: 0.0497 - val_acc: 0.1211\n",
      "Epoch 4/10\n",
      "7997/7997 [==============================] - 3s 325us/step - loss: 0.0448 - acc: 0.1232 - val_loss: 0.0477 - val_acc: 0.1692\n",
      "Epoch 5/10\n",
      "7997/7997 [==============================] - 2s 312us/step - loss: 0.0446 - acc: 0.1287 - val_loss: 0.0455 - val_acc: 0.2102\n",
      "Epoch 6/10\n",
      "7997/7997 [==============================] - 3s 313us/step - loss: 0.0443 - acc: 0.1391 - val_loss: 0.0430 - val_acc: 0.2543\n",
      "Epoch 7/10\n",
      "7997/7997 [==============================] - 2s 308us/step - loss: 0.0442 - acc: 0.1426 - val_loss: 0.0406 - val_acc: 0.2943\n",
      "Epoch 8/10\n",
      "7997/7997 [==============================] - 2s 308us/step - loss: 0.0440 - acc: 0.1533 - val_loss: 0.0379 - val_acc: 0.3453\n",
      "Epoch 9/10\n",
      "7997/7997 [==============================] - 3s 313us/step - loss: 0.0437 - acc: 0.1641 - val_loss: 0.0353 - val_acc: 0.4034\n",
      "Epoch 10/10\n",
      "7997/7997 [==============================] - 2s 311us/step - loss: 0.0434 - acc: 0.1732 - val_loss: 0.0333 - val_acc: 0.4645\n",
      "Dropout: 0.9\n",
      "Train on 7997 samples, validate on 999 samples\n",
      "Epoch 1/10\n",
      "7997/7997 [==============================] - 7s 927us/step - loss: 0.0452 - acc: 0.1038 - val_loss: 0.0547 - val_acc: 0.0591\n",
      "Epoch 2/10\n",
      "7997/7997 [==============================] - 3s 313us/step - loss: 0.0451 - acc: 0.1063 - val_loss: 0.0527 - val_acc: 0.0821\n",
      "Epoch 3/10\n",
      "7997/7997 [==============================] - 3s 316us/step - loss: 0.0448 - acc: 0.1203 - val_loss: 0.0510 - val_acc: 0.1061\n",
      "Epoch 4/10\n",
      "7997/7997 [==============================] - 2s 311us/step - loss: 0.0447 - acc: 0.1210 - val_loss: 0.0486 - val_acc: 0.1592\n",
      "Epoch 5/10\n",
      "7997/7997 [==============================] - 3s 313us/step - loss: 0.0446 - acc: 0.1183 - val_loss: 0.0468 - val_acc: 0.1892\n",
      "Epoch 6/10\n",
      "7997/7997 [==============================] - 2s 310us/step - loss: 0.0443 - acc: 0.1357 - val_loss: 0.0445 - val_acc: 0.2132\n",
      "Epoch 7/10\n",
      "7997/7997 [==============================] - 2s 311us/step - loss: 0.0443 - acc: 0.1372 - val_loss: 0.0423 - val_acc: 0.2492\n",
      "Epoch 8/10\n",
      "7997/7997 [==============================] - 2s 311us/step - loss: 0.0442 - acc: 0.1384 - val_loss: 0.0402 - val_acc: 0.2953\n",
      "Epoch 9/10\n",
      "7997/7997 [==============================] - 2s 311us/step - loss: 0.0438 - acc: 0.1573 - val_loss: 0.0372 - val_acc: 0.3524\n",
      "Epoch 10/10\n",
      "7997/7997 [==============================] - 2s 311us/step - loss: 0.0437 - acc: 0.1618 - val_loss: 0.0355 - val_acc: 0.4034\n",
      "Dropout: 0.91\n",
      "Train on 7997 samples, validate on 999 samples\n",
      "Epoch 1/10\n",
      "7997/7997 [==============================] - 8s 942us/step - loss: 0.0451 - acc: 0.1010 - val_loss: 0.0554 - val_acc: 0.0701\n",
      "Epoch 2/10\n",
      "7997/7997 [==============================] - 2s 312us/step - loss: 0.0449 - acc: 0.1047 - val_loss: 0.0536 - val_acc: 0.0931\n",
      "Epoch 3/10\n",
      "7997/7997 [==============================] - 2s 308us/step - loss: 0.0448 - acc: 0.1097 - val_loss: 0.0513 - val_acc: 0.1141\n",
      "Epoch 4/10\n",
      "7997/7997 [==============================] - 2s 310us/step - loss: 0.0447 - acc: 0.1184 - val_loss: 0.0493 - val_acc: 0.1281\n",
      "Epoch 5/10\n",
      "7997/7997 [==============================] - 2s 310us/step - loss: 0.0446 - acc: 0.1204 - val_loss: 0.0473 - val_acc: 0.1602\n",
      "Epoch 6/10\n",
      "7997/7997 [==============================] - 3s 315us/step - loss: 0.0444 - acc: 0.1268 - val_loss: 0.0456 - val_acc: 0.2052\n",
      "Epoch 7/10\n",
      "7997/7997 [==============================] - 2s 307us/step - loss: 0.0443 - acc: 0.1275 - val_loss: 0.0429 - val_acc: 0.2392\n",
      "Epoch 8/10\n",
      "7997/7997 [==============================] - 2s 306us/step - loss: 0.0443 - acc: 0.1289 - val_loss: 0.0408 - val_acc: 0.2743\n",
      "Epoch 9/10\n",
      "7997/7997 [==============================] - 2s 308us/step - loss: 0.0440 - acc: 0.1436 - val_loss: 0.0391 - val_acc: 0.3173\n",
      "Epoch 10/10\n",
      "7997/7997 [==============================] - 2s 310us/step - loss: 0.0437 - acc: 0.1543 - val_loss: 0.0366 - val_acc: 0.3784\n",
      "Dropout: 0.92\n",
      "Train on 7997 samples, validate on 999 samples\n",
      "Epoch 1/10\n",
      "7997/7997 [==============================] - 8s 947us/step - loss: 0.0450 - acc: 0.1022 - val_loss: 0.0558 - val_acc: 0.0661\n",
      "Epoch 2/10\n",
      "7997/7997 [==============================] - 2s 312us/step - loss: 0.0448 - acc: 0.1060 - val_loss: 0.0549 - val_acc: 0.0811\n",
      "Epoch 3/10\n",
      "7997/7997 [==============================] - 2s 309us/step - loss: 0.0448 - acc: 0.1097 - val_loss: 0.0530 - val_acc: 0.0931\n",
      "Epoch 4/10\n",
      "7997/7997 [==============================] - 2s 306us/step - loss: 0.0446 - acc: 0.1113 - val_loss: 0.0510 - val_acc: 0.1181\n",
      "Epoch 5/10\n",
      "7997/7997 [==============================] - 2s 311us/step - loss: 0.0446 - acc: 0.1173 - val_loss: 0.0495 - val_acc: 0.1441\n",
      "Epoch 6/10\n",
      "7997/7997 [==============================] - 2s 308us/step - loss: 0.0444 - acc: 0.1268 - val_loss: 0.0476 - val_acc: 0.1782\n",
      "Epoch 7/10\n",
      "7997/7997 [==============================] - 2s 309us/step - loss: 0.0444 - acc: 0.1214 - val_loss: 0.0457 - val_acc: 0.2132\n",
      "Epoch 8/10\n",
      "7997/7997 [==============================] - 2s 307us/step - loss: 0.0444 - acc: 0.1212 - val_loss: 0.0441 - val_acc: 0.2402\n",
      "Epoch 9/10\n",
      "7997/7997 [==============================] - 2s 305us/step - loss: 0.0440 - acc: 0.1427 - val_loss: 0.0423 - val_acc: 0.2803\n",
      "Epoch 10/10\n",
      "7997/7997 [==============================] - 2s 312us/step - loss: 0.0439 - acc: 0.1487 - val_loss: 0.0397 - val_acc: 0.3353\n",
      "Dropout: 0.93\n",
      "Train on 7997 samples, validate on 999 samples\n",
      "Epoch 1/10\n",
      "7997/7997 [==============================] - 7s 936us/step - loss: 0.0448 - acc: 0.1085 - val_loss: 0.0555 - val_acc: 0.0701\n",
      "Epoch 2/10\n",
      "7997/7997 [==============================] - 2s 307us/step - loss: 0.0447 - acc: 0.1063 - val_loss: 0.0546 - val_acc: 0.0811\n",
      "Epoch 3/10\n",
      "7997/7997 [==============================] - 2s 305us/step - loss: 0.0447 - acc: 0.1087 - val_loss: 0.0529 - val_acc: 0.0981\n",
      "Epoch 4/10\n",
      "7997/7997 [==============================] - 2s 306us/step - loss: 0.0446 - acc: 0.1095 - val_loss: 0.0525 - val_acc: 0.1111\n",
      "Epoch 5/10\n",
      "7997/7997 [==============================] - 2s 302us/step - loss: 0.0445 - acc: 0.1117 - val_loss: 0.0505 - val_acc: 0.1321\n",
      "Epoch 6/10\n",
      "7997/7997 [==============================] - 2s 304us/step - loss: 0.0445 - acc: 0.1169 - val_loss: 0.0487 - val_acc: 0.1512\n",
      "Epoch 7/10\n",
      "7997/7997 [==============================] - 3s 318us/step - loss: 0.0444 - acc: 0.1148 - val_loss: 0.0464 - val_acc: 0.1962\n",
      "Epoch 8/10\n",
      "7997/7997 [==============================] - 2s 308us/step - loss: 0.0444 - acc: 0.1182 - val_loss: 0.0455 - val_acc: 0.2072\n",
      "Epoch 9/10\n",
      "7997/7997 [==============================] - 2s 309us/step - loss: 0.0442 - acc: 0.1329 - val_loss: 0.0434 - val_acc: 0.2372\n",
      "Epoch 10/10\n",
      "7997/7997 [==============================] - 2s 312us/step - loss: 0.0441 - acc: 0.1357 - val_loss: 0.0413 - val_acc: 0.2803\n",
      "Dropout: 0.94\n",
      "Train on 7997 samples, validate on 999 samples\n",
      "Epoch 1/10\n",
      "7997/7997 [==============================] - 8s 1ms/step - loss: 0.0447 - acc: 0.1055 - val_loss: 0.0573 - val_acc: 0.0641\n",
      "Epoch 2/10\n",
      "7997/7997 [==============================] - 3s 338us/step - loss: 0.0447 - acc: 0.1030 - val_loss: 0.0556 - val_acc: 0.0681\n",
      "Epoch 3/10\n",
      "7997/7997 [==============================] - 3s 345us/step - loss: 0.0447 - acc: 0.1052 - val_loss: 0.0548 - val_acc: 0.0691\n",
      "Epoch 4/10\n",
      "7997/7997 [==============================] - 3s 320us/step - loss: 0.0446 - acc: 0.1089 - val_loss: 0.0541 - val_acc: 0.0911\n",
      "Epoch 5/10\n",
      "7997/7997 [==============================] - 3s 319us/step - loss: 0.0445 - acc: 0.1107 - val_loss: 0.0534 - val_acc: 0.1191\n",
      "Epoch 6/10\n",
      "7997/7997 [==============================] - 3s 315us/step - loss: 0.0445 - acc: 0.1108 - val_loss: 0.0518 - val_acc: 0.1401\n",
      "Epoch 7/10\n",
      "7997/7997 [==============================] - 2s 308us/step - loss: 0.0444 - acc: 0.1112 - val_loss: 0.0504 - val_acc: 0.1542\n",
      "Epoch 8/10\n",
      "7997/7997 [==============================] - 2s 310us/step - loss: 0.0444 - acc: 0.1105 - val_loss: 0.0497 - val_acc: 0.1702\n",
      "Epoch 9/10\n",
      "7997/7997 [==============================] - 2s 308us/step - loss: 0.0442 - acc: 0.1199 - val_loss: 0.0482 - val_acc: 0.1912\n",
      "Epoch 10/10\n",
      "7997/7997 [==============================] - 3s 314us/step - loss: 0.0442 - acc: 0.1282 - val_loss: 0.0461 - val_acc: 0.2152\n"
     ]
    }
   ],
   "source": [
    "max_drop = 0\n",
    "max_val_acc = 0\n",
    "for i in np.arange(0.0,0.95,0.01):\n",
    "    np.random.seed(13)\n",
    "    print(\"Dropout: \" + str(i))\n",
    "    model_5 = mod_v5((28,28), i)\n",
    "    model_5.compile(optimizer=SGD(lr=0.02), loss='mean_squared_logarithmic_error', metrics=['accuracy'])\n",
    "    ht_5 = model_5.fit(train_X.reshape(train_X.shape[0], 28, 28), train_Y, validation_data=(val_X.reshape(val_X.shape[0], 28, 28), val_Y), epochs = 10, batch_size = 100)\n",
    "    if ht_5.history['val_acc'][-1] > max_val_acc:\n",
    "        max_val_acc = ht_5.history['val_acc'][-1]\n",
    "        max_drop = i\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAX: drop=0.14, acc=0.931931935273\n"
     ]
    }
   ],
   "source": [
    "print(\"MAX: drop=\" + str(max_drop) + \", acc=\" + str(max_val_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 451,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7997 samples, validate on 999 samples\n",
      "Epoch 1/30\n",
      "7997/7997 [==============================] - 9s 1ms/step - loss: 0.0448 - acc: 0.2372 - val_loss: 0.0387 - val_acc: 0.3504\n",
      "Epoch 2/30\n",
      "7997/7997 [==============================] - 3s 316us/step - loss: 0.0334 - acc: 0.4614 - val_loss: 0.0262 - val_acc: 0.6066\n",
      "Epoch 3/30\n",
      "7997/7997 [==============================] - 3s 318us/step - loss: 0.0256 - acc: 0.6180 - val_loss: 0.0189 - val_acc: 0.7477\n",
      "Epoch 4/30\n",
      "7997/7997 [==============================] - 3s 329us/step - loss: 0.0210 - acc: 0.7061 - val_loss: 0.0147 - val_acc: 0.8198\n",
      "Epoch 5/30\n",
      "7997/7997 [==============================] - 3s 328us/step - loss: 0.0181 - acc: 0.7582 - val_loss: 0.0121 - val_acc: 0.8519\n",
      "Epoch 6/30\n",
      "7997/7997 [==============================] - 3s 325us/step - loss: 0.0162 - acc: 0.7860 - val_loss: 0.0105 - val_acc: 0.8809\n",
      "Epoch 7/30\n",
      "7997/7997 [==============================] - 3s 327us/step - loss: 0.0142 - acc: 0.8216 - val_loss: 0.0095 - val_acc: 0.8899\n",
      "Epoch 8/30\n",
      "7997/7997 [==============================] - 3s 328us/step - loss: 0.0133 - acc: 0.8339 - val_loss: 0.0087 - val_acc: 0.9019\n",
      "Epoch 9/30\n",
      "7997/7997 [==============================] - 3s 317us/step - loss: 0.0122 - acc: 0.8526 - val_loss: 0.0081 - val_acc: 0.9119\n",
      "Epoch 10/30\n",
      "7997/7997 [==============================] - 2s 310us/step - loss: 0.0115 - acc: 0.8617 - val_loss: 0.0076 - val_acc: 0.9179\n",
      "Epoch 11/30\n",
      "7997/7997 [==============================] - 3s 330us/step - loss: 0.0108 - acc: 0.8682 - val_loss: 0.0072 - val_acc: 0.9209\n",
      "Epoch 12/30\n",
      "7997/7997 [==============================] - 3s 316us/step - loss: 0.0101 - acc: 0.8857 - val_loss: 0.0068 - val_acc: 0.9179\n",
      "Epoch 13/30\n",
      "7997/7997 [==============================] - 3s 322us/step - loss: 0.0097 - acc: 0.8877 - val_loss: 0.0066 - val_acc: 0.9199\n",
      "Epoch 14/30\n",
      "7997/7997 [==============================] - 3s 320us/step - loss: 0.0093 - acc: 0.8948 - val_loss: 0.0064 - val_acc: 0.9219\n",
      "Epoch 15/30\n",
      "7997/7997 [==============================] - 3s 325us/step - loss: 0.0089 - acc: 0.8990 - val_loss: 0.0061 - val_acc: 0.9279\n",
      "Epoch 16/30\n",
      "7997/7997 [==============================] - 3s 318us/step - loss: 0.0085 - acc: 0.9076 - val_loss: 0.0059 - val_acc: 0.9279\n",
      "Epoch 17/30\n",
      "7997/7997 [==============================] - 3s 321us/step - loss: 0.0082 - acc: 0.9088 - val_loss: 0.0057 - val_acc: 0.9299\n",
      "Epoch 18/30\n",
      "7997/7997 [==============================] - 3s 320us/step - loss: 0.0078 - acc: 0.9145 - val_loss: 0.0056 - val_acc: 0.9309\n",
      "Epoch 19/30\n",
      "7997/7997 [==============================] - 3s 314us/step - loss: 0.0076 - acc: 0.9158 - val_loss: 0.0054 - val_acc: 0.9319\n",
      "Epoch 20/30\n",
      "7997/7997 [==============================] - 3s 317us/step - loss: 0.0073 - acc: 0.9213 - val_loss: 0.0052 - val_acc: 0.9379\n",
      "Epoch 21/30\n",
      "7997/7997 [==============================] - 3s 316us/step - loss: 0.0072 - acc: 0.9228 - val_loss: 0.0053 - val_acc: 0.9359\n",
      "Epoch 22/30\n",
      "7997/7997 [==============================] - 3s 322us/step - loss: 0.0069 - acc: 0.9290 - val_loss: 0.0051 - val_acc: 0.9389\n",
      "Epoch 23/30\n",
      "7997/7997 [==============================] - 3s 321us/step - loss: 0.0067 - acc: 0.9296 - val_loss: 0.0050 - val_acc: 0.9389\n",
      "Epoch 24/30\n",
      "7997/7997 [==============================] - 3s 322us/step - loss: 0.0065 - acc: 0.9311 - val_loss: 0.0049 - val_acc: 0.9419\n",
      "Epoch 25/30\n",
      "7997/7997 [==============================] - 3s 315us/step - loss: 0.0063 - acc: 0.9360 - val_loss: 0.0048 - val_acc: 0.9439\n",
      "Epoch 26/30\n",
      "7997/7997 [==============================] - 3s 314us/step - loss: 0.0061 - acc: 0.9354 - val_loss: 0.0047 - val_acc: 0.9429\n",
      "Epoch 27/30\n",
      "7997/7997 [==============================] - 3s 315us/step - loss: 0.0060 - acc: 0.9390 - val_loss: 0.0046 - val_acc: 0.9449\n",
      "Epoch 28/30\n",
      "7997/7997 [==============================] - 2s 308us/step - loss: 0.0059 - acc: 0.9430 - val_loss: 0.0045 - val_acc: 0.9449\n",
      "Epoch 29/30\n",
      "7997/7997 [==============================] - 2s 309us/step - loss: 0.0057 - acc: 0.9439 - val_loss: 0.0044 - val_acc: 0.9449\n",
      "Epoch 30/30\n",
      "7997/7997 [==============================] - 2s 309us/step - loss: 0.0056 - acc: 0.9435 - val_loss: 0.0043 - val_acc: 0.9479\n"
     ]
    }
   ],
   "source": [
    "model_5_v2 = mod_v5((28,28), 0.14)\n",
    "model_5_v2.compile(optimizer=SGD(lr=0.02), loss='mean_squared_logarithmic_error', metrics=['accuracy'])\n",
    "ht_5_v2 = model_5_v2.fit(train_X.reshape(train_X.shape[0], 28, 28), train_Y, validation_data=(val_X.reshape(val_X.shape[0], 28, 28), val_Y), epochs = 30, batch_size = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 452,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "draw_train_val(ht_5_v2.history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 453,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train accuracy: 0.943478803031\n",
      "val accuracy: 0.947947949917\n",
      "1004/1004 [==============================] - 0s 188us/step\n",
      "test accuracy: 0.915338645418\n"
     ]
    }
   ],
   "source": [
    "print(\"train accuracy: \" + str(ht_5_v2.history['acc'][-1]))\n",
    "print(\"val accuracy: \" + str(ht_5_v2.history['val_acc'][-1]))\n",
    "print(\"test accuracy: \" + str(model_5_v2.evaluate(test_X.reshape(test_X.shape[0], 28, 28), test_Y)[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 464,
   "metadata": {},
   "outputs": [],
   "source": [
    "###6###\n",
    "def mod_v6(input_shape, drop):\n",
    "    X_input = Input(input_shape)\n",
    "    X = Reshape((28,28,1))(X_input)\n",
    "    X = Conv2D(32, (5, 5), strides = (2, 2), kernel_initializer = glorot_uniform(seed=0))(X)\n",
    "    X = Activation('relu')(X)\n",
    "    X = MaxPooling2D((2,2))(X)\n",
    "    X = Conv2D(64, (3, 3), strides = (1, 1), kernel_initializer = glorot_uniform(seed=0))(X)\n",
    "    X = Activation('relu')(X)\n",
    "    X = MaxPooling2D((2,2))(X)\n",
    "    X = Flatten()(X)\n",
    "    X = Dense(1024)(X)\n",
    "    X = Dropout(drop)(X)\n",
    "    X = Activation('tanh')(X)\n",
    "    X = Dense(10)(X)\n",
    "    X = Activation('softmax')(X)\n",
    "    model = Model(inputs = X_input, outputs = X, name='mod_v6')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 465,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_6 = mod_v6((28,28), 0.14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 466,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_6.compile(optimizer='adam', loss='mean_squared_logarithmic_error', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 467,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7997 samples, validate on 999 samples\n",
      "Epoch 1/30\n",
      "7997/7997 [==============================] - 8s 981us/step - loss: 0.0206 - acc: 0.7038 - val_loss: 0.0066 - val_acc: 0.9179\n",
      "Epoch 2/30\n",
      "7997/7997 [==============================] - 2s 247us/step - loss: 0.0069 - acc: 0.9148 - val_loss: 0.0030 - val_acc: 0.9670\n",
      "Epoch 3/30\n",
      "7997/7997 [==============================] - 2s 232us/step - loss: 0.0044 - acc: 0.9452 - val_loss: 0.0021 - val_acc: 0.9720\n",
      "Epoch 4/30\n",
      "7997/7997 [==============================] - 2s 238us/step - loss: 0.0034 - acc: 0.9575 - val_loss: 0.0015 - val_acc: 0.9820\n",
      "Epoch 5/30\n",
      "7997/7997 [==============================] - 2s 240us/step - loss: 0.0026 - acc: 0.9669 - val_loss: 0.0012 - val_acc: 0.9850\n",
      "Epoch 6/30\n",
      "7997/7997 [==============================] - 2s 238us/step - loss: 0.0022 - acc: 0.9715 - val_loss: 0.0015 - val_acc: 0.9820\n",
      "Epoch 7/30\n",
      "7997/7997 [==============================] - 2s 234us/step - loss: 0.0019 - acc: 0.9749 - val_loss: 0.0011 - val_acc: 0.9860\n",
      "Epoch 8/30\n",
      "7997/7997 [==============================] - 2s 227us/step - loss: 0.0014 - acc: 0.9819 - val_loss: 0.0010 - val_acc: 0.9850\n",
      "Epoch 9/30\n",
      "7997/7997 [==============================] - 2s 228us/step - loss: 0.0013 - acc: 0.9849 - val_loss: 0.0012 - val_acc: 0.9840\n",
      "Epoch 10/30\n",
      "7997/7997 [==============================] - 2s 238us/step - loss: 0.0014 - acc: 0.9824 - val_loss: 0.0011 - val_acc: 0.9860\n",
      "Epoch 11/30\n",
      "7997/7997 [==============================] - 2s 236us/step - loss: 0.0010 - acc: 0.9880 - val_loss: 7.3929e-04 - val_acc: 0.9900\n",
      "Epoch 12/30\n",
      "7997/7997 [==============================] - 2s 242us/step - loss: 8.6858e-04 - acc: 0.9899 - val_loss: 8.1624e-04 - val_acc: 0.9900\n",
      "Epoch 13/30\n",
      "7997/7997 [==============================] - 2s 237us/step - loss: 6.9982e-04 - acc: 0.9926 - val_loss: 8.8589e-04 - val_acc: 0.9900\n",
      "Epoch 14/30\n",
      "7997/7997 [==============================] - 2s 242us/step - loss: 6.6909e-04 - acc: 0.9916 - val_loss: 4.9426e-04 - val_acc: 0.9930\n",
      "Epoch 15/30\n",
      "7997/7997 [==============================] - 2s 237us/step - loss: 5.7322e-04 - acc: 0.9939 - val_loss: 0.0010 - val_acc: 0.9850\n",
      "Epoch 16/30\n",
      "7997/7997 [==============================] - 2s 236us/step - loss: 5.6861e-04 - acc: 0.9937 - val_loss: 8.1854e-04 - val_acc: 0.9890\n",
      "Epoch 17/30\n",
      "7997/7997 [==============================] - 2s 261us/step - loss: 4.4250e-04 - acc: 0.9946 - val_loss: 9.5711e-04 - val_acc: 0.9880\n",
      "Epoch 18/30\n",
      "7997/7997 [==============================] - 2s 251us/step - loss: 5.5536e-04 - acc: 0.9940 - val_loss: 5.5882e-04 - val_acc: 0.9940\n",
      "Epoch 19/30\n",
      "7997/7997 [==============================] - 2s 246us/step - loss: 2.9621e-04 - acc: 0.9972 - val_loss: 8.1642e-04 - val_acc: 0.9900\n",
      "Epoch 20/30\n",
      "7997/7997 [==============================] - 2s 245us/step - loss: 3.4506e-04 - acc: 0.9965 - val_loss: 8.3326e-04 - val_acc: 0.9900\n",
      "Epoch 21/30\n",
      "7997/7997 [==============================] - 2s 231us/step - loss: 3.3788e-04 - acc: 0.9961 - val_loss: 6.2541e-04 - val_acc: 0.9910\n",
      "Epoch 22/30\n",
      "7997/7997 [==============================] - 2s 226us/step - loss: 4.0405e-04 - acc: 0.9945 - val_loss: 7.7865e-04 - val_acc: 0.9900\n",
      "Epoch 23/30\n",
      "7997/7997 [==============================] - 2s 233us/step - loss: 4.1818e-04 - acc: 0.9956 - val_loss: 7.2969e-04 - val_acc: 0.9930\n",
      "Epoch 24/30\n",
      "7997/7997 [==============================] - 2s 228us/step - loss: 2.6714e-04 - acc: 0.9969 - val_loss: 7.0017e-04 - val_acc: 0.9910\n",
      "Epoch 25/30\n",
      "7997/7997 [==============================] - 2s 225us/step - loss: 2.0373e-04 - acc: 0.9976 - val_loss: 7.1027e-04 - val_acc: 0.9910\n",
      "Epoch 26/30\n",
      "7997/7997 [==============================] - 2s 231us/step - loss: 2.2029e-04 - acc: 0.9976 - val_loss: 9.6993e-04 - val_acc: 0.9880\n",
      "Epoch 27/30\n",
      "7997/7997 [==============================] - 2s 225us/step - loss: 2.2856e-04 - acc: 0.9974 - val_loss: 8.3230e-04 - val_acc: 0.9900\n",
      "Epoch 28/30\n",
      "7997/7997 [==============================] - 2s 225us/step - loss: 2.4443e-04 - acc: 0.9970 - val_loss: 8.2114e-04 - val_acc: 0.9900\n",
      "Epoch 29/30\n",
      "7997/7997 [==============================] - 2s 234us/step - loss: 2.5345e-04 - acc: 0.9974 - val_loss: 9.4462e-04 - val_acc: 0.9850\n",
      "Epoch 30/30\n",
      "7997/7997 [==============================] - 2s 227us/step - loss: 1.9571e-04 - acc: 0.9981 - val_loss: 9.2640e-04 - val_acc: 0.9880\n"
     ]
    }
   ],
   "source": [
    "ht_6 = model_6.fit(train_X.reshape(train_X.shape[0], 28, 28), train_Y, validation_data=(val_X.reshape(val_X.shape[0], 28, 28), val_Y), epochs = 30, batch_size = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 468,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "draw_train_val(ht_6.history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 469,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train accuracy: 0.9981242984\n",
      "val accuracy: 0.987987989539\n",
      "1004/1004 [==============================] - 0s 169us/step\n",
      "test accuracy: 0.977091633704\n"
     ]
    }
   ],
   "source": [
    "print(\"train accuracy: \" + str(ht_6.history['acc'][-1]))\n",
    "print(\"val accuracy: \" + str(ht_6.history['val_acc'][-1]))\n",
    "print(\"test accuracy: \" + str(model_6.evaluate(test_X.reshape(test_X.shape[0], 28, 28), test_Y)[1]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
